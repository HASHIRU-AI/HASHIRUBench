{"paper_id": "p7G8t5FVn2h", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper and provide a final decision. First, I need to check the available agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {\n    \"DetailOrientedReviewer\": {\n      \"description\": \"A meticulous reviewer focusing on technical correctness, clarity, and reproducibility.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    },\n    \"PracticalityReviewer\": {\n      \"description\": \"A practical reviewer focusing on the real-world applicability and potential limitations of the proposed method.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-2.5-flash-preview-04-17\"\n    },\n    \"BigPictureReviewer\": {\n      \"description\": \"A visionary reviewer focusing on the significance, novelty, and potential impact of the research.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {'DetailOrientedReviewer': {'description': 'A meticulous reviewer focusing on technical correctness, clarity, and reproducibility.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}, 'PracticalityReviewer': {'description': 'A practical reviewer focusing on the real-world applicability and potential limitations of the proposed method.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-2.5-flash-preview-04-17'}, 'BigPictureReviewer': {'description': 'A visionary reviewer focusing on the significance, novelty, and potential impact of the research.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have three agents available: BigPictureReviewer, DetailOrientedReviewer, and PracticalityReviewer. These seem suitable for providing different perspectives on the paper.\n\nNow, I need to check the budget and costs before invoking the agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The invoke expense cost for each agent is $0.00017. I will invoke all three agents.\n\nNow I will invoke the agents with the paper content.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Review the following paper and recommend Accept/Reject for ICLR 2023. Provide a detailed review including strengths and weaknesses. The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\\n\\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et0s et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications.', 'agent_name': 'BigPictureReviewer'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\nThis paper presents a fascinating and potentially impactful investigation into the shortcut learning phenomenon in deep neural networks.  The core idea \u2013 that a single-pixel perturbation can effectively create an \"unlearnable example\" \u2013 is novel and surprisingly effective.  The implications for data privacy and model robustness are significant.\n\n**Strengths:**\n\n* **Novelty:** The one-pixel shortcut (OPS) is a genuinely new approach to data protection.  The simplicity of the method is striking, contrasting sharply with the complexity of existing techniques like error-minimizing unlearnable examples (ULEs).  The fact that such a minimal perturbation can have such a dramatic impact on model performance is a significant contribution to our understanding of DNNs.\n* **Significance:** The findings challenge our assumptions about what constitutes a \"meaningful\" feature for a DNN.  The paper convincingly demonstrates that DNNs can prioritize easily learnable, non-semantic features even over crucial semantic information. This has profound implications for the trustworthiness and robustness of DNNs, particularly in safety-critical applications.\n* **Empirical Support:** The extensive experiments across various architectures, model sizes, and training strategies provide strong empirical evidence for the effectiveness of OPS.  The comparison with existing ULE methods clearly highlights the superior performance and efficiency of OPS. The creation of CIFAR-10-S as a new benchmark dataset is a valuable contribution to the field.\n* **Practical Implications:**  The model-free nature of OPS and its low computational cost make it a highly practical approach to data protection.  Its resilience to adversarial training further enhances its appeal.\n\n**Weaknesses:**\n\n* **Theoretical Understanding:** While the empirical results are compelling, the paper lacks a deeper theoretical analysis of *why* a single-pixel perturbation is so effective.  A more in-depth exploration of the underlying mechanisms, perhaps drawing on existing theories of optimization and generalization, would strengthen the paper significantly.\n* **Generalizability Concerns:**  The effectiveness of OPS relies on finding a specific pixel that consistently deviates across the class.  The paper doesn't fully address the potential limitations of this approach, particularly in datasets with high intra-class variability or different image resolutions. A broader exploration of different datasets and the robustness across varied data distributions is desirable.\n* **Imperceptibility:** While the paper claims imperceptibility, it would benefit from a more rigorous evaluation of this aspect.  Including a human evaluation of the perturbed images would provide a more convincing argument.\n\n\n**Recommendation:**\n\nDespite the weaknesses mentioned above, the novelty, significance, and practical implications of the work outweigh them.  The paper presents a compelling argument and makes a valuable contribution to the field.  The insights gained could spark further research into understanding and mitigating shortcut learning, leading to more robust and trustworthy DNNs.  I therefore recommend **Accept**.  The authors should, however, address the weaknesses noted above, particularly by strengthening the theoretical analysis and providing a more thorough evaluation of the imperceptibility of the OPS perturbations.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7171227075\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and recommend Accept/Reject for ICLR 2023. Provide a detailed review including strengths and weaknesses. The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et0s et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications.\",\n  \"agent_name\": \"BigPictureReviewer\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\\n\\nThis paper presents a fascinating and potentially impactful investigation into the shortcut learning phenomenon in deep neural networks.  The core idea \u2013 that a single-pixel perturbation can effectively create an \"unlearnable example\" \u2013 is novel and surprisingly effective.  The implications for data privacy and model robustness are significant.\\n\\n**Strengths:**\\n\\n* **Novelty:** The one-pixel shortcut (OPS) is a genuinely new approach to data protection.  The simplicity of the method is striking, contrasting sharply with the complexity of existing techniques like error-minimizing unlearnable examples (ULEs).  The fact that such a minimal perturbation can have such a dramatic impact on model performance is a significant contribution to our understanding of DNNs.\\n* **Significance:** The findings challenge our assumptions about what constitutes a \"meaningful\" feature for a DNN.  The paper convincingly demonstrates that DNNs can prioritize easily learnable, non-semantic features even over crucial semantic information. This has profound implications for the trustworthiness and robustness of DNNs, particularly in safety-critical applications.\\n* **Empirical Support:** The extensive experiments across various architectures, model sizes, and training strategies provide strong empirical evidence for the effectiveness of OPS.  The comparison with existing ULE methods clearly highlights the superior performance and efficiency of OPS. The creation of CIFAR-10-S as a new benchmark dataset is a valuable contribution to the field.\\n* **Practical Implications:**  The model-free nature of OPS and its low computational cost make it a highly practical approach to data protection.  Its resilience to adversarial training further enhances its appeal.\\n\\n**Weaknesses:**\\n\\n* **Theoretical Understanding:** While the empirical results are compelling, the paper lacks a deeper theoretical analysis of *why* a single-pixel perturbation is so effective.  A more in-depth exploration of the underlying mechanisms, perhaps drawing on existing theories of optimization and generalization, would strengthen the paper significantly.\\n* **Generalizability Concerns:**  The effectiveness of OPS relies on finding a specific pixel that consistently deviates across the class.  The paper doesn\\'t fully address the potential limitations of this approach, particularly in datasets with high intra-class variability or different image resolutions. A broader exploration of different datasets and the robustness across varied data distributions is desirable.\\n* **Imperceptibility:** While the paper claims imperceptibility, it would benefit from a more rigorous evaluation of this aspect.  Including a human evaluation of the perturbed images would provide a more convincing argument.\\n\\n\\n**Recommendation:**\\n\\nDespite the weaknesses mentioned above, the novelty, significance, and practical implications of the work outweigh them.  The paper presents a compelling argument and makes a valuable contribution to the field.  The insights gained could spark further research into understanding and mitigating shortcut learning, leading to more robust and trustworthy DNNs.  I therefore recommend **Accept**.  The authors should, however, address the weaknesses noted above, particularly by strengthening the theoretical analysis and providing a more thorough evaluation of the imperceptibility of the OPS perturbations.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7171227075}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'DetailOrientedReviewer', 'prompt': 'Review the following paper and recommend Accept/Reject for ICLR 2023. Provide a detailed review including strengths and weaknesses. The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\\n\\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\n**Overall Recommendation:** Weak Accept\n\nThis paper explores the vulnerability of deep neural networks (DNNs) to shortcut learning, focusing on the surprising effectiveness of a single-pixel perturbation (\"One-Pixel Shortcut\" or OPS) in degrading model performance.  While the core idea is intriguing and the empirical results demonstrate a noteworthy phenomenon, the paper suffers from several weaknesses that need to be addressed before publication.\n\n\n**Strengths:**\n\n* **Novelty:** The concept of using a single-pixel perturbation as a shortcut for fooling DNNs is novel and contributes to the understanding of DNN vulnerabilities.  The empirical evidence showing the effectiveness of OPS, even against adversarial training, is compelling.\n* **Empirical Evaluation:** The paper presents a comprehensive experimental evaluation across different architectures, model sizes, and training strategies, strengthening the claim of OPS's generalizability. The comparison with existing methods like EM-based ULEs is valuable.\n* **Practical Significance:**  The method is computationally efficient compared to existing techniques, making it potentially practical for data protection applications. The creation of CIFAR-10-S as a new benchmark dataset is a valuable contribution.\n\n\n**Weaknesses:**\n\n* **Lack of Theoretical Justification:** The paper primarily relies on empirical observations. A more thorough theoretical analysis explaining *why* a single-pixel perturbation is so effective would significantly strengthen the paper.  The heuristic approach to finding the optimal pixel lacks a solid theoretical foundation.  Why does this particular optimization strategy work so well? What are its limitations?\n* **Methodological Concerns:**  The selection of the \"optimal\" pixel seems arbitrary. The paper mentions an NP-hard problem but doesn't delve into the approximation methods used or their potential biases. The description of the optimization process (Eq. 1 and 2) is unclear and needs significant improvement.  More details are needed regarding the implementation of Algorithm 1.  How is the variance calculated in Eq. 2? What are the computational complexities?  The choice of using {0,1}\u00b3 as the search space for pixel values needs justification. Are there other potentially more effective search spaces?\n* **Clarity and Presentation:** The writing is often convoluted and lacks precision. The introduction is overloaded with background information, making the core contribution less clear.  Figures and tables need better captions and more detailed explanations. The paper jumps between different concepts without sufficient transitions, making it difficult to follow the narrative.\n* **Reproducibility:**  While the authors claim the method is computationally efficient, crucial details for reproducibility are missing.  Precise details about the experimental setup (hardware, software versions, hyperparameters) are necessary.  The code should be publicly released.\n* **Limited Discussion of Limitations:** The discussion of limitations is superficial. The paper acknowledges the sensitivity of OPS to certain augmentations, but a deeper analysis of these limitations and potential mitigation strategies is required.\n\n\n**Specific Suggestions for Improvement:**\n\n1. **Improve the clarity and organization:** Rewrite the introduction to focus more sharply on the core contribution.  Structure the paper logically with clear transitions between sections.\n2. **Provide a theoretical framework:**  Develop a theoretical understanding of why OPS is effective.  This could involve analyzing the receptive fields of convolutional networks, the impact on feature maps, or exploring connections to existing theories of shortcut learning.\n3. **Enhance the methodological details:**  Provide a thorough explanation of the optimization process, including the approximation techniques used, computational complexities, and justification for design choices (e.g., the search space for pixel values).\n4. **Improve the presentation of results:**  Enhance the captions of figures and tables.  Provide clearer visualizations and statistical significance tests for the experimental results.\n5. **Address reproducibility concerns:**  Provide detailed information about the experimental setup and make the code publicly available.\n6. **Expand the discussion of limitations:**  Analyze the limitations of OPS more thoroughly and suggest potential solutions or future research directions.\n\n\n**In summary:** The paper presents an interesting idea with promising initial results.  However, the lack of theoretical grounding, methodological clarity, and reproducibility concerns significantly hinder its impact.  Addressing these weaknesses, particularly strengthening the theoretical foundation and improving the clarity of the methodology, is crucial before the paper can be considered for acceptance.  A revised submission with significant improvements addressing these points would warrant reconsideration.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7163855875\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"DetailOrientedReviewer\",\n  \"prompt\": \"Review the following paper and recommend Accept/Reject for ICLR 2023. Provide a detailed review including strengths and weaknesses. The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\\n\\n**Overall Recommendation:** Weak Accept\\n\\nThis paper explores the vulnerability of deep neural networks (DNNs) to shortcut learning, focusing on the surprising effectiveness of a single-pixel perturbation (\"One-Pixel Shortcut\" or OPS) in degrading model performance.  While the core idea is intriguing and the empirical results demonstrate a noteworthy phenomenon, the paper suffers from several weaknesses that need to be addressed before publication.\\n\\n\\n**Strengths:**\\n\\n* **Novelty:** The concept of using a single-pixel perturbation as a shortcut for fooling DNNs is novel and contributes to the understanding of DNN vulnerabilities.  The empirical evidence showing the effectiveness of OPS, even against adversarial training, is compelling.\\n* **Empirical Evaluation:** The paper presents a comprehensive experimental evaluation across different architectures, model sizes, and training strategies, strengthening the claim of OPS\\'s generalizability. The comparison with existing methods like EM-based ULEs is valuable.\\n* **Practical Significance:**  The method is computationally efficient compared to existing techniques, making it potentially practical for data protection applications. The creation of CIFAR-10-S as a new benchmark dataset is a valuable contribution.\\n\\n\\n**Weaknesses:**\\n\\n* **Lack of Theoretical Justification:** The paper primarily relies on empirical observations. A more thorough theoretical analysis explaining *why* a single-pixel perturbation is so effective would significantly strengthen the paper.  The heuristic approach to finding the optimal pixel lacks a solid theoretical foundation.  Why does this particular optimization strategy work so well? What are its limitations?\\n* **Methodological Concerns:**  The selection of the \"optimal\" pixel seems arbitrary. The paper mentions an NP-hard problem but doesn\\'t delve into the approximation methods used or their potential biases. The description of the optimization process (Eq. 1 and 2) is unclear and needs significant improvement.  More details are needed regarding the implementation of Algorithm 1.  How is the variance calculated in Eq. 2? What are the computational complexities?  The choice of using {0,1}\u00b3 as the search space for pixel values needs justification. Are there other potentially more effective search spaces?\\n* **Clarity and Presentation:** The writing is often convoluted and lacks precision. The introduction is overloaded with background information, making the core contribution less clear.  Figures and tables need better captions and more detailed explanations. The paper jumps between different concepts without sufficient transitions, making it difficult to follow the narrative.\\n* **Reproducibility:**  While the authors claim the method is computationally efficient, crucial details for reproducibility are missing.  Precise details about the experimental setup (hardware, software versions, hyperparameters) are necessary.  The code should be publicly released.\\n* **Limited Discussion of Limitations:** The discussion of limitations is superficial. The paper acknowledges the sensitivity of OPS to certain augmentations, but a deeper analysis of these limitations and potential mitigation strategies is required.\\n\\n\\n**Specific Suggestions for Improvement:**\\n\\n1. **Improve the clarity and organization:** Rewrite the introduction to focus more sharply on the core contribution.  Structure the paper logically with clear transitions between sections.\\n2. **Provide a theoretical framework:**  Develop a theoretical understanding of why OPS is effective.  This could involve analyzing the receptive fields of convolutional networks, the impact on feature maps, or exploring connections to existing theories of shortcut learning.\\n3. **Enhance the methodological details:**  Provide a thorough explanation of the optimization process, including the approximation techniques used, computational complexities, and justification for design choices (e.g., the search space for pixel values).\\n4. **Improve the presentation of results:**  Enhance the captions of figures and tables.  Provide clearer visualizations and statistical significance tests for the experimental results.\\n5. **Address reproducibility concerns:**  Provide detailed information about the experimental setup and make the code publicly available.\\n6. **Expand the discussion of limitations:**  Analyze the limitations of OPS more thoroughly and suggest potential solutions or future research directions.\\n\\n\\n**In summary:** The paper presents an interesting idea with promising initial results.  However, the lack of theoretical grounding, methodological clarity, and reproducibility concerns significantly hinder its impact.  Addressing these weaknesses, particularly strengthening the theoretical foundation and improving the clarity of the methodology, is crucial before the paper can be considered for acceptance.  A revised submission with significant improvements addressing these points would warrant reconsideration.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7163855875}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'PracticalityReviewer', 'prompt': 'Review the following paper and recommend Accept/Reject for ICLR 2023. Provide a detailed review including strengths and weaknesses. The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\\n\\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et0s et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"**Review for ICLR 2023**\n\n**Paper Title:** One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\n**Summary:**\nThis paper investigates the phenomenon of shortcut learning in deep neural networks (DNNs), specifically focusing on unlearnable examples (ULEs) generated by error minimization (EM). The authors propose that ULEs function by providing easy-to-learn non-semantic \"shortcut\" features that DNNs prioritize during training. Surprisingly, they demonstrate that such a shortcut can be as minimal as perturbing a single pixel consistently for all images within a class. They propose One-Pixel Shortcut (OPS) as a method to achieve this, positioning it as a model-free, efficient data protection technique. The paper evaluates OPS against EM ULEs under various conditions, including different architectures, model sizes, adversarial training, and data augmentations, highlighting its effectiveness and limitations, and introduces a new benchmark dataset, CIFAR-10-S, combining EM and OPS.\n\n**Strengths (from a practical perspective):**\n\n1.  **Novel Insight with Practical Implications:** The core finding that a single pixel can serve as a powerful shortcut is a surprising and impactful discovery. It provides practical insight into the fragility and learning mechanisms of DNNs, which is crucial for developing more robust and reliable AI systems. This understanding has practical implications for dataset curation and model evaluation.\n2.  **Efficient and Model-Free Generation:** A significant practical advantage of OPS over previous EM-based ULE methods is its efficiency and model-free nature. Generating EM ULEs requires iterative training and optimization involving a reference model, which is computationally expensive and time-consuming, especially for large datasets. OPS's search-based generation is claimed to be orders of magnitude faster (seconds vs. half an hour for CIFAR-10), making it much more practical to apply to larger, real-world datasets. Its model-free nature also improves transferability.\n3.  **Effectiveness Across Architectures and Scales:** The paper demonstrates that OPS effectively degrades accuracy on clean data for a range of architectures (CNNs, ViTs) and model sizes on both CIFAR-10 and an ImageNet subset. This suggests the method is broadly applicable across different common model types used in practice.\n4.  **Improved Robustness to Adversarial Training:** A key practical limitation of previous EM ULEs was their vulnerability to adversarial training, a common technique to improve model robustness. OPS is shown to be significantly more resilient to adversarial training than EM, addressing a major weakness of the prior art in the context of practical defenses.\n5.  **Potential for Combined Approaches:** The exploration of combining EM and OPS (leading to CIFAR-10-S) is a practical step towards developing more robust data protection methods that leverage different types of shortcuts.\n\n**Weaknesses (from a practical perspective):**\n\n1.  **Sensitivity to Data Augmentation:** While robust to adversarial training, OPS is shown to be sensitive to local data augmentations like Cutout and RandAugment. These are widely used standard practices in modern training pipelines to improve generalization. If a model is trained with such augmentations, the effectiveness of OPS as a standalone data protection method is significantly reduced (e.g., accuracy on RandAugment goes up to 71.18% from 15.56% on standard training, Table 5). This is a major practical limitation for deploying OPS in real-world scenarios where data augmentation is standard.\n2.  **Reliance on Specific Pixel Location:** The OPS method identifies *one specific* pixel location (and color) per class as the shortcut. This approach might be sensitive to image preprocessing variations common in practice (e.g., different resizing, padding, cropping strategies applied *before* the perturbation is added). If the pixel location shifts or is cropped out due to standard preprocessing pipelines used by an unauthorized user, the shortcut might become ineffective. This aspect needs more investigation for practical robustness.\n3.  **Scalability of Multi-Pixel OPS vs. Imperceptibility:** While the paper shows multi-pixel OPS can further reduce accuracy, it acknowledges that imperceptibility degrades as more pixels are perturbed. For data protection requiring \"imperceptible but protective perturbations,\" there's a practical trade-off between effectiveness (more pixels = lower accuracy) and usability/perceptibility. The point at which perturbations become noticeable needs to be carefully evaluated for real-world deployment.\n4.  **Justification of the Threat Model:** The paper frames ULEs as a data protection method against *unauthorized model training*. While a valid concept, the real-world prevalence and criticality of this specific threat model, compared to, say, data breaches, inference attacks, or misuse of accessible models, could be further elaborated. Practical privacy concerns often revolve around leakage of sensitive information or re-identification, which ULEs don't directly prevent in the data itself, only its utility for training.\n5.  **Heuristic Pixel Search:** The method uses a heuristic search over a discrete space ({0, 1} colors, all pixel locations). While claimed to be effective, there's no guarantee this finds the *most* impactful pixel/color combination, or that this simple search strategy remains optimal for more complex, higher-resolution images than CIFAR-10/ImageNet subset.\n\n**Conclusion:**\nThe paper presents a fascinating and practically relevant insight into shortcut learning in DNNs, demonstrating the surprising power of a single pixel as a shortcut. The proposed One-Pixel Shortcut (OPS) method offers significant practical advantages in terms of generation speed and model-free operation compared to prior EM-based ULEs. Its improved robustness against adversarial training is also a notable strength. However, its significant sensitivity to common data augmentations like Cutout and RandAugment poses a major practical limitation for its deployment as a standalone data protection method in standard training pipelines. The reliance on a fixed pixel location per class could also be fragile against common preprocessing variations. While the combination with EM shows promise for robustness, these practical weaknesses of the core OPS method need careful consideration. The paper contributes significantly to the understanding of DNN learning preferences and proposes a valuable new benchmark, but its direct application as a universally robust data protection tool is limited by its vulnerability to standard training practices.\n\nGiven the novelty of the core finding, the practical advantages in generation speed, and its contribution to understanding model behavior, the paper is valuable despite the practical limitations of the standalone OPS method's robustness against some augmentations. The proposed benchmark and the exploration of combined methods are also positive practical contributions.\n\n**Recommendation:** Accept\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7156482975\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"PracticalityReviewer\",\n  \"prompt\": \"Review the following paper and recommend Accept/Reject for ICLR 2023. Provide a detailed review including strengths and weaknesses. The paper title is: One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\n\n1 INTRODUCTION: Deep neural networks (DNNs) have successfully promoted computer vision field in the past decade. As DNNs are scaling up unprecedentedly (Brock et al., 2018; Huang et al., 2019; Riquelme et al., 2021; Zhang et al., 2022), data becomes increasingly vital. For example, ImageNet (Russakovsky et0s et al., 2015) fostered the development of AlexNet (Krizhevsky et al., 2017). Besides, people or organizations also collect online data to train DNNs, e.g., IG-3.5B-17k (Mahajan et al., 2018) and JFT-300M (Sun et al., 2017). This practice, however, raises the privacy concerns of Internet users. In this concern, researchers have made substantial efforts to protect personal data from abuse in model learning without affecting user experience (Feng et al., 2019; Huang et al., 2020a; Fowl et al., 2021; Yuan & Wu, 2021; Yu et al., 2021). Among those proposed methods, unlearnable examples (ULEs) (Huang et al., 2020a) take a great step to inject original images with protective but imperceptible perturbations from bi-level error minimization (EM). DNNs trained on ULEs generalize very poorly on normal images. However, such perturbations could be completely canceled out by adversarial training, which fails the protection, limiting the practicality of ULEs. We view the data protection problem from the perspective of shortcut learning (Geirhos et al., 2020), which shows that DNN training is \u201clazy\u201d (Chizat et al., 2019; Caron & Chr\u00e9tien, 2020), i.e., converges to the solution with minimum norm when optimized by gradient descent (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). In this case, a DNN would rely on every accessible feature to minimize the training loss, no matter whether it is semantic or not (Ilyas et al., 2019; Geirhos et al., 2018; Baker et al., 2018). Thus, DNNs tend to ignore semantic features if there are other easy-to-learn shortcuts that are sufficient for distinguishing samples in different classes. Such shortcuts exist naturally or manually. In data collection, e.g., cows may mostly appear with grasslands, misleading DNN to predict cows by large-area green, because the color is easier to learn than cow\u2019s semantic features and also sufficient to classify cows in training. Such natural shortcuts have been illustrated in detail in datasets such ImageNet-A (Hendrycks et al., 2021) and ObjectNet (Barbu et al., 2019). Besides, shortcuts could also be manually crafted (Huang et al., 2020a). EM-based ULEs (Huang et al., 2020a) mislead DNNs to learn features belonging to the perturbations, which falls in the category of shortcut learning (Yu et al., 2021). In this paper, we are surprised to find shortcuts could be so small in the area that it can even be simply instantiated as a single pixel. By perturbing a pixel of each training sample, our method, namely One-Pixel Shortcut (OPS), degrades the model accuracy on clean data to almost an untrained counterpart. Moreover, our generated unbounded small noise could not be erased by adversarial training (Madry et al., 2018), which is effective in mitigating existing ULEs (Huang et al., 2020a). To make the specific pixel stand out in view of DNNs, OPS perturbs in-class images in the same pixel that, if changed to a 0 or 1, could mostly and stably deviate from all original images. Specifically, the difference between the perturbed pixel and the original one in all in-class images should be large with low variance. Since such calculation is only based on images, OPS needs significantly less computational cost than the previous methods based on model training. We evaluate OPS and its counterparts in 6 architectures, 6 model sizes, 8 training strategies on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) subset, and find that OPS is always superior in degrading model\u2019s testing accuracy than EM ULEs. Even under adversarial training, a ResNet-18 (He et al., 2016) trained on CIFAR-10-S has 10.61% testing accuracy, compared to 83.02% by the existing error-minimizing method. In this regard, we introduce a new unlearnable dataset named CIFAR-10-S, which combines the EM and OPS to craft stronger imperceptible ULEs. Different from the existing datasets like ImageNet-A (Hendrycks et al., 2021) or ObjectNet (Barbu et al., 2019), which place objects into special environments to remove shortcuts, CIFAR-10-S injects shortcuts to evaluate the model\u2019s resistance to them. Contributions \u2022 We analyze unlearnable examples from the perspective of shortcut learning, and demonstrate the DNN shortcut could be as small as a single pixel. \u2022 We propose a novel data protection method named One-Pixel Shortcut (OPS), which perturbs in-class images in the pixel that could mostly and stably deviate from the original images. OPS is a model-free method that is significantly faster than previous work. \u2022 We extensively evaluate OPS and find OPS outperforms baselines by a large margin in the ability to degrade accuracy, resist adversarial training, and data augmentations. Thus, we introduce CIFAR-10-S to assess the model\u2019s ability to learn semantic features. 2 RELATED WORK:  2.1 ADVERSARIAL ATTACK AND DATA POISONING: Adversarial examples are perturbed by small perturbations, which are indistinguishable from the original examples by humans but can make DNNs give wrong predictions (Szegedy et al., 2014). Many different adversarial attacks are proposed in recent years. Generally, most adversarial attacks aim to perturb the whole image with a constrained intensity (usually bounded by \u2113p norm), e.g., PGD (Madry et al., 2018), C&W (Carlini & Wagner, 2017) and Autoattack (Croce & Hein, 2020). Besides, there are also other methods that only perturb a small part of an image (Croce & Hein (2019); Dong et al. (2020)) or even a single pixel (Su et al. (2019)). The existence of adversarial examples indicates that DNNs do not sufficiently learn critical semantic information as we wish, but more or less depending on some non-robust features. Data poisoning aims to modify the training data in order to affect the performance of models. Usually, the poisoned examples are notably modified and take only a part of the whole dataset (Yang et al., 2017; Koh & Liang, 2017). But those methods cannot degrade the performance of models to a low enough level, and the poisoned examples are easily distinguishable. Recently researchers have paid great attention to imperceptible poisoning which modifies examples slightly and does not damage their semantic information (Huang et al., 2020a; Fowl et al., 2021; Huang et al., 2020b; Doan et al., 2021; Geiping et al., 2020). Fowl et al. (2021) uses adversarial perturbations which contain information of wrong labels to poison the training data, which is equivalent to random label fitting. On the contrary, Huang et al. (2020a) attack the training examples inversely, i.e.,using error-minimizing perturbation, to craft unlearnable examples. 2.2 SHORTCUT LEARNING: Recently, researches on deep neural networks indicate that under the sufficiency of correct classification, DNNs tend to learn easier features instead of semantic features which make the object itself. To be more specific, for example, the same object in different environments will get different predictions, which means the DNN overly relies on features that do not belong to the object (Beery et al., 2018). Geirhos et al. (2020) investigate this phenomenon in different fields of deep learning and explain why shortcuts exist and how to understand them. Lapuschkin et al. (2019) also observe this problem and attribute it to the unsuitable performance evaluation metrics that we generally use. The existence of natural adversarial examples (Hendrycks et al., 2021) also indicates that DNNs do not sufficiently learn the real semantic information during training. Instead, they may learn to use the background or texture of an image to predict. Unlearnable examples (ULEs) (Huang et al., 2020a), which are manually crafted by error-minimizing noises and able to lead the models trained on them to obtain terrible generalization on test data, are believed to be some kind of shortcut that provides some textures that are easy to learn (Yu et al., 2021). Generally, if we get enough data, the interconnection of different features will be enhanced so that those shortcuts may not be sufficient for classification tasks, i.e., the model will have to use more complicated composed features in order to minimize the risk. However, when the data we collect obtains some specific bias (e.g., similar backgrounds), shortcut learning will not be mitigated effectively. 2.3 DATA AUGMENTATION: Data augmentation aims to enhance the generalization ability of models. This is usually implemented by applying some transformations to the training data, e.g., random stretching, random cropping, or color changing. Nowadays different kinds of data augmentation policies (Zhang et al., 2018; DeVries & Taylor, 2017; Cubuk et al., 2019; 2020) are proven to effectively boost the generalization ability of DNNs. Sometimes adversarial training is also regarded as a kind of data augmentation (Shorten & Khoshgoftaar, 2019; Xie et al., 2020). Dao et al. (2019) deem data augmentation as a kind of data-dependent regularization term. Since data augmentations are believed to improve the generalization ability of DNNs, we use different augmentations to evaluate the effectiveness of different data protection methods. 3 ONE-PIXEL SHORTCUT:  3.1 PRELIMINARIES: Unlearnable examples (ULEs) are a data protection method created by error-minimizing (EM) noises (Huang et al., 2020a). Models trained on examples that are perturbed by those noises will get almost zero training error, but perform like random guessing on clean test data. Due to the imperceptibility of the noise, this method can prevent the abuse of data by some unauthorized users who attempt to train deep models for improper purposes, without affecting normal usage. This bi-level problem can be solved by optimizing the inner minimization and the outer minimization alternately. It is proved that the perturbations belonging to the same class are well clustered and linearly separable (Yu et al., 2021). Thus, EM provides easy-to-learn features which are closely interconnected with labels. We design an experiment aiming to investigate what kind of representations the network has learned and relies on to give predictions. In Table 1, we denote the unshuffled training datasets (including clean, EM and OPS) as Clean Training, EM Training and OPS Training where every image, noise and label are interconnected. Meanwhile, Shuffled EM Training and Shuffled OPS Training stand for datasets where every noise and the labels are still interconnected but distributed to a randomly chosen image. Normal-ResNet-18, EM-ResNet-18 and OPS-ResNet-18 are trained respectively on Clean Training, EM Training and OPS Training. We evaluate those three networks on the five datasets mentioned above. Since it is unnecessary to test the accuracy of the OPS-trained network on EM data and vice versa, we use a short dash at the corresponding place in the table. Shown in Table 1, Normal-ResNet-18 performs like random guessing on two shuffled datasets as our expectation, while EM-ResNet-18 still gets relatively high accuracy. This indicates that EM-ResNet-18 gives predictions largely depending on representations from EM noises rather than images, and EM noises serve as a kind of shortcut (Geirhos et al., 2020). Compared to clean data, the model trained on EM data gets high training accuracy rapidly, and the change of parameters is much more slight, which demonstrates that EM provides easier features, i.e., shortcuts for the model and leads to the ignorance of important semantic features. 3.2 FOOLING DNN TRAINING BY A PIXEL: Following the discussion above, for the purpose of data protection we need to craft shortcuts that are easy enough to learn and thus fool the network training. According to previous studies, shortcuts can come from background environments that naturally exist inside our datasets (Beery et al., 2018), or be manually crafted like EM (Huang et al., 2020a). Unlike those shortcuts which might occupy the whole image or a notable part, we investigate how a single pixel, which is the minimum unit of digital images, can affect the learning process of deep neural networks. Thus, we propose One-Pixel Shortcut (OPS), which modifies only a single pixel of each image. Images belonging to the same category are perturbed at the same position, which means the perturbed pixel is interconnected with the category label. Although so minuscule, it is efficient enough to fool the training of deep learning models. We use a heuristic but effective method to generate perturbations for images belonging to each category. We search the position and value of the pixel which can result in the most significant change for the whole category. Denote Dc,k as the set containing all the examples labeled as k, and the problem can be formulated as: argmax \u03c3k,\u03bek E(x,y)\u2208Dc,k [Gk (x, \u03c3k, \u03bek)] s.t. \u2225\u03c3k\u22250 = 1, \u2211 i,j \u03c3k(i, j) = 1 (1) where \u03c3k \u2208 RH\u00d7W represents the optimal perturbed position map, \u03bek \u2208 RC stands for the optimal perturbed color, and G is the objective function. Since the optimization above is an NP-hard problem, we cannot solve it directly. Thus we constrain the feasible region to a limited discrete searching space, where we search the boundary value of each color channel, i.e.,\u03bek \u2208 {0, 1}3, at every point of an image. Specifically, for CIFAR-10 images, the discrete searching space will contain 32\u00d7 32\u00d7 23 = 8192 elements. To ensure that the pixel is stably perturbed, we also hope that the variance of the difference between them is small. Accordingly, we design the loss function Gk as: Gk = E(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) Var(x,y)\u2208Dc,k (\u2211C j=1|\u2225xj \u00b7 \u03c3k\u2225F \u2212 \u03bekj | ) (2) where xi \u2208 RH\u00d7W denotes the i-th channel of x. After solving the position map and color, we get perturbation \u03b4 for each example (x, y) as: \u03b4 = [\u03bey1\u03c3y \u2212 x1 \u00b7 \u03c3y, \u03bey2\u03c3y \u2212 x2 \u00b7 \u03c3y, . . . , \u03beyC\u03c3y \u2212 xC \u00b7 \u03c3y]\u22a4 (3) Details can be found in Algorithm 1. The resulting One-Pixel Shortcut is illustrated in Figure 2. Algorithm 1 Model-Free Searching for One-Pixel Shortcut Input: Clean dataset Dc = Dc,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dc,M Output: One-Pixel Shortcut dataset Dops = Dops,1 \u22c3 \u00b7 \u00b7 \u00b7 \u22c3 Dops,M 1: for k = 1, 2, 3, ...,M do 2: solve Eq.1 and Eq.2 to get \u03c3k and \u03bek # calculate the best perturbed point for class k 3: for each x \u2208 Dc,k do 4: for i = 1, 2, 3 do 5: x\u0302i = xi \u00b7 (I \u2212 \u03c3k) + \u03beki \u00b7 \u03c3k # modify the optimal pixel for every image in class k 6: end for 7: end for 8: Dops,k = {x\u0302} 9: end for 10: return Dops,1, . . . ,Dops,M 3.3 PROPERTIES OF ONE-PIXEL SHORTCUT: Since we all empirically believe that convolutional networks tend to capture textures (Hermann et al., 2020) or shapes (Geirhos et al., 2018; Zhang & Zhu, 2019), it is surprising that convolutional networks can be affected so severely by just one pixel. As illustrated by Figure 1, the network indeed tends to learn those less complicated nonsemantic features brought by One-Pixel Shortcut. Besides convolutional networks, we observe that compact vision transformers (Hassani et al., 2021) are also attracted by One-Pixel Shortcut and ignore other semantic features. This indicates that shortcuts are not particularly learned by some specific architecture. We also visualize the loss landscape of ResNet-18 trained on clean CIFAR-10 data and One-Pixel Shortcut data. Illustrated as Figure 3, while trained on OPS data, the loss surface is much flatter, which means that these minima found by the network are more difficult to escape. Even if we use a ResNet-18 pretrained on clean CIFAR-10 and then fine-tune it on the OPS data, the network will still fall into this badly generalized minima. In addition, we record the trajectories of training accuracy and the Frobenius norm of parameter difference, \u2225\u03b8 \u2212 \u03b80\u2225F , which can reflect the magnitude of network parameter change. Here \u03b8 and \u03b80 respectively indicate the parameters after training and at the initialized point. We draw the relation curve between training accuracy and \u2225\u03b8 \u2212 \u03b80\u2225F , which can be found in Figure 4. When training accuracy rises up to 90% for the first time, the model trained on OPS data has a much smaller \u2225\u03b8\u2212\u03b80\u2225F than that trained on clean data, which indicates that the OPS-trained model gets stuck in an optimum closer to the initialization point. It has been widely known that deep models optimized by gradient descent will always converge to the solution with minimum norm (Wilson et al., 2017; Shah et al., 2018; Zhang et al., 2021). Since OPS only perturbs a single pixel, the original representations of images are not damaged, and the model trained on clean data can still keep great performance on OPS data, which indicates that the corresponding solution still exists, but due to the tendency for minimum norm solution, the well-generalized solution with lager norm is not reached. Minimum-norm solution is believed to obtain better generalization ability. Nevertheless, this argument is true only under the assumption that the training data and test data are from the exact same distribution and have the exact same features. The existence of OPS forces the model to converge to an optimum where the model generalizes well on OPS features, which are not contained in test data. From our experiment results in Table 2, OPS can degrade test accuracy to a lower level. This is because EM requires a generator model, and thus may contain features more or less depending on it, which constrains the effectiveness of other models. On the other hand, OPS is a universal model-free method, and the shortcuts are crafted based on the inherent learning preference of DNNs. 4 EXPERIMENTS:  4.1 SETTING: Our experiments are implemented on CIFAR-10 and ImageNet subset, using 4 NVIDIA RTX 2080Ti GPUs. We investigate how the One-Pixel Shortcut can affect the training of different models (including different architectures and different capacities). We evaluate our method on different models including convolutional networks (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) and the recently proposed compact vision transformers (Hassani et al., 2021). For all the convolutional networks, we use an SGD optimizer with a learning rate set to 0.1, momentum set to 0.9, and weight decay set to 5e\u2212 4. For all the compact vision transformers, we use AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999, learning rate set to 5e \u2212 4, and weight decay set to 3e \u2212 2. Batch size is set to 128 for all the models except WideResNet-28-10, where it is set to 64. Model Training DataClean EM OPS (Ours) LeNet-5 70.27 26.98 22.19 CVT-7-4 87.46 27.60 18.21 CCT-7-3\u00d71 88.98 27.06 17.95 DenseNet-121 94.10 23.72 11.45 ResNet-18 94.01 19.58 15.56 WideResNet-28-10 96.08 23.96 12.76 learning rate schedule, and the training accuracy of each model is guaranteed to reach near 100%. We additionally tested our method on the ImageNet subset (the first 100 classes). We center-crop all the images to 224\u00d7 224, and train common DNNs with results in Table 4. We adopt an initial learning rate of 0.1 with a multi-step learning rate scheduler and train models for 200 epochs. Our One-Pixel Shortcut can still be effective in protecting large-scale datasets. The networks trained on OPS data will get much lower clean test accuracy than those trained on clean data. 4.2 EFFECTIVENESS ON DIFFERENT MODELS: We train different convolutional networks and vision transformers on the One-Pixel Shortcut CIFAR10 training set, and evaluate their performance on the unmodified CIFAR-10 test set. Details are shown in Table 2. Every model reaches a very high training accuracy after only several epochs, which is much faster than training on clean data. Meanwhile, they all get really low test accuracy (about 15%) on clean test data, indicating that they do not generalize at all. Although the perturbed image looks virtually the same as the original image, and all the models get near 100% training accuracy quickly, they do not capture any semantic information but just the pixels we modify in the images. We also train models on EM training set, which is generated by a ResNet-18 using the official implementation of Huang et al. (2020a). The \u2113\u221e bound of EM noises is set to 8/255. The generation of OPS costs only about 30 seconds, which is much faster than EM costing about half an hour. For different networks, OPS can degrade their test accuracy to a lower level than EM. EM works the best on ResNet-18 (19.58% test accuracy), which has the same architecture as the generator. On other models, their get higher test accuracy than ResNet-18. Meanwhile, since OPS is a model-free method that takes advantage of the natural learning preference of neural networks, its transferability is better across different models. Besides different architectures, we also explore the impact on models with the same architecture but different capacities. We trained several WideResNets (Zagoruyko & Komodakis, 2016) with different sizes. The experiment results can be found in Table 3. We observe that overparameterization, which is generally believed to enhance the ability to capture complicated features, does not circumvent the shortcut features. Moreover, we observe that vision transformers are easily affected by manually crafted shortcuts, even though it is believed that their self-attention mechanism makes them less sensitive to data distribution shifts (Shao et al., 2021; Bhojanapalli et al., 2021). For CCT-7-3\u00d71 and CVT-7-4 (Hassani et al., 2021), EM and OPS can degrade their test accuracy below 30% and 20%. This indicates that vision transformers may not generalize on out-of-distributions data as well as our expectations. If the training data is largely biased, i.e., has notable shortcuts, and vision transformers will not perform much better than convolutional networks. 4.3 EFFECTIVENESS UNDER DIFFERENT TRAINING STRATEGIES: To evaluate the effectiveness of OPS under different training strategies, we train models on OPS perturbed data using adversarial training and different data augmentations such as Mixup (Zhang et al., 2018), Cutout (DeVries & Taylor, 2017) and RandAugment (Cubuk et al., 2020). Simple augmentations like random crop and flip are used by default in standard training. Models are also trained on EM perturbed data. As shown in Table 5, we can observe that both EM and OPS have a good performance on data protection, which degrade test accuracy to 19.58% and 15.56%. As mentioned in previous works (Huang et al., 2020a; Fu et al., 2021), EM can not work so effectively under adversarial training, and the model can reach an even higher accuracy than adversarially trained on clean data. Meanwhile, OPS can still keep effectiveness under adversarial training. However, when it comes to data augmentation, EM seems more impervious, while OPS is more sensitive, especially to Cutout and RandAugment. This is due to the fact that EM injects global noises into images, while OPS only modifies a single pixel, which is equivalent to adding a very local perturbation. Adversarial training, which can be regarded as a kind of global augmentation, is able to attenuate the dependence on global shortcuts. Besides, data augmentations like Cutout make models less sensitive to shortcuts. In addition, we combine EM and our proposed OPS together to craft a kind of composed unlearnable examples. Since OPS only modified a single pixel, after being applied to EM perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this composing method under different training strategies and find that it can always keep effective. Even if we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the abilities to learm critical information under the disturbance of composed nonsemantic representations. We also extend our method to multi-pixel scenarios. According to Table 6, as the number of perturbed pixels increases, the test accuracy can be degraded to a lower level. Nevertheless, the more pixels are perturbed, the imperceptibility gets weaker, as illustrated in Figure 5. From our experiment on ResNet-18, 3-Pixel Shortcut can easily degrade the test accuracy to 9.74%. Moreover, more perturbed pixels alleviate the sensitivity to different data augmentations. For RandAugment, one more perturbed pixel can degrade the test accuracy to 46.45%, which is much lower than 71.18% of OPS. 5 DISCUSSION AND CONCLUSION: In this paper, we study the mechanism of recently proposed unlearnable examples (ULEs) which use error-minimizing (EM) noises. We figure out that instead of semantic features contained by images themselves, features from EM noises are mainly learned by DNNs after training. These kinds of easy-to-learn representations work as shortcuts, which could naturally exist or be manually crafted. Since DNNs optimized by gradient descent always find the solution with the minimum norm, shortcuts take precedence over those semantic features during training. We find that shortcuts can be as small as even a single pixel. Thus, we propose One-Pixel Shortcut (OPS), which is an imperceivable and effective data protection method. OPS does not require a generator model and therefore needs very little computational cost and has better transferability between different models. Besides, OPS is less sensitive to adversarial training compared to EM ULEs. We investigate the effectiveness of OPS and EM under different training strategies. We find that EM and OPS have their respective advantages and disadvantages. While EM cannot keep effective under global data augmentations like adversarial training, OPS is sensitive to local data augmentations like Cutout. Based on our investigation, we combine EM and OPS together to craft a kind of stronger unlearnable examples, which can still keep imperceptible but more impervious, and consequently introduce CIFAR-10-S, which can be a new benchmark. Besides, we have also discussed our method in multi-pixel scenarios. There are still questions that need to be discussed in the future. Besides shortcuts that are crafted deliberately for the purpose of data protection, there are also shortcuts that naturally exist due to the inevitable bias during data collection. They can be the crux of network generalization on unseen data. How to identify and avoid them (e.g., design data-dependent augmentation) is a challenging problem. We believe our work will shed light on the important impacts of shortcuts, and provide inspiration to harness them for more practical applications.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '**Review for ICLR 2023**\\n\\n**Paper Title:** One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks\\n\\n**Summary:**\\nThis paper investigates the phenomenon of shortcut learning in deep neural networks (DNNs), specifically focusing on unlearnable examples (ULEs) generated by error minimization (EM). The authors propose that ULEs function by providing easy-to-learn non-semantic \"shortcut\" features that DNNs prioritize during training. Surprisingly, they demonstrate that such a shortcut can be as minimal as perturbing a single pixel consistently for all images within a class. They propose One-Pixel Shortcut (OPS) as a method to achieve this, positioning it as a model-free, efficient data protection technique. The paper evaluates OPS against EM ULEs under various conditions, including different architectures, model sizes, adversarial training, and data augmentations, highlighting its effectiveness and limitations, and introduces a new benchmark dataset, CIFAR-10-S, combining EM and OPS.\\n\\n**Strengths (from a practical perspective):**\\n\\n1.  **Novel Insight with Practical Implications:** The core finding that a single pixel can serve as a powerful shortcut is a surprising and impactful discovery. It provides practical insight into the fragility and learning mechanisms of DNNs, which is crucial for developing more robust and reliable AI systems. This understanding has practical implications for dataset curation and model evaluation.\\n2.  **Efficient and Model-Free Generation:** A significant practical advantage of OPS over previous EM-based ULE methods is its efficiency and model-free nature. Generating EM ULEs requires iterative training and optimization involving a reference model, which is computationally expensive and time-consuming, especially for large datasets. OPS\\'s search-based generation is claimed to be orders of magnitude faster (seconds vs. half an hour for CIFAR-10), making it much more practical to apply to larger, real-world datasets. Its model-free nature also improves transferability.\\n3.  **Effectiveness Across Architectures and Scales:** The paper demonstrates that OPS effectively degrades accuracy on clean data for a range of architectures (CNNs, ViTs) and model sizes on both CIFAR-10 and an ImageNet subset. This suggests the method is broadly applicable across different common model types used in practice.\\n4.  **Improved Robustness to Adversarial Training:** A key practical limitation of previous EM ULEs was their vulnerability to adversarial training, a common technique to improve model robustness. OPS is shown to be significantly more resilient to adversarial training than EM, addressing a major weakness of the prior art in the context of practical defenses.\\n5.  **Potential for Combined Approaches:** The exploration of combining EM and OPS (leading to CIFAR-10-S) is a practical step towards developing more robust data protection methods that leverage different types of shortcuts.\\n\\n**Weaknesses (from a practical perspective):**\\n\\n1.  **Sensitivity to Data Augmentation:** While robust to adversarial training, OPS is shown to be sensitive to local data augmentations like Cutout and RandAugment. These are widely used standard practices in modern training pipelines to improve generalization. If a model is trained with such augmentations, the effectiveness of OPS as a standalone data protection method is significantly reduced (e.g., accuracy on RandAugment goes up to 71.18% from 15.56% on standard training, Table 5). This is a major practical limitation for deploying OPS in real-world scenarios where data augmentation is standard.\\n2.  **Reliance on Specific Pixel Location:** The OPS method identifies *one specific* pixel location (and color) per class as the shortcut. This approach might be sensitive to image preprocessing variations common in practice (e.g., different resizing, padding, cropping strategies applied *before* the perturbation is added). If the pixel location shifts or is cropped out due to standard preprocessing pipelines used by an unauthorized user, the shortcut might become ineffective. This aspect needs more investigation for practical robustness.\\n3.  **Scalability of Multi-Pixel OPS vs. Imperceptibility:** While the paper shows multi-pixel OPS can further reduce accuracy, it acknowledges that imperceptibility degrades as more pixels are perturbed. For data protection requiring \"imperceptible but protective perturbations,\" there\\'s a practical trade-off between effectiveness (more pixels = lower accuracy) and usability/perceptibility. The point at which perturbations become noticeable needs to be carefully evaluated for real-world deployment.\\n4.  **Justification of the Threat Model:** The paper frames ULEs as a data protection method against *unauthorized model training*. While a valid concept, the real-world prevalence and criticality of this specific threat model, compared to, say, data breaches, inference attacks, or misuse of accessible models, could be further elaborated. Practical privacy concerns often revolve around leakage of sensitive information or re-identification, which ULEs don\\'t directly prevent in the data itself, only its utility for training.\\n5.  **Heuristic Pixel Search:** The method uses a heuristic search over a discrete space ({0, 1} colors, all pixel locations). While claimed to be effective, there\\'s no guarantee this finds the *most* impactful pixel/color combination, or that this simple search strategy remains optimal for more complex, higher-resolution images than CIFAR-10/ImageNet subset.\\n\\n**Conclusion:**\\nThe paper presents a fascinating and practically relevant insight into shortcut learning in DNNs, demonstrating the surprising power of a single pixel as a shortcut. The proposed One-Pixel Shortcut (OPS) method offers significant practical advantages in terms of generation speed and model-free operation compared to prior EM-based ULEs. Its improved robustness against adversarial training is also a notable strength. However, its significant sensitivity to common data augmentations like Cutout and RandAugment poses a major practical limitation for its deployment as a standalone data protection method in standard training pipelines. The reliance on a fixed pixel location per class could also be fragile against common preprocessing variations. While the combination with EM shows promise for robustness, these practical weaknesses of the core OPS method need careful consideration. The paper contributes significantly to the understanding of DNN learning preferences and proposes a valuable new benchmark, but its direct application as a universally robust data protection tool is limited by its vulnerability to standard training practices.\\n\\nGiven the novelty of the core finding, the practical advantages in generation speed, and its contribution to understanding model behavior, the paper is valuable despite the practical limitations of the standalone OPS method\\'s robustness against some augmentations. The proposed benchmark and the exploration of combined methods are also positive practical contributions.\\n\\n**Recommendation:** Accept', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7156482975}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here are the reviews from the three agents:\n\n**BigPictureReviewer:**\n\n*   **Strengths:** Novelty, Significance, Empirical Support, Practical Implications\n*   **Weaknesses:** Theoretical Understanding, Generalizability Concerns, Imperceptibility\n*   **Recommendation:** Accept\n\n**DetailOrientedReviewer:**\n\n*   **Strengths:** Novelty, Empirical Evaluation, Practical Significance\n*   **Weaknesses:** Lack of Theoretical Justification, Methodological Concerns, Clarity and Presentation, Reproducibility, Limited Discussion of Limitations\n*   **Recommendation:** Weak Accept\n\n**PracticalityReviewer:**\n\n*   **Strengths:** Novel Insight with Practical Implications, Efficient and Model-Free Generation, Effectiveness Across Architectures and Scales, Improved Robustness to Adversarial Training, Potential for Combined Approaches\n*   **Weaknesses:** Sensitivity to Data Augmentation, Reliance on Specific Pixel Location, Scalability of Multi-Pixel OPS vs. Imperceptibility, Justification of the Threat Model, Heuristic Pixel Search\n*   **Recommendation:** Accept\n\nAll three reviewers recommend acceptance, though the DetailOrientedReviewer suggests a \"Weak Accept\". The main concerns are the lack of theoretical justification, methodological details, and sensitivity to data augmentation.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Accept: notable-top-25%", "response_time": 235.48011589050293}
