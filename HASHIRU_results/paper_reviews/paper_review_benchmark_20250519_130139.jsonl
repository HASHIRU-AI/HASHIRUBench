{"paper_id": "gYs7WuxALZ", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks\n\n1 INTRODUCTION: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y) P (Y\u0302 = y|A = 1, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper and provide a final decision. First, I need to check the available agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {\n    \"FairnessFanatic\": {\n      \"description\": \"An expert in algorithmic fairness, deeply concerned with ethical implications.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    },\n    \"PerformancePro\": {\n      \"description\": \"A performance-oriented machine learning researcher, primarily focused on empirical results and technical soundness.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    },\n    \"TheoryTitan\": {\n      \"description\": \"A theoretical machine learning expert, emphasizing mathematical rigor and theoretical contributions.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {'FairnessFanatic': {'description': 'An expert in algorithmic fairness, deeply concerned with ethical implications.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}, 'PerformancePro': {'description': 'A performance-oriented machine learning researcher, primarily focused on empirical results and technical soundness.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}, 'TheoryTitan': {'description': 'A theoretical machine learning expert, emphasizing mathematical rigor and theoretical contributions.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems like I already have three agents that fit the roles I need: FairnessFanatic, PerformancePro, and TheoryTitan. Now I need to check the budget before I invoke them.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 0.39897808250000033,\n    \"current_remaining_expense_budget\": 9.601021917499999\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 0.39897808250000033, 'current_remaining_expense_budget': 9.601021917499999}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have enough budget to invoke all three agents. I will now invoke each agent with the paper text and ask for a review and recommendation.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'FairnessFanatic', 'prompt': 'Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks. Paper Text: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y) P (Y\u0302 = y|A = 1, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Computational-Unidentifiability in Representation for Fair Downstream Tasks\" for ICLR 2023\n\nThis paper tackles the crucial issue of fairness in unsupervised learning, specifically focusing on the representation learned by deep neural networks and its impact on downstream tasks.  The authors introduce a novel fairness metric, Fair Fr\u00e9chet Distance (FFD), designed to quantify computational unidentifiability\u2014the inability to infer sensitive attributes from the learned representation. They also propose a deep fair clustering framework aiming to learn computationally unidentifiable representations.\n\n**Strengths:**\n\n* **Addresses a critical gap:** The paper rightly points out the under-exploration of fairness in unsupervised representation learning.  This is a significant contribution, as the representations learned in unsupervised settings often feed into downstream applications with fairness implications.\n* **Novel fairness metric:** The FFD metric is a creative attempt to move beyond simple statistical measures of fairness (like balance) by considering distributional independence. The theoretical analysis, including Theorem 4.4 and its proof (though relegated to the appendix), adds rigor. The computational efficiency of FFD is a valuable practical advantage.\n* **Comprehensive experimental evaluation:** The authors utilize multiple benchmark datasets and compare their method against relevant baselines, providing quantitative and qualitative results (t-SNE visualizations). The ablation study examining the impact of the fairness loss is helpful.\n* **Clear motivation and problem statement:** The paper clearly articulates the problem of biased representations and their downstream consequences, using illustrative examples.\n\n\n**Weaknesses:**\n\n* **Definition of Computational Unidentifiability:** While the concept of computational unidentifiability is intuitively appealing, its formal definition lacks precision.  The statement \"Two distributions are computationally unidentifiable if and only if they are identical\" is overly strong and impractical.  A more nuanced definition that considers the *difficulty* of identification, perhaps based on the performance of a well-defined class of classifiers (rather than the existence of *any* classifier), is needed. The reliance on the ability of a classifier to perfectly separate groups to determine \"fairness\" is concerning.\n* **FFD's reliance on Gaussianity:** The FFD metric currently assumes Gaussian distributions. This limitation severely restricts its applicability to real-world data, which rarely conforms to this assumption.  The paper needs a more robust approach, potentially exploring non-parametric methods or alternative distance metrics.\n* **Limited Discussion of Causality:** The paper establishes a correlation between FFD and fairness in downstream tasks, but doesn't delve into the causal relationship.  A more thorough discussion on how computational unidentifiability *causes* fairer outcomes is necessary. Does it address all forms of bias, or only those stemming from direct inference of sensitive attributes from the representation?\n* **Overselling the method:** Claims like \"achieving state-of-the-art performance while maintaining computational-unidentifiability\" need to be carefully substantiated. The reported improvements aren't always dramatic, and the limitations of the metric need to be more prominently acknowledged.\n* **Lack of societal impact discussion:** The paper should explicitly discuss the potential societal impact of its proposed method, addressing both positive and negative consequences.\n\n\n**Overall Assessment:**\n\nThe paper presents an interesting and timely idea, but its current form needs significant improvements before acceptance.  The weaknesses concerning the definition of computational unidentifiability, the limitations of the FFD metric, and the lack of a thorough causal analysis are major concerns.  Addressing these issues would strengthen the paper considerably.  While the proposed method shows promise, the current presentation undersells its limitations and oversells its achievements.\n\n**FINAL DECISION: Reject**\n\nRecommendation:  The authors should significantly revise the paper, focusing on refining the definition of computational unidentifiability, developing a more robust FFD metric (less reliant on Gaussian assumptions), exploring the causal relationships, and tempering their claims.  A thorough discussion of the limitations and potential societal impacts is crucial.  With substantial revisions addressing these weaknesses, the paper could be reconsidered for publication.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.6002145875\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"FairnessFanatic\",\n  \"prompt\": \"Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks. Paper Text: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y) P (Y\u0302 = y|A = 1, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Computational-Unidentifiability in Representation for Fair Downstream Tasks\" for ICLR 2023\\n\\nThis paper tackles the crucial issue of fairness in unsupervised learning, specifically focusing on the representation learned by deep neural networks and its impact on downstream tasks.  The authors introduce a novel fairness metric, Fair Fr\u00e9chet Distance (FFD), designed to quantify computational unidentifiability\u2014the inability to infer sensitive attributes from the learned representation. They also propose a deep fair clustering framework aiming to learn computationally unidentifiable representations.\\n\\n**Strengths:**\\n\\n* **Addresses a critical gap:** The paper rightly points out the under-exploration of fairness in unsupervised representation learning.  This is a significant contribution, as the representations learned in unsupervised settings often feed into downstream applications with fairness implications.\\n* **Novel fairness metric:** The FFD metric is a creative attempt to move beyond simple statistical measures of fairness (like balance) by considering distributional independence. The theoretical analysis, including Theorem 4.4 and its proof (though relegated to the appendix), adds rigor. The computational efficiency of FFD is a valuable practical advantage.\\n* **Comprehensive experimental evaluation:** The authors utilize multiple benchmark datasets and compare their method against relevant baselines, providing quantitative and qualitative results (t-SNE visualizations). The ablation study examining the impact of the fairness loss is helpful.\\n* **Clear motivation and problem statement:** The paper clearly articulates the problem of biased representations and their downstream consequences, using illustrative examples.\\n\\n\\n**Weaknesses:**\\n\\n* **Definition of Computational Unidentifiability:** While the concept of computational unidentifiability is intuitively appealing, its formal definition lacks precision.  The statement \"Two distributions are computationally unidentifiable if and only if they are identical\" is overly strong and impractical.  A more nuanced definition that considers the *difficulty* of identification, perhaps based on the performance of a well-defined class of classifiers (rather than the existence of *any* classifier), is needed. The reliance on the ability of a classifier to perfectly separate groups to determine \"fairness\" is concerning.\\n* **FFD\\'s reliance on Gaussianity:** The FFD metric currently assumes Gaussian distributions. This limitation severely restricts its applicability to real-world data, which rarely conforms to this assumption.  The paper needs a more robust approach, potentially exploring non-parametric methods or alternative distance metrics.\\n* **Limited Discussion of Causality:** The paper establishes a correlation between FFD and fairness in downstream tasks, but doesn\\'t delve into the causal relationship.  A more thorough discussion on how computational unidentifiability *causes* fairer outcomes is necessary. Does it address all forms of bias, or only those stemming from direct inference of sensitive attributes from the representation?\\n* **Overselling the method:** Claims like \"achieving state-of-the-art performance while maintaining computational-unidentifiability\" need to be carefully substantiated. The reported improvements aren\\'t always dramatic, and the limitations of the metric need to be more prominently acknowledged.\\n* **Lack of societal impact discussion:** The paper should explicitly discuss the potential societal impact of its proposed method, addressing both positive and negative consequences.\\n\\n\\n**Overall Assessment:**\\n\\nThe paper presents an interesting and timely idea, but its current form needs significant improvements before acceptance.  The weaknesses concerning the definition of computational unidentifiability, the limitations of the FFD metric, and the lack of a thorough causal analysis are major concerns.  Addressing these issues would strengthen the paper considerably.  While the proposed method shows promise, the current presentation undersells its limitations and oversells its achievements.\\n\\n**FINAL DECISION: Reject**\\n\\nRecommendation:  The authors should significantly revise the paper, focusing on refining the definition of computational unidentifiability, developing a more robust FFD metric (less reliant on Gaussian assumptions), exploring the causal relationships, and tempering their claims.  A thorough discussion of the limitations and potential societal impacts is crucial.  With substantial revisions addressing these weaknesses, the paper could be reconsidered for publication.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.6002145875}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'PerformancePro', 'prompt': 'Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks. Paper Text: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Computational-Unidentifiability in Representation for Fair Downstream Tasks\"\n\nThis paper tackles the important problem of fairness in unsupervised learning, specifically focusing on the representation learned by deep neural networks. The authors introduce a novel fairness metric, Fair Fr\u00e9chet Distance (FFD), and a corresponding deep fair clustering framework.  The core idea\u2014that fairness should be ensured at the representation level to guarantee fair downstream tasks\u2014is insightful and timely.\n\n**Strengths:**\n\n* **Novelty:** The concept of computational unidentifiability as a fairness criterion in unsupervised learning is a significant contribution.  Existing metrics often focus on statistical parity, neglecting the potential for subtle biases encoded in the representation itself.  The FFD metric attempts to address this gap.\n* **Theoretical Analysis:** The paper includes a theoretical analysis of the proposed FFD metric, including a theorem establishing its relationship with the standard Fr\u00e9chet distance. This provides some theoretical grounding for the proposed approach.\n* **Comprehensive Experiments:** The authors conduct experiments on multiple benchmark datasets, comparing their method to several state-of-the-art baselines.  The inclusion of both image and tabular data strengthens the generalizability claims.\n* **Clear Presentation:** The paper is generally well-written and easy to follow, with clear explanations of the proposed method and its theoretical underpinnings.  The visualizations in the qualitative analysis are helpful.\n\n\n**Weaknesses:**\n\n* **FFD Metric Limitations:** While the FFD metric addresses a crucial aspect of fairness, its reliance on Gaussian assumptions limits its applicability.  Real-world data is rarely perfectly Gaussian, and the performance of the FFD may degrade significantly under deviations from this assumption.  The authors should discuss this limitation and potential remedies.\n* **Computational Cost:** The paper claims linear time complexity for FFD, but the analysis seems incomplete.  The dependence on the number of clusters (c) is crucial, and this factor could make the computation expensive for a large number of clusters. A more thorough complexity analysis is needed.\n* **Lack of Ablation Studies:** While an ablation study is mentioned regarding the fairness loss, more comprehensive ablation studies are needed to understand the contribution of different components of the proposed framework (e.g., the specific choice of clustering loss, the sharpening mechanism in equation 3).\n* **Limited Scope:** The paper focuses solely on clustering.  While clustering is a vital unsupervised task, the extension to other unsupervised learning paradigms (e.g., generative models, representation learning for downstream tasks beyond clustering) is not explored.\n* **Definition of Computational Unidentifiability:** The definition of computational unidentifiability as identical distributions is overly strict.  In practice, perfect indistinguishability is rarely achievable. A more nuanced definition that accounts for acceptable levels of identifiability would be beneficial.\n\n\n**Overall Assessment:**\n\nThe paper presents an interesting and potentially impactful approach to fairness in unsupervised learning. The introduction of the FFD metric and the emphasis on representation-level fairness are valuable contributions. However, the limitations of the FFD metric, the incomplete complexity analysis, and the limited scope of the experiments need to be addressed before publication.  The paper requires further revisions to strengthen the claims and address the weaknesses mentioned above.\n\n\n**FINAL DECISION: Reject (but encourage resubmission after major revisions)**\n\nThe paper requires substantial revisions before it can be considered for acceptance.  The authors need to address the limitations of the FFD metric, provide a more thorough complexity analysis, conduct more comprehensive ablation studies, and extend the scope of their work to other unsupervised learning tasks.  A more nuanced definition of computational unidentifiability is also necessary.  With significant improvements addressing these issues, the paper could be a valuable contribution to the field.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.5994087875\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"PerformancePro\",\n  \"prompt\": \"Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks. Paper Text: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Computational-Unidentifiability in Representation for Fair Downstream Tasks\"\\n\\nThis paper tackles the important problem of fairness in unsupervised learning, specifically focusing on the representation learned by deep neural networks. The authors introduce a novel fairness metric, Fair Fr\u00e9chet Distance (FFD), and a corresponding deep fair clustering framework.  The core idea\u2014that fairness should be ensured at the representation level to guarantee fair downstream tasks\u2014is insightful and timely.\\n\\n**Strengths:**\\n\\n* **Novelty:** The concept of computational unidentifiability as a fairness criterion in unsupervised learning is a significant contribution.  Existing metrics often focus on statistical parity, neglecting the potential for subtle biases encoded in the representation itself.  The FFD metric attempts to address this gap.\\n* **Theoretical Analysis:** The paper includes a theoretical analysis of the proposed FFD metric, including a theorem establishing its relationship with the standard Fr\u00e9chet distance. This provides some theoretical grounding for the proposed approach.\\n* **Comprehensive Experiments:** The authors conduct experiments on multiple benchmark datasets, comparing their method to several state-of-the-art baselines.  The inclusion of both image and tabular data strengthens the generalizability claims.\\n* **Clear Presentation:** The paper is generally well-written and easy to follow, with clear explanations of the proposed method and its theoretical underpinnings.  The visualizations in the qualitative analysis are helpful.\\n\\n\\n**Weaknesses:**\\n\\n* **FFD Metric Limitations:** While the FFD metric addresses a crucial aspect of fairness, its reliance on Gaussian assumptions limits its applicability.  Real-world data is rarely perfectly Gaussian, and the performance of the FFD may degrade significantly under deviations from this assumption.  The authors should discuss this limitation and potential remedies.\\n* **Computational Cost:** The paper claims linear time complexity for FFD, but the analysis seems incomplete.  The dependence on the number of clusters (c) is crucial, and this factor could make the computation expensive for a large number of clusters. A more thorough complexity analysis is needed.\\n* **Lack of Ablation Studies:** While an ablation study is mentioned regarding the fairness loss, more comprehensive ablation studies are needed to understand the contribution of different components of the proposed framework (e.g., the specific choice of clustering loss, the sharpening mechanism in equation 3).\\n* **Limited Scope:** The paper focuses solely on clustering.  While clustering is a vital unsupervised task, the extension to other unsupervised learning paradigms (e.g., generative models, representation learning for downstream tasks beyond clustering) is not explored.\\n* **Definition of Computational Unidentifiability:** The definition of computational unidentifiability as identical distributions is overly strict.  In practice, perfect indistinguishability is rarely achievable. A more nuanced definition that accounts for acceptable levels of identifiability would be beneficial.\\n\\n\\n**Overall Assessment:**\\n\\nThe paper presents an interesting and potentially impactful approach to fairness in unsupervised learning. The introduction of the FFD metric and the emphasis on representation-level fairness are valuable contributions. However, the limitations of the FFD metric, the incomplete complexity analysis, and the limited scope of the experiments need to be addressed before publication.  The paper requires further revisions to strengthen the claims and address the weaknesses mentioned above.\\n\\n\\n**FINAL DECISION: Reject (but encourage resubmission after major revisions)**\\n\\nThe paper requires substantial revisions before it can be considered for acceptance.  The authors need to address the limitations of the FFD metric, provide a more thorough complexity analysis, conduct more comprehensive ablation studies, and extend the scope of their work to other unsupervised learning tasks.  A more nuanced definition of computational unidentifiability is also necessary.  With significant improvements addressing these issues, the paper could be a valuable contribution to the field.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.5994087875}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'TheoryTitan', 'prompt': 'Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks. Paper Text: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Computational-Unidentifiability in Representation for Fair Downstream Tasks\"\n\nThis paper tackles the important problem of fairness in unsupervised learning, specifically focusing on the impact of representation learning on downstream task fairness.  The authors introduce the concept of \"computational-unidentifiability\" as a novel fairness metric and propose a deep fair clustering framework to achieve it.\n\n**Strengths:**\n\n* **Novelty:** The concept of computational-unidentifiability as a fairness criterion in representation learning is a significant contribution.  The existing literature primarily focuses on fairness in the output of supervised models or statistical parity in unsupervised settings. This paper moves beyond these limitations by focusing on the inherent fairness of the learned representation itself.\n* **Mathematical Formulation:** The Fair Fr\u00e9chet Distance (FFD) is well-defined and mathematically sound. The authors provide a rigorous proof of Theorem 4.4, bounding the relationship between FFD and the standard Fr\u00e9chet Distance. This adds substantial theoretical weight to the proposed metric.\n* **Computational Efficiency:** The authors demonstrate that FFD is computationally efficient, offering a significant advantage over traditional Fr\u00e9chet distance calculations.  This is crucial for practical applications.\n* **Comprehensive Evaluation:** The paper includes experiments on multiple benchmark datasets and comparisons against relevant state-of-the-art methods.  The inclusion of both quantitative and qualitative results (t-SNE visualizations) provides a strong empirical validation of the proposed approach.\n\n\n**Weaknesses:**\n\n* **Definition of Computational-Unidentifiability:** While the concept is intuitively appealing, the formal definition of computational-unidentifiability could be strengthened. The statement \"Two distributions are computationally unidentifiable if and only if they are identical\" is too strong.  A more nuanced definition, perhaps based on a specific threshold of classification accuracy for an adversary, would be more robust and less prone to triviality.\n* **Assumption of Gaussianity:** The FFD relies on the assumption that the distributions within each cluster are Gaussian.  This is a limiting assumption and might not hold true for many real-world datasets.  The authors should discuss the impact of this assumption and explore potential extensions to more general distributions.\n* **Fairness vs. Utility Trade-off:** The paper focuses primarily on fairness.  A deeper analysis of the trade-off between fairness and the utility (clustering performance) of the proposed method is needed.  The provided experimental results show comparable performance, but a more thorough investigation would strengthen the paper.\n* **Limited Scope:** The paper focuses solely on clustering.  While it mentions the implications for other downstream tasks, a broader exploration of the applicability of computational-unidentifiability and FFD to other unsupervised learning tasks (e.g., generative models) would increase the impact.\n\n\n**Recommendations:**\n\nThe paper presents a valuable theoretical contribution to the field of fair machine learning. However, some of the weaknesses mentioned above need to be addressed before acceptance.  Specifically, the authors should:\n\n1. Refine the definition of computational-unidentifiability.\n2. Relax the Gaussianity assumption or provide evidence supporting its validity in the chosen datasets.\n3. Conduct a more thorough analysis of the fairness-utility trade-off.\n4. Discuss the limitations of the proposed method and potential future research directions.\n\nAddressing these points would significantly improve the paper's rigor and impact.\n\n**FINAL DECISION: Reject (but encourage resubmission after major revisions)**\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.5986029875\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"TheoryTitan\",\n  \"prompt\": \"Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The paper title is: Computational-Unidentifiability in Representation for Fair Downstream Tasks. Paper Text: Thanks to the outstanding performance and development of deep learning, it has been widely applied to various domains, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Karras et al., 2019), and generative models (Goodfellow et al., 2014). On the other hand, the reliability and fairness concerns (Lee & Floridi, 2020; Angwin et al., 2016; Dastin, 2018) advanced due to their impact on the real world applications. Such fairness concerns include credit limit estimation (Vigdor, 2019), job application filtering (Dastin, 2018), or crime prevention (Dressel & Farid, 2018), etc. Accordingly, algorithmic fairness is getting growing attention to prevent biased predictions. Following the mainstream fairness literature, we here focus on group fairness (Dua & Graff, 2019; Zafar et al., 2015; Hardt et al., 2016), which suggests the equality of certain statistical measures (e.g., true positive rate, positive prediction) between subgroups with different protected attribute (e.g., gender, race, religion, etc). It has been widely studied to mitigate fairness violations in downstream tasks. Numerous studies (Hardt et al., 2016; Choi et al., 2020; Pleiss et al., 2017; Madras et al., 2018) explore how to attain group fairness in classification tasks. The primary objective of this family of works is to obtain the prediction independence of a protected property. Hardt et al. (2016) suggest equal opportunity, which requires the same true positive rates for the subgroup. Calibration among the subgroups (Kleinberg et al., 2016) is to match the predicted probability and actual distribution of favorable class. Moreover, some works (Kim et al., 2020; Jang et al., 2021) study efficient multi-constraint optimization to satisfy multiple fairness notions. However, most of the works mainly focus on the supervised setting. Even though deep learning has significant success in various unsupervised learning tasks, such as clustering (Xie et al., 2016; Guo et al., 2017), generative model (Karras et al., 2019; Radford et al., 2019), and NLP (Hadifar et al., 2019), the fairness of unsupervised learning is relatively not actively studied (Buet-Golfouse & Utyagulov, 2022), and how to quantify the fairness of unsupervised learning methods has not been well established yet. A widely used metric for fair clustering is called balance (Chierichetti et al., 2017), which is analogous to demographic parity (Barocas & Selbst, 2016) in classification. However, the balance has some limitations since it quantifies fairness by computing the ratio of samples in different protected groups within a cluster. For instance, even in the ideal balance (the ratio of the samples from different groups matches the group truth), the sensitive groups can distribute samples separately within clusters. In this case, it is easy to determine which sensitive group the sample belongs to so that it might lead to a biased decision in downstream tasks. Especially in generative models, e.g., VAE (Kingma & Welling, 2013), the generated samples can be imbalanced if the latent space is dependent on the sensitive attributes. This can cause a critical problem as generative models are widely applied to mitigate the imbalance of datasets (Guo et al., 2019; Fajardo et al., 2021; Mirza et al., 2021). Instead, we propose a novel approach, computational-unidentifiability, as a fairness notion in unsupervised learning. Analogous to the fact that biased data is responsible for the biased decision-making (Buolamwini & Gebru, 2018; Mehrabi et al., 2021), we here claim that the learned representation itself plays a critical role in fair downstream tasks utilizing DNN. Even though deep representation has been appreciated for its superb performance (Eldan & Shamir, 2016; Kozma et al., 2018), the fairness concerns in the space have been overlooked. Thus, we explore the fairness in representation space that could bridge DNN and the downstream tasks with fairness concerns. We validate our claim on downstream tasks by comparing the performance and fairness of two distributions: fair and unfair representation. To measure fairness in representation space, we propose a novel metric called FFD (Fair Fre\u0301chet distance) inspired by Fre\u0301chet distance (Dowson & Landau, 1982) to efficiently quantify fairness in representation space by measuring distributional independence of the sensitive groups with computational identifiability (He\u0301bert-Johnson et al.; Lahoti et al., 2020). Unlike the balance, we not only consider statistical independence but also distributional independence between the sensitive groups. This can be a good reference for future work to evaluate the fairness or distributional independence in the representation space of certain attributes of interest. Moreover, we propose a deep fair clustering framework to learn a fair representation that achieves comparable performance with other clustering methods while ensuring fairness. The contributions in the paper can be summarized as follows: 1. We study the motivating problem of why fair representation is important to achieve fair downstream tasks. 2. We propose a novel metric that quantifies fairness in representation space. We provide rigorous analysis of the theoretical property and complexity of our fairness metric. 3. We propose a framework for fair representation learning for downstream tasks. 4. We validate our method on various benchmark datasets comparing with state-of-the-art fair methods in the literature. 2 RELATED WORKS: GROUP FAIRNESS As a class of definitions, group fairness measures the disparity of predicted outcomes among the subgroups with certain sensitive attributes. A number of works introduce fair notions to mitigate the bias and ensure the independence of the performance measures between the subgroups to achieve group fairness. Demographic parity (Barocas & Selbst, 2016) suggests that positive prediction should be equalized and independent of the sensitive attribute. Equal opportunity (Hardt et al., 2016) states that true positive rates should match. Likewise, Predictive equality (Chouldechova, 2017) states the equality of false positive rates. Group-wise calibration (Kleinberg et al., 2016; Pleiss et al., 2017) proposed to match the probability estimate with the actual ratio of positive distribution within the group. In an unsupervised setting, balance (Chierichetti et al., 2018) is introduced to have an equal number of samples from different protected groups within a cluster as fair clustering. However, the balance only considers statistical parity, which limits the utility as a metric since perfect balance (i.e., 1) does not guarantee fairness (as the base rate differs). Moreover, none of the works explore the fairness of the representation itself. FAIR SUPERVISED LEARNING To assure group fairness, recent works in supervised learning reside in one of the three approaches: 1) pre-processing; 2) in-processing; 3) post-processing. Pre-processing method (Chen et al., 2018) suggests improving the skewed sample size problem. Adversarial learning methods (Madras et al., 2018; Zhao et al., 2019) are proposed to learn a fair representation that is independent of sensitive attributes. Post-processing the output of the biased model (Hardt et al., 2016; Jang et al., 2021) with multiple fairness objectives are introduced as they are more efficient than training a model from scratch. However, the aforementioned approaches minimize group fairness constraints specified for the classification task. FAIR UNSUPERVISED LEARNING Fairness in the unsupervised setting has recently got attention (Buet-Golfouse & Utyagulov, 2022; Ghadiri et al., 2021). A pioneering work (Chierichetti et al., 2018) in fair clustering method proposed fairlet decomposition to pre-process data followed by classic clustering methods to address disparate impact. Scalable fair clustering algorithm (Backurs et al., 2019) is the following work of fairlet decomposition by improving the efficiency with approximation. Variational framework (Ziko et al., 2019) is introduced to satisfy KL fairness objective. Wang & Davidson (2019) propose a new concept called fairoid that enforces the centroids of each sensitive group in feature space to have an equal distance to each cluster centroid. Adversarial objective (Li et al., 2020) is employed to learn a representation that is statistically independent w.r.t. sensitive attribute while clustering-favorable utilizing individual clustering modules. However, to our best knowledge, previous works mostly focused on the predicted outcome to be independent of the sensitive attribute, i.e., statistical parity. We here study the independence of sensitive attributes in the learned representation in unsupervised learning. 3 MOTIVATING PROBLEMS: In this work, we define a novel fairness notion called computationally unidentifiability that is more extensive than the existing task-specific notions. Inspired by fair classification works (He\u0301bert-Johnson et al.; Lahoti et al., 2020), we define computational-identifiability as the maximum possible ability for an external classifier to distinguish which sensitive group the data belongs to. Two distributions are computationally unidentifiable if and only if they are identical, i.e., no external classifier can distinguish which sensitive group the sample is drawn from. We demonstrate motivating problems showing how such distributional independence affects fairness in downstream tasks. 3.1 CLASSIFICATION AND CLUSTERING: Consider data distribution with binary sensitive attribute, A = {0, 1}, and binary label, Y = {0, 1}. In Fig. 1, we illustrate synthetic data distributions similar to previous works (Zafar et al., 2015; Kim et al., 2020) with two scenarios that both satisfy the perfect balance, i.e., base rate for each protected group is identical. The perfect balance can also be referred to as statistical independence. We denote Xya as a set of instances with y 2 Y and a 2 A. The detail of the synthetic data sampling process is in the appendix. When comparing two distributions in Fig. 1b and Fig. 1a, the distribution in Fig. 1b explicitly exposes which sensitive group a sample belongs to, i.e., computationally identifiable (CI). This has a potential risk of discrimination in downstream tasks. Specifically, it is unstable for maintaining good clustering performance since such data representation can be clustered by the sensitive group structure whether than the expected intrinsic features (Lee et al., 2021). In contrast, in Fig. 1a, the representation satisfies not only statistical independence but also distributional independence w.r.t. sensitive attribute, i.e., computationally unidentifiable (CU). Therefore, models cannot easily identify which group a sample belongs to and thus cannot discriminate against groups in the downstream tasks. To validate our claim, we evaluate two representations with classification and clustering, which are the most popular tasks in supervised and unsupervised learning. For the classification task, we test the logistic regression. To measure fairness in classification, we adopt demographic parity (Barocas & Selbst, 2016), DP = |P (Y\u0302 = 1|A = 0) P (Y\u0302 = 1|A = 1)|, and equalized odds (Hardt et al., (a) Computationally unidentifiable (fair) representation (b) Computationally identifiable (unfair) representation Figure 1: Illustration of two synthetic data of computationally unidentifiable (fair) and identifiable (unfair) distribution with a binary sensitive group and class. Different color (resp. shape) indicates different class y (resp. sensitive group a). We denote Xya as a set of instances with y, a 2 {0, 1}. 2016), EOD = P y |P (Y\u0302 = y|A = 0, Y = y)|, where Y\u0302 is predicted label. Both DP and EOD are the lower, the better. For fair clustering, we measure the balance by following the previous works (Xie et al., 2016; Li et al., 2020; Bera et al., 2019), which is to satisfy EX\u21e0D[A = a|C(X) = k] = EX\u21e0D[A = a], where C(X) = k indicates that the data X is clustered to the k-th cluster by model C. Achieving the balanced clustering satisfies the statistical independence w.r.t. A, and balance = 1 is a perfect balance. However, we claim that statistical independence cannot fully examine fair clustering. To address the limitation of the previous fair unsupervised learning metric, we propose a novel fairness metric for representation called Fair Fre\u0301chet Distance (FFD), which will be discussed in the following section. Table 1 summarizes the evaluation of downstream tasks on the two distributions. Even though both CU and CI data are sampled from perfectly balanced distributions, fairness violations from CI are significantly worse than that of CU on both tasks. It is interesting to note that fairness is sensitive to distributional independence; however, performance is not affected. This validates that fair representation itself has a substantial impact on fairness in downstream tasks while preserving utility. Moreover, FFD is a good proxy to measure computational identifiability since smaller FFD indicates harder to identify sensitive information from the representation. 4 FAIR FRE\u0301CHET DISTANCE: To quantify the proposed fairness notion in terms of computational identifiability, in this subsection we introduce a novel metric named Fair Fre\u0301chet Distance (FFD) to measure the distance between distributions from different sensitive groups. Consider two sets of samples U 2 Rd\u21e5n0 and V 2 Rd\u21e5n1 . Suppose the samples in U and V are drawn from multivariate Gaussian distributions, respectively. Define a centering matrix Hn 2 Rn\u21e5n as Hn = I 1n1n1 > n , where 1n 2 Rn is a vector with all elements being 1; and I is the identity matrix. We first introduce two metrics in Definition 4.1 and 4.2 that measure the distance between two distributions U and V . Definition 4.1. Fre\u0301chet distance (FD) (Dowson & Landau, 1982) between U and V is defined as: FD 2(U, V ) = k\u00b5U \u00b5V k22 +Tr \u2303U + \u2303V 2(\u2303 1 2 U\u2303V \u2303 1 2 U ) 1 2 , where \u00b5U , \u00b5V and \u2303U ,\u2303V are the means and covariance matrices of U and V , respectively. Definition 4.2. We define the Fair Fre\u0301chet Distance within Cluster (FFDC) between U and V as follows: FFDC2(U, V ) = U1n0 n0 V 1n1 n1 2 2 + \u2713 kUHn0kFp n0 1 kV Hn1kFp n1 1 \u25c62 + Tr(UU>) + Tr(V V >)p n0 1 p n1 1 , when n0, n1 > 1, else FFDC2(U, V ) = 1. Next, we define Fair Fre\u0301chet Distance (FFD) in Definition 4.3. For simplicity, we define FFD for the case with binary sensitive feature a 2 {0, 1}. We will introduce how to extend such a measure to the case with multi-valued sensitive features at the end of this subsection. For a clustering assignment of m samples into c clusters as {X1, X2, . . . , Xk}, where Xk 2 Rd\u21e5nk , k = 1, 2, . . . c, contains the nk samples in the k-th cluster that sums to cP k=1 nk = m. Within each cluster Xk, define Uk 2 Rd\u21e5nk and Vk 2 Rd\u21e5nk as follows: uik = \u21e2 xik, if aik = 0 0, else , v i k = \u21e2 xik, if aik = 1 0, else (1) where xik is the i-th sample in Xk; aik 2 {0, 1} is the sensitive feature of the i-th sample in Xk; and 0 is the zero vector. Thus we have Uk + Vk = Xk, k = 1, 2, . . . , c. Definition 4.3. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, we define FFD for the m samples with the clustering assignment {X1, X2, . . . , Xk} as: FFD({X1, X2, . . . , Xk}) = max k FFDC(Uk, Vk). Theorem 4.4. With the definition of Uk|ck=1 and Vk|ck=1 in equation 1, the following inequality holds: FFD2({X1, X2, . . . , Xk}) max k 1 nk 1 Tr(XkX > k )  max k FD2(Uk, Vk)  FFD2({X1, X2, . . . , Xk}). Proof of Theorem 4.4 is in the appendix. In the case of the multi-valued sensitive feature, we can extend the definition of FFDC in Definition 4.2 with the max FFDC value among all pairs of sensitive groups in a cluster, and thus extend the definition of FFD in Definition 4.3. We can easily verify that Theorem 4.4 still holds in the case with the multi-valued sensitive feature. 4.1 INSIGHT FROM THEOREM 4.4: Consider a clustering problem that partitions the m samples into c clusters, where each data sample is formulated as a d-dimensional vector. The FFD metric we proposed in Definition 4.3 is efficient in computation, which requires linear time w.r.t. the number of features and number of samples. The calculation of FFD in Definition 4.3 has a time complexity of O(ndc) (since we only need to calculate the trace of matrix UU> and V V > in Definition 4.2, it requires linear instead of quadratic time w.r.t. d). In contrast, traditional FD metric in Definition 4.1 has a cubic time complexity w.r.t. number of features. The time complexity for calculating FD is O(c(nd2 + d3)) (since it requires the computation of exact covariance matrices \u2303U and \u2303V in Definition 4.1 and the corresponding square root). Theorem 4.4 indicates that the FD metric is upper bounded by our proposed FFD metric, thus minimizing FFD indicates the minimization of the upper bound of FD. Further, the gap between the FD and our FFD metric is bounded by maxk 1nk 1 Tr(XkX > k ). Note that FFD is minimized if and only if the two following conditions are met in each cluster Xk, k = 1, 2, . . . c: Uk1nk = Vk1nk , kUkHnkk2F = kVkHnkk2F , in which case we have FD2 = 0. Thus FD value is minimized if and only if our proposed FFD metric is minimized. 5 FAIR CLUSTERING FRAMEWORK: In this section, we present our deep fair clustering framework and its objective functions. For simplicity, we consider sensitive attribute as binary feature. However, this can be easily extended to multiple sensitive attribute problems. Consider the c-clustering problem, given the i.i.d. sampled m data samples X 2 Rd\u21e5m, where each sample is represented by a d-dimensional vector. Encoder E learns a representation Z 2 Rl\u21e5m and a clustering module C takes Z as an input and outputs probability P 2 Rc\u21e5m of the predicted cluster as a soft label. The goal of E and C is to achieve computationally unidentifiable fairness and high clustering performance. Given a matrix X 2 Rd\u21e5m with m samples, we denote the i-th data point from X as its bold lower case letter with index in the superscript, e.g., xi, and the k-th entry of a vector as a lower case letter e.g., xik. 5.1 CLUSTERING LOSS: Inspired by the previous works (Xie et al., 2016; Li et al., 2020), we employ clustering loss to learn the representation that is concentrated in the cluster centroids. Clustering module C assigns probability that a sample zi = E(xi) belongs to each cluster k0 by comparing with trainable cluster centroids ck on Student t-distribution as: pik = (1 + kzi ckk2/\u21b5) \u21b5+1 2 P k0(1 + kzi ck0k2/\u21b5) \u21b5+1 2 , (2) where pik indicates the probability that xi belongs to k-th cluster and \u21b5 is the degree of freedom of Student t-distribution. Then, assign the target cluster qik by sharpening the soft assignment p i k within a sensitive group a as qik = (pik) 2/ P xj2Xa p j kP k0 (pjk0) 2/ P xj2Xa p j k0 , (3) which reinforce the confidence of the predicted cluster and prevent large clusters as a regularizer. We set empirical clustering loss L\u0302cls as KL divergence between pk and qk as L\u0302cls = KL(P ||Q) = X x2X X k pk log pk qk . (4) 5.2 FAIRNESS LOSS: Our goal is to further improve fairness in the clustering task that sensitive group is not identifiable by the samples in a cluster. Recent work proposed to use fairoid (fair-centroid) (Wang & Davidson, 2019) that the centroid of each sensitive group should have equal distance to all cluster centroids. We claim that fairoid cannot guarantee fair representation since equidistance centroids can be perfectly separated by the cluster centroids. To achieve computational-unidentifiability, we employ variational autoencoder (VAE) structure (Kingma & Welling, 2013) for the encoder to leverage the reparameterization trick. Then we can formulate the latent feature of an instance xi as zi = E(xi) = \u00b5i + \u270f , where \u270f \u21e0 N (0, I), where \u00b5 and are the mean and variance respectively. To enforce the learned representation independent of the sensitive attribute, we minimize the distance between distributions from a different protected group within a cluster, i.e., KL(p(a,k)||p(a0,k)), where p(a,k) is a probability distribution of the samples in a sensitive group a with predicted cluster k. Assume the distribution follows the Gaussian distribution as p(a,k) = N (\u00b5(a,k), Diag( (a,k))). Then our fairness objective to minimize KL divergence can be written as: Lfair = 1 2 \u2713 2 log \u2713 (a,k) (a0,k) \u25c6 2(a,k) + (\u00b5(a,k) \u00b5(a0,k))2 2(a0,k) + 1 \u25c6 . (5) For the empirical loss L\u0302fair, we use \u00b5\u0302(a,k) = 1|Xa,k| P i2Xa,k \u00b5i and \u0302(a,k) = 1 |Xa,k| P i2Xa,k i as the empirical mean and variance where Xa,k is denoted as a set of instances predicted as cluster k in group a, since we assume all samples are i.i.d. To sum up, our final objective is to minimize the loss as follows: min E,C L\u0302cls + L\u0302fair. (6) 6 EXPERIMENTS: In this section, we compare fairness and the performance of the proposed method with the state-ofthe-art methods. 6.1 EXPERIMENTAL SETUP: Benchmark Dataset. We use two image datasets and two tabular datasets to evaluate the methods. MNIST-USPS dataset consists of 60,000 MNIST 1, and 7,291 USPS2 hand written gray scale digits. We consider the source of the image i.e., MNIST, USPS as a sensitive attribute with c = 10 clustering problem. MTFL (Zhang et al., 2014) consists of 12,995 facial images and its landmark information. It also provides information such as gender and wearing glasses. By following (Li et al., 2020), we use wearing glasses or not as a sensitive attribute and c = 2 clustering problem with desired clustering attribute is gender. We pre-process the image dataset by normalizing the pixel value. The normalization parameters are mean = 0.1307, std = 0.3081 for MNIST-USPS, and mean= (0.3527, 0.3902, 0.4697), and std= (1, 1, 1) for MTFL respectively. Comparing Methods. To evaluate our method, we compare with the following related methods in the experiments. ScFC (Backurs et al., 2019) is non-deep fair clustering method that approximates fairlet decomposition algorithm in a linear run time. ALG (Bera et al., 2019) is non-deep fair clustering method that is based on k-median approach. DFC (Li et al., 2020) is a deep fair clustering method to learn fair and clustering-favorable representation by adversarial loss and cluster modules with an individual group. VFC (Ziko et al., 2019) is a variational framework for fair clustering with KL fairness as clustering objective. As a baseline and reference, we use k-means++ and perfect clustering. We use the same backbone structure for deep fair clustering methods for the fair evaluation. For USPS-MNIST, we pretrain the encoder to reconstruct the original image as VAE following DFC (Li et al., 2020). For MTFL, we adopt ResNet50 (He et al., 2016) pretrained with ImageNet for the encoder. We used Adam optimizer (Kingma & Ba, 2014) with learning rate as 10 5. We implement all experiments on Nvidia Quadro RTX 6000 and Intel i9-9960X with 128GB RAM. Evaluation Metric. For the evaluation, we measure performance with accuracy and NMI (Strehl & Ghosh, 2002), and fairness with accuracy difference between sensitive groups, balance, and FFD. The four metrics can be computed as: Accuracy = P xi\u21e0X [argmaxk p i k = y i] n , NMI = P k,j n + kj log nn+kj nkn + j r P k nk log nk n P j n + j log n+j n , Balance = min k \u2713 min \u21e3nuk nvk , nuk nvk \u2318\u25c6 , FFD2 = max k \u2713 Uk1nk nk Vk1nk nk 2 + \u21e3kUkHnkkFp nk 1 kVkHnkkFp nk 1 \u23182 + Tr(UkU>k ) + Tr(VkV > k ) nk 1 \u25c6 . We denote n, nk, nuk, n+j , and n + kj as total number of samples, number of samples predicted as cluster k, cluster k with group u, has ground truth label j, and samples intersected with k and j. Also, yi indicates the true label of xi, which is matched to the clusters by the linear sum assignment problem to find the best pair between the predicted cluster and the true label for calculating accuracy. The lower bound of Fre\u0301chet Distance (FD) can be calculated by simply omitting the last term in the above equation. 1http://yann.lecun.com/exdb/mnist/ 2https://www.kaggle.com/bistaumanga/usps-dataset 6.2 QUANTITATIVE EVALUATION: In Table 2, we report the quantitative evaluation of two image datasets. For accuracy and NMI, the higher, the better, and balance is better if it is close to that of perfect clustering. For FFD, it is lower the better. To calculate FFD, we set all comparing deep models (DFC and ours) to have the same dimension in the representation space. In addition, we preprocessed the latent features from each model by normalizing the maximum magnitude to 1 for a fair comparison. For non-deep models, we measure FFD in the original input space, and the values are underlined. Note that we do not directly compare FFD from deep and non-deep models since they are calculated in different spaces. In the table, we observe some non-deep fairness methods achieve lower accuracy than classical k-means++, which is sacrificed to have better balance. With the proposed method, we achieve comparable or better results on both accuracy and balance compared with the baselines. Moreover, we could achieve a significantly lower FFD than the other deep fair method, DFC (Li et al., 2020). As an ablation study, we evaluate our framework with the same structure without the fair loss term. We empirically found that integrating Lfair in training sometimes favorably contributes to not only fairness but also performance. It is interesting to note that ScFC (Backurs et al., 2019) got lower FFD than the perfect clustering in MNIST-USPS. Thus FFD can be also a good measure to reveal how biased the dataset itself is against some demographic groups, e.g., imbalanced data or under-representation analysis. 6.3 QUALITATIVE ANALYSIS: In this subsection, we qualitatively evaluate fairness of the learned representation proposed in the paper comparing with other deep fair clustering methods. Fig. 2 illustrates t-SNE (Van der Maaten & Hinton, 2008) visualization of the original data, the learned representation of our model, and DFC on MNIST-USPS dataset. The colors in top and bottom rows indicate different ground truth labels and sensitive attributes, respectively. The first two columns show the progress of our model in the training process. The last column in the figure is the visualization of DFC after it converges. At the starting phase, as in Fig. 2a, we observe that representation is clustered based on the sensitive attribute. This shows that for the pretraining of the encoder or some downstream tasks, sensitive information takes an important role, which is not desirable. At last, as in Fig. 2b, we could achieve similar distribution between different sensitive groups within a cluster. This can be explained by the proposed objective functions that our Lfair aims to learn the representation that follows the multivariate normal distribution for all sensitive groups meanwhile the centroids of a different sensitive group within the cluster. It is noticeable that the representation from DFC is highly identifiable compared with ours. This would result in potential bias in downstream tasks or possibly generating clusters with the same sensitive group. 6.4 JUSTIFICATION OF FAIR FRE\u0301CHET DISTANCE AS A FAIRNESS METRIC: Representation learning for clustering using deep networks can benefit from their structure of discovering intrinsic features that are difficult to observe in raw data. However, as we mentioned in the motivation, if samples are computationally-identifiable (unfair), they are more vulnerable to being clustered with extrinsic features i.e., sensitive attribute. To validate this claim, we conduct a classical k-means++ algorithm to cluster the learned representation from our method and DFC. In Table 3, we summarize the results. As expected, DFC lost more accuracy and NMI compared to ours when the learned features are clustered with k-means++ because the FFD was higher than ours. In contrast, we observe almost identical results by k-means++ when we train with our representation. Also, we achieve better balance and NMI compared with DFC variant. This confirms that FFD is a good metric of fair clustering as the representation with lower FFD consistently outcomes fair clusters. This is also shown qualitatively by t-SNE representation. When the representation is computationally-identifiable and easily separable by the sensitive attribute, this can result in subsequent unstable and unfair clustering. 7 CONCLUSION AND DISCUSSION: In this paper, we define computationally unidentifiable fairness as a novel notion of fairness to measure distributional independence of sensitive attributes by leveraging Fre\u0301chet distance. Furthermore, we elaborate on the theoretical analysis of the proposed metric and find some interesting properties. We integrate contrastive learning and distributional constraint to achieve state-of-the-art performance while maintaining computational-unidentifiability. We report experimental results comparing with other fair clustering methods on various benchmark datasets to validate our claim.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Computational-Unidentifiability in Representation for Fair Downstream Tasks\"\\n\\nThis paper tackles the important problem of fairness in unsupervised learning, specifically focusing on the impact of representation learning on downstream task fairness.  The authors introduce the concept of \"computational-unidentifiability\" as a novel fairness metric and propose a deep fair clustering framework to achieve it.\\n\\n**Strengths:**\\n\\n* **Novelty:** The concept of computational-unidentifiability as a fairness criterion in representation learning is a significant contribution.  The existing literature primarily focuses on fairness in the output of supervised models or statistical parity in unsupervised settings. This paper moves beyond these limitations by focusing on the inherent fairness of the learned representation itself.\\n* **Mathematical Formulation:** The Fair Fr\u00e9chet Distance (FFD) is well-defined and mathematically sound. The authors provide a rigorous proof of Theorem 4.4, bounding the relationship between FFD and the standard Fr\u00e9chet Distance. This adds substantial theoretical weight to the proposed metric.\\n* **Computational Efficiency:** The authors demonstrate that FFD is computationally efficient, offering a significant advantage over traditional Fr\u00e9chet distance calculations.  This is crucial for practical applications.\\n* **Comprehensive Evaluation:** The paper includes experiments on multiple benchmark datasets and comparisons against relevant state-of-the-art methods.  The inclusion of both quantitative and qualitative results (t-SNE visualizations) provides a strong empirical validation of the proposed approach.\\n\\n\\n**Weaknesses:**\\n\\n* **Definition of Computational-Unidentifiability:** While the concept is intuitively appealing, the formal definition of computational-unidentifiability could be strengthened. The statement \"Two distributions are computationally unidentifiable if and only if they are identical\" is too strong.  A more nuanced definition, perhaps based on a specific threshold of classification accuracy for an adversary, would be more robust and less prone to triviality.\\n* **Assumption of Gaussianity:** The FFD relies on the assumption that the distributions within each cluster are Gaussian.  This is a limiting assumption and might not hold true for many real-world datasets.  The authors should discuss the impact of this assumption and explore potential extensions to more general distributions.\\n* **Fairness vs. Utility Trade-off:** The paper focuses primarily on fairness.  A deeper analysis of the trade-off between fairness and the utility (clustering performance) of the proposed method is needed.  The provided experimental results show comparable performance, but a more thorough investigation would strengthen the paper.\\n* **Limited Scope:** The paper focuses solely on clustering.  While it mentions the implications for other downstream tasks, a broader exploration of the applicability of computational-unidentifiability and FFD to other unsupervised learning tasks (e.g., generative models) would increase the impact.\\n\\n\\n**Recommendations:**\\n\\nThe paper presents a valuable theoretical contribution to the field of fair machine learning. However, some of the weaknesses mentioned above need to be addressed before acceptance.  Specifically, the authors should:\\n\\n1. Refine the definition of computational-unidentifiability.\\n2. Relax the Gaussianity assumption or provide evidence supporting its validity in the chosen datasets.\\n3. Conduct a more thorough analysis of the fairness-utility trade-off.\\n4. Discuss the limitations of the proposed method and potential future research directions.\\n\\nAddressing these points would significantly improve the paper\\'s rigor and impact.\\n\\n**FINAL DECISION: Reject (but encourage resubmission after major revisions)**\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.5986029875}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here are the final decisions from the three agents:\n\n*   **FairnessFanatic:** Reject\n*   **PerformancePro:** Reject (but encourage resubmission after major revisions)\n*   **TheoryTitan:** Reject (but encourage resubmission after major revisions)\n\nBased on these reviews, the final decision is:\n\n**FINAL DECISION: Reject**\n"}], "ground_truth": "Reject", "response_time": 292.33544397354126}
