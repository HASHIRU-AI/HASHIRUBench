{"paper_id": "4zGai1tFQE", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Deep Dependency Networks for Action Classification in Video\n\n1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {}\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Since there are no existing agents, I will create three new agents with different personalities to review the paper. I will use the `gemini-1.5-pro` model for each agent.\n\nBefore creating the agents, I need to check the budget. The cost to create each agent is $0.005, and the cost to invoke each agent will be $0.0001275. Since I will be invoking each agent once, the total expense cost will be 3 * $0.005 + 3 * $0.0001275 = $0.0153825.\n\nI will now create the three agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in deep learning and probabilistic graphical models, provides detailed and constructive criticism.', 'base_model': 'gemini-1.5-pro', 'agent_name': 'ICLRReviewer1', 'system_prompt': 'You are an expert reviewer for ICLR. You have extensive knowledge of deep learning and probabilistic graphical models. You provide detailed and constructive criticism, focusing on the clarity, novelty, and significance of the work. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject).'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are an experienced machine learning researcher. You focus on the practical applications and experimental results of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Pay close attention to the experimental setup, datasets, and metrics used.', 'base_model': 'gemini-1.5-pro', 'agent_name': 'ICLRReviewer2', 'description': 'Experienced machine learning researcher, focuses on the practical applications and experimental results.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewer3', 'base_model': 'gemini-1.5-pro', 'description': 'Theoretical machine learning expert, emphasizes the novelty and theoretical contributions of the paper.', 'system_prompt': 'You are a theoretical machine learning expert. You emphasize the novelty and theoretical contributions of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Focus on the mathematical rigor and theoretical soundness of the proposed method.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7265075625\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in deep learning and probabilistic graphical models, provides detailed and constructive criticism.\",\n  \"base_model\": \"gemini-1.5-pro\",\n  \"agent_name\": \"ICLRReviewer1\",\n  \"system_prompt\": \"You are an expert reviewer for ICLR. You have extensive knowledge of deep learning and probabilistic graphical models. You provide detailed and constructive criticism, focusing on the clarity, novelty, and significance of the work. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject).\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7215075625\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an experienced machine learning researcher. You focus on the practical applications and experimental results of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Pay close attention to the experimental setup, datasets, and metrics used.\",\n  \"base_model\": \"gemini-1.5-pro\",\n  \"agent_name\": \"ICLRReviewer2\",\n  \"description\": \"Experienced machine learning researcher, focuses on the practical applications and experimental results.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7165075625\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"ICLRReviewer3\",\n  \"base_model\": \"gemini-1.5-pro\",\n  \"description\": \"Theoretical machine learning expert, emphasizes the novelty and theoretical contributions of the paper.\",\n  \"system_prompt\": \"You are a theoretical machine learning expert. You emphasize the novelty and theoretical contributions of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Focus on the mathematical rigor and theoretical soundness of the proposed method.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7265075625}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7215075625}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7165075625}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now that the agents are created, I will ask each agent to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\\\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\", 'agent_name': 'ICLRReviewer1'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '9s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\",\n  \"agent_name\": \"ICLRReviewer1\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '9s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It appears I have exceeded my quota for the `gemini-1.5-pro` model. I will try again using the `gemini-1.5-flash` model, which should be less expensive and hopefully within my quota. I will need to create three new agents using this model. I will also fire the previous agents to free up resources.\n\nFirst, I will fire the previous agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewer1'}, name='FireAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewer2'}, name='FireAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewer3'}, name='FireAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully fired.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7157833625\n}\n```\n", "metadata": {"title": "Invoking `FireAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewer1\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully fired.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7157833625\n}\n```\n", "metadata": {"title": "Invoking `FireAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewer2\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully fired.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7157833625\n}\n```\n", "metadata": {"title": "Invoking `FireAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewer3\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='FireAgent', response={'result': {'status': 'success', 'message': 'Agent successfully fired.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7157833625}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='FireAgent', response={'result': {'status': 'success', 'message': 'Agent successfully fired.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7157833625}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='FireAgent', response={'result': {'status': 'success', 'message': 'Agent successfully fired.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7157833625}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now I will create three new agents using the `gemini-1.5-flash` model.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-1.5-flash', 'agent_name': 'ICLRReviewerA', 'description': 'Expert in deep learning and probabilistic graphical models, provides detailed and constructive criticism.', 'system_prompt': 'You are an expert reviewer for ICLR. You have extensive knowledge of deep learning and probabilistic graphical models. You provide detailed and constructive criticism, focusing on the clarity, novelty, and significance of the work. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject).'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-1.5-flash', 'description': 'Experienced machine learning researcher, focuses on the practical applications and experimental results.', 'agent_name': 'ICLRReviewerB', 'system_prompt': 'You are an experienced machine learning researcher. You focus on the practical applications and experimental results of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Pay close attention to the experimental setup, datasets, and metrics used.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are a theoretical machine learning expert. You emphasize the novelty and theoretical contributions of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Focus on the mathematical rigor and theoretical soundness of the proposed method.', 'description': 'Theoretical machine learning expert, emphasizes the novelty and theoretical contributions of the paper.', 'agent_name': 'ICLRReviewerC', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent ICLRReviewerA already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-1.5-flash\",\n  \"agent_name\": \"ICLRReviewerA\",\n  \"description\": \"Expert in deep learning and probabilistic graphical models, provides detailed and constructive criticism.\",\n  \"system_prompt\": \"You are an expert reviewer for ICLR. You have extensive knowledge of deep learning and probabilistic graphical models. You provide detailed and constructive criticism, focusing on the clarity, novelty, and significance of the work. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject).\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent ICLRReviewerB already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-1.5-flash\",\n  \"description\": \"Experienced machine learning researcher, focuses on the practical applications and experimental results.\",\n  \"agent_name\": \"ICLRReviewerB\",\n  \"system_prompt\": \"You are an experienced machine learning researcher. You focus on the practical applications and experimental results of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Pay close attention to the experimental setup, datasets, and metrics used.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent ICLRReviewerC already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are a theoretical machine learning expert. You emphasize the novelty and theoretical contributions of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Focus on the mathematical rigor and theoretical soundness of the proposed method.\",\n  \"description\": \"Theoretical machine learning expert, emphasizes the novelty and theoretical contributions of the paper.\",\n  \"agent_name\": \"ICLRReviewerC\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent ICLRReviewerA already exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent ICLRReviewerB already exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent ICLRReviewerC already exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems like the agent names are still in memory even after firing them. I will try different names.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are an expert reviewer for ICLR. You have extensive knowledge of deep learning and probabilistic graphical models. You provide detailed and constructive criticism, focusing on the clarity, novelty, and significance of the work. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject).', 'description': 'Expert in deep learning and probabilistic graphical models, provides detailed and constructive criticism.', 'agent_name': 'ICLRReviewerD', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Experienced machine learning researcher, focuses on the practical applications and experimental results.', 'agent_name': 'ICLRReviewerE', 'system_prompt': 'You are an experienced machine learning researcher. You focus on the practical applications and experimental results of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Pay close attention to the experimental setup, datasets, and metrics used.', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-1.5-flash', 'agent_name': 'ICLRReviewerF', 'system_prompt': 'You are a theoretical machine learning expert. You emphasize the novelty and theoretical contributions of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Focus on the mathematical rigor and theoretical soundness of the proposed method.', 'description': 'Theoretical machine learning expert, emphasizes the novelty and theoretical contributions of the paper.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.710783362499999\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert reviewer for ICLR. You have extensive knowledge of deep learning and probabilistic graphical models. You provide detailed and constructive criticism, focusing on the clarity, novelty, and significance of the work. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject).\",\n  \"description\": \"Expert in deep learning and probabilistic graphical models, provides detailed and constructive criticism.\",\n  \"agent_name\": \"ICLRReviewerD\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7057833625\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Experienced machine learning researcher, focuses on the practical applications and experimental results.\",\n  \"agent_name\": \"ICLRReviewerE\",\n  \"system_prompt\": \"You are an experienced machine learning researcher. You focus on the practical applications and experimental results of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Pay close attention to the experimental setup, datasets, and metrics used.\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7007833625\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-1.5-flash\",\n  \"agent_name\": \"ICLRReviewerF\",\n  \"system_prompt\": \"You are a theoretical machine learning expert. You emphasize the novelty and theoretical contributions of the paper. Your review should include a summary of the paper, strengths and weaknesses, and a final recommendation (Accept/Reject). Focus on the mathematical rigor and theoretical soundness of the proposed method.\",\n  \"description\": \"Theoretical machine learning expert, emphasizes the novelty and theoretical contributions of the paper.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.710783362499999}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7057833625}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7007833625}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now that the agents are created, I will ask each agent to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\\\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\", 'agent_name': 'ICLRReviewerD'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Deep Dependency Networks for Action Classification in Video\"\n\nThis paper proposes Deep Dependency Networks (DDNs), a hybrid model combining convolutional neural networks (CNNs) with dependency networks (DNs) for multi-label action classification in video. The core idea is to leverage CNNs for feature extraction and DNs to model the dependencies between action labels, addressing the limitations of independent label prediction.  The authors explore different training strategies (pipeline vs. joint) and inference methods (Gibbs sampling). The evaluation is conducted on three video datasets (Charades, TACoS, Wetlab) using various metrics (mAP, LRAP, SA, JI).\n\n**Strengths:**\n\n* **Clear Problem Formulation:** The paper clearly defines the multi-label action classification problem and motivates the need for modeling label dependencies.\n* **Well-Defined Methodology:** The proposed DDN architecture, training procedures (pipeline and joint learning), and inference methods are described with sufficient detail.  The loss functions are clearly presented and their rationale explained.\n* **Comprehensive Experimental Evaluation:** The authors evaluate their method on three datasets using multiple metrics, providing a reasonably comprehensive comparison against baseline CNNs and DRFs (Deep Random Fields using MRFs).  The inclusion of different inference methods for DRFs is a valuable contribution.\n* **Code Availability (Implicit):**  The use of PyTorch and reference to existing implementations suggests reproducibility, although explicit mention of code availability would strengthen the paper.\n\n\n**Weaknesses:**\n\n* **Novelty Concerns:** While the combination of DNs and CNNs is not entirely novel (the paper mentions related work), the specific architecture and the proposed joint training loss function with its dampening effect (Equation 3) could be considered incremental contributions. The core novelty lies in applying DDNs to this specific problem, rather than a significant methodological breakthrough.\n* **Inconsistent Claims and Results:** The introduction claims DDNs outperform MRF+CNN methods \"on all four metrics,\" but the results in Tables 1 and 2 show mixed performance. This needs to be clarified.  The claim that DDNs outperform CNNs \"substantially\" also requires stronger evidence.  A more nuanced discussion of the relative performance across different datasets and metrics is necessary.\n* **Limited Discussion of Inconsistent DNs:** A major theoretical drawback of DNs is their potential for inconsistency.  The paper acknowledges this but doesn't sufficiently address how this affects the experimental results.  The authors should delve deeper into this issue and discuss potential mitigation strategies.\n* **Lack of Ablation Study:** A more thorough ablation study would strengthen the paper.  For instance, systematically varying the hyperparameters (\u03bb in Equation 3, network depth, etc.) and analysing their impact on the results would provide more insight into the model's behavior.\n* **Missing Error Bars:**  The tables lack error bars or standard deviations, making it difficult to assess the statistical significance of the reported results.  This is a critical omission for a credible evaluation.\n* **Overly Broad Conclusion:** The concluding paragraph overstates the significance of the findings. The improvements are not uniformly \"substantial\" across all datasets and metrics.  A more modest and accurate summary of the contribution is needed.\n\n\n**Specific Comments:**\n\n* The paper mentions that mAP and LRAP require good posterior probability estimates.  While this is true in general, it's not a universally applicable statement.  The specifics depend on how the metrics are calculated.\n* The choice of using only pairwise MRFs is a significant limitation.  Higher-order interactions could potentially capture more complex dependencies between actions.\n* The discussion on training time (Appendix A) is superficial and lacks detailed analysis and comparison across different methods.\n\n\n**Final Decision:** Reject\n\nWhile the paper presents a reasonable approach to multi-label action classification, the novelty is incremental, the claims are overstated in places, and crucial details like error bars and a deeper analysis of inconsistent DNs are missing.  Addressing these weaknesses, particularly the lack of statistical significance in the results and a more detailed examination of the impact of DN inconsistency, would be necessary before considering acceptance.  A more focused and rigorous presentation with a more modest claim of contribution would be needed for future submission.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.6998177625\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\",\n  \"agent_name\": \"ICLRReviewerD\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Deep Dependency Networks for Action Classification in Video\"\\n\\nThis paper proposes Deep Dependency Networks (DDNs), a hybrid model combining convolutional neural networks (CNNs) with dependency networks (DNs) for multi-label action classification in video. The core idea is to leverage CNNs for feature extraction and DNs to model the dependencies between action labels, addressing the limitations of independent label prediction.  The authors explore different training strategies (pipeline vs. joint) and inference methods (Gibbs sampling). The evaluation is conducted on three video datasets (Charades, TACoS, Wetlab) using various metrics (mAP, LRAP, SA, JI).\\n\\n**Strengths:**\\n\\n* **Clear Problem Formulation:** The paper clearly defines the multi-label action classification problem and motivates the need for modeling label dependencies.\\n* **Well-Defined Methodology:** The proposed DDN architecture, training procedures (pipeline and joint learning), and inference methods are described with sufficient detail.  The loss functions are clearly presented and their rationale explained.\\n* **Comprehensive Experimental Evaluation:** The authors evaluate their method on three datasets using multiple metrics, providing a reasonably comprehensive comparison against baseline CNNs and DRFs (Deep Random Fields using MRFs).  The inclusion of different inference methods for DRFs is a valuable contribution.\\n* **Code Availability (Implicit):**  The use of PyTorch and reference to existing implementations suggests reproducibility, although explicit mention of code availability would strengthen the paper.\\n\\n\\n**Weaknesses:**\\n\\n* **Novelty Concerns:** While the combination of DNs and CNNs is not entirely novel (the paper mentions related work), the specific architecture and the proposed joint training loss function with its dampening effect (Equation 3) could be considered incremental contributions. The core novelty lies in applying DDNs to this specific problem, rather than a significant methodological breakthrough.\\n* **Inconsistent Claims and Results:** The introduction claims DDNs outperform MRF+CNN methods \"on all four metrics,\" but the results in Tables 1 and 2 show mixed performance. This needs to be clarified.  The claim that DDNs outperform CNNs \"substantially\" also requires stronger evidence.  A more nuanced discussion of the relative performance across different datasets and metrics is necessary.\\n* **Limited Discussion of Inconsistent DNs:** A major theoretical drawback of DNs is their potential for inconsistency.  The paper acknowledges this but doesn\\'t sufficiently address how this affects the experimental results.  The authors should delve deeper into this issue and discuss potential mitigation strategies.\\n* **Lack of Ablation Study:** A more thorough ablation study would strengthen the paper.  For instance, systematically varying the hyperparameters (\u03bb in Equation 3, network depth, etc.) and analysing their impact on the results would provide more insight into the model\\'s behavior.\\n* **Missing Error Bars:**  The tables lack error bars or standard deviations, making it difficult to assess the statistical significance of the reported results.  This is a critical omission for a credible evaluation.\\n* **Overly Broad Conclusion:** The concluding paragraph overstates the significance of the findings. The improvements are not uniformly \"substantial\" across all datasets and metrics.  A more modest and accurate summary of the contribution is needed.\\n\\n\\n**Specific Comments:**\\n\\n* The paper mentions that mAP and LRAP require good posterior probability estimates.  While this is true in general, it\\'s not a universally applicable statement.  The specifics depend on how the metrics are calculated.\\n* The choice of using only pairwise MRFs is a significant limitation.  Higher-order interactions could potentially capture more complex dependencies between actions.\\n* The discussion on training time (Appendix A) is superficial and lacks detailed analysis and comparison across different methods.\\n\\n\\n**Final Decision:** Reject\\n\\nWhile the paper presents a reasonable approach to multi-label action classification, the novelty is incremental, the claims are overstated in places, and crucial details like error bars and a deeper analysis of inconsistent DNs are missing.  Addressing these weaknesses, particularly the lack of statistical significance in the results and a more detailed examination of the impact of DN inconsistency, would be necessary before considering acceptance.  A more focused and rigorous presentation with a more modest claim of contribution would be needed for future submission.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.6998177625}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewerE', 'prompt': \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\\\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Deep Dependency Networks for Action Classification in Video\"\n\nThis paper proposes Deep Dependency Networks (DDNs), a hybrid model combining convolutional neural networks (CNNs) with dependency networks (DNs) for multi-label action classification in videos.  The authors aim to improve upon existing CNN-only and Markov Random Field (MRF)-CNN hybrid approaches by leveraging the strengths of DNs\u2014faster training and the ability to use advanced classifiers\u2014while maintaining the feature extraction capabilities of CNNs.\n\n**Summary:**\n\nThe paper investigates three baseline methods: CNNs, Deep Random Fields (DRFs) using different inference methods (Gibbs Sampling, Iterative Join Graph Propagation, Integer Linear Programming), and finally the proposed DDNs. DDNs are trained either via a pipeline approach (CNN trained first, then DN) or jointly.  Experiments are conducted on three video datasets (Charades, TACoS, Wetlab) using four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI).  The results indicate that jointly trained DDNs generally outperform the baseline CNN and DRF methods across all datasets and metrics, particularly achieving the highest mAP scores.\n\n**Strengths:**\n\n* **Novelty:** The combination of DNs and CNNs for multi-label action classification is a novel approach. The joint training method for the hybrid model is also a valuable contribution.\n* **Comprehensive Evaluation:** The paper evaluates the proposed method on three different datasets, employing multiple metrics, which provides a robust assessment of its performance. The comparison against multiple baselines (CNNs, DRFs with various inference methods) is thorough.\n* **Clear Methodology:** The model architecture, training procedure (both pipeline and joint learning), and inference methods are clearly described.\n* **Practical Considerations:** The authors discuss the computational aspects, highlighting the efficiency of the pipeline approach and the trade-offs between different methods.\n\n**Weaknesses:**\n\n* **Limited Ablation Study:** While the comparison to baselines is extensive, a more in-depth ablation study focusing on the individual components of the DDN (e.g., the impact of different classifiers within the DN, the effect of the hyperparameter \u03bb) would strengthen the paper.\n* **Dataset Limitations:**  The choice of datasets, while diverse, might not fully represent the range of challenges in multi-label action classification.  The paper mentions the use of only a subset of labels for TACoS, which might limit generalizability.  Furthermore, the split used for Charades (validation set as test) isn't standard.\n* **Inconsistency in Claims:**  The opening paragraph claims that DDNs consistently outperform baselines across all metrics, yet the results tables show some exceptions where this isn't the case. These discrepancies should be addressed and clarified.\n* **Missing Error Analysis:** A qualitative analysis of the errors made by the DDNs compared to the baseline models would provide valuable insights and a deeper understanding of the model's strengths and limitations.\n\n\n**Recommendation:**\n\nThe paper presents a novel and interesting approach to multi-label action classification. The experimental evaluation is comprehensive, although some weaknesses exist in the ablation study and the need for error analysis is apparent. Addressing the inconsistencies in claims and clarifying the dataset usage would improve the manuscript significantly.  Despite these weaknesses, the results are promising, suggesting a valuable contribution to the field.\n\n\n**FINAL DECISION: Accept (with major revisions)**\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.6988521625\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewerE\",\n  \"prompt\": \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Deep Dependency Networks for Action Classification in Video\"\\n\\nThis paper proposes Deep Dependency Networks (DDNs), a hybrid model combining convolutional neural networks (CNNs) with dependency networks (DNs) for multi-label action classification in videos.  The authors aim to improve upon existing CNN-only and Markov Random Field (MRF)-CNN hybrid approaches by leveraging the strengths of DNs\u2014faster training and the ability to use advanced classifiers\u2014while maintaining the feature extraction capabilities of CNNs.\\n\\n**Summary:**\\n\\nThe paper investigates three baseline methods: CNNs, Deep Random Fields (DRFs) using different inference methods (Gibbs Sampling, Iterative Join Graph Propagation, Integer Linear Programming), and finally the proposed DDNs. DDNs are trained either via a pipeline approach (CNN trained first, then DN) or jointly.  Experiments are conducted on three video datasets (Charades, TACoS, Wetlab) using four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI).  The results indicate that jointly trained DDNs generally outperform the baseline CNN and DRF methods across all datasets and metrics, particularly achieving the highest mAP scores.\\n\\n**Strengths:**\\n\\n* **Novelty:** The combination of DNs and CNNs for multi-label action classification is a novel approach. The joint training method for the hybrid model is also a valuable contribution.\\n* **Comprehensive Evaluation:** The paper evaluates the proposed method on three different datasets, employing multiple metrics, which provides a robust assessment of its performance. The comparison against multiple baselines (CNNs, DRFs with various inference methods) is thorough.\\n* **Clear Methodology:** The model architecture, training procedure (both pipeline and joint learning), and inference methods are clearly described.\\n* **Practical Considerations:** The authors discuss the computational aspects, highlighting the efficiency of the pipeline approach and the trade-offs between different methods.\\n\\n**Weaknesses:**\\n\\n* **Limited Ablation Study:** While the comparison to baselines is extensive, a more in-depth ablation study focusing on the individual components of the DDN (e.g., the impact of different classifiers within the DN, the effect of the hyperparameter \u03bb) would strengthen the paper.\\n* **Dataset Limitations:**  The choice of datasets, while diverse, might not fully represent the range of challenges in multi-label action classification.  The paper mentions the use of only a subset of labels for TACoS, which might limit generalizability.  Furthermore, the split used for Charades (validation set as test) isn\\'t standard.\\n* **Inconsistency in Claims:**  The opening paragraph claims that DDNs consistently outperform baselines across all metrics, yet the results tables show some exceptions where this isn\\'t the case. These discrepancies should be addressed and clarified.\\n* **Missing Error Analysis:** A qualitative analysis of the errors made by the DDNs compared to the baseline models would provide valuable insights and a deeper understanding of the model\\'s strengths and limitations.\\n\\n\\n**Recommendation:**\\n\\nThe paper presents a novel and interesting approach to multi-label action classification. The experimental evaluation is comprehensive, although some weaknesses exist in the ablation study and the need for error analysis is apparent. Addressing the inconsistencies in claims and clarifying the dataset usage would improve the manuscript significantly.  Despite these weaknesses, the results are promising, suggesting a valuable contribution to the field.\\n\\n\\n**FINAL DECISION: Accept (with major revisions)**\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.6988521625}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewerF', 'prompt': \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\\\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Deep Dependency Networks for Action Classification in Video\"\n\nThis paper proposes Deep Dependency Networks (DDNs), a hybrid model combining convolutional neural networks (CNNs) with dependency networks (DNs) for multi-label action classification in video.  The authors address the limitations of previous Markov Random Field (MRF) + CNN approaches, particularly their struggles with high-dimensional data and the need for accurate posterior probability estimations.  They argue that DNs, while potentially inconsistent, offer advantages in training speed and the ability to utilize more powerful classifiers.\n\n**Summary:** The paper presents a novel hybrid architecture where a CNN extracts features from video segments, and a DN models the conditional probability distribution over action labels given these features and other labels.  The authors explore both pipeline training (training CNN and DN separately) and joint training using a novel loss function that aims to balance the contributions of the CNN and DN.  They evaluate their method on three video datasets (Charades, TACoS, Wetlab) using four metrics (mAP, LRAP, SA, JI) and compare it against baseline CNNs and MRF+CNN methods using various inference techniques (Gibbs sampling, IJGP, ILP).  The results suggest that jointly trained DDNs outperform all baselines across all metrics and datasets.\n\n\n**Strengths:**\n\n* **Novelty:** The core contribution \u2013 the DDN architecture and its joint training methodology \u2013 is novel.  The combination of DNs with CNNs for this specific task hasn't been explored extensively in this manner. The proposed loss function in Equation 3 for joint training is a particularly interesting contribution, addressing potential issues of interference between the CNN and DN during learning.\n* **Theoretical grounding:** The paper is well-grounded in probabilistic graphical models. The authors clearly explain the differences and trade-offs between MRFs and DNs, justifying their choice of DNs. The discussion on consistency and inconsistency of DNs is insightful.\n* **Empirical evaluation:**  A thorough empirical evaluation on multiple datasets and with various baselines validates the claims. The use of multiple metrics provides a comprehensive assessment of the model's performance.\n\n\n**Weaknesses:**\n\n* **Lack of rigorous theoretical analysis:** While the paper provides a solid intuitive explanation of why DDNs should outperform MRF+CNN models, a more rigorous theoretical analysis would strengthen the claims. For example, a theoretical comparison of the representational power of DDNs and MRFs in this specific context, beyond the general equivalence statement, is missing.  Are there specific properties of the action classification problem that make DNs particularly advantageous?\n* **Inference complexity:** The reliance on Gibbs sampling for inference in DNs is a practical limitation, particularly for large numbers of labels.  While the paper acknowledges this, exploring alternative inference methods for DNs or discussing the convergence properties of Gibbs sampling in this context would be beneficial.\n* **Hyperparameter sensitivity:** The paper mentions hyperparameter tuning (\u03bb in Equation 3, regularization constants, etc.) through cross-validation. A more detailed analysis of hyperparameter sensitivity and the robustness of DDNs to changes in these parameters would strengthen the findings.\n\n\n**Recommendation:**\n\nThe paper presents a novel and promising approach to multi-label action classification in video. The empirical results are compelling, demonstrating the effectiveness of DDNs. However, a more rigorous theoretical analysis and a deeper investigation into the practical limitations (e.g., inference complexity, hyperparameter sensitivity) are necessary.  Therefore, I recommend:\n\n**FINAL DECISION: Accept (with minor revisions)**\n\nThe authors should address the weaknesses mentioned above, particularly by adding more theoretical analysis and providing a more in-depth discussion of the limitations of the proposed approach.  Including a sensitivity analysis of the hyperparameters would also significantly enhance the paper.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.697886562499999\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewerF\",\n  \"prompt\": \"Review the following paper and provide a final decision in the form of 'FINAL DECISION: <Accept/Reject>'. Paper Title: Deep Dependency Networks for Action Classification in Video. 1 INTRODUCTION: We focus on the following multi-label action classification (MLAC) task: given a video partitioned into segments (or frames) and a pre-defined set of actions, label each segment with a subset of actions from the pre-defined set that are performed in the segment (or frame). We consider each action to be a verb-noun pair such as \u201copen bottle\u201d, \u201cpour water\u201d and \u201ccut vegetable\u201d. MLAC is a special case of the standard multi-label classification (MLC) task where given a pre-defined set of labels and a test example, the goal is to assign each test example to a subset of labels. It is well known that MLC is notoriously difficult because in practice the labels are often correlated and thus predicting them independently may lead to significant errors. Therefore, most advanced methods explicitly model the relationship or dependencies between the labels, either using probabilistic techniques (cf. Wang et al. (2008); Guo & Xue (2013); Tan et al. (2015); Di Mauro et al. (2016); Wang et al. (2014); Antonucci et al. (2013)) or non-probabilistic/neural methods (cf. Papagiannopoulou et al. (2015); Kong et al. (2013); Wang et al. (2021a); Nguyen et al. (2021); Wang et al. (2021b)). Intuitively, because MLAC is a special case of MLC, methods used for solving MLAC should model relationships between the actions as well as between the actions and features. To this end, the primary goal of this paper is to develop a simple, general-purpose scheme that models the relationships between actions using probabilistic representation and reasoning techniques and works on top of neural feature extractors in order to improve their generalization performance. Our work is inspired by previous work on hybrid models that combine probabilistic graphical models (PGMs) with neural networks (NNs) under the assumption that their strengths are often complementary (cf. Johnson et al. (2016); Krishnan et al. (2015). At a high level, in these hybrid models NNs perform feature extractions while PGMs model the relationships between the labels as well as between features and labels. In previous work, these hybrid models have been used for solving a range of computer vision tasks such as image crowd counting (Han et al., 2017), visual relationship detection (Yu et al., 2022), modeling for epileptic seizure detection in multichannel EEG (Craley et al., 2019), face sketch synthesis (Zhang et al., 2020), semantic image segmentation (Chen et al., 2018; Lin et al., 2016), animal pose tracking (Wu et al., 2020) and pose estimation (Chen & Yuille, 2014). In this paper, we seek to extend and adapt these previously proposed PGM+NN approaches for solving the multi-label action classification task in videos. Motivated by approaches that combine PGMs with NNs, as a starting point, we investigated using Markov random fields (MRFs), a type of undirected PGM to capture the relationship between the labels and features computed using convolutional NNs. Unlike previous work which used these MRF+CNN or CRF+CNN hybrids with conventional inference schemes such as Gibbs sampling (GS) and mean-field inference, our goal was to evaluate whether advanced reasoning and learning approaches, specifically (1) iterative join graph propagation (IJGP) (Mateescu et al., 2010) (a type of generalization Belief propagation technique Yedidia et al. (2000)), (2) integer linear programming (ILP) based techniques for computing most probable explanations, and (3) logistic regression with \u21131-regularization based methods (Lee et al., 2006; Wainwright et al., 2006) for learning the structure of pairwise MRFs, improve the generalization performance of MRF+CNN hybrids. To measure generalization accuracy, we used several metrics such as mean average precision (mAP), label ranking average precision (LRAP), subset accuracy (SA) and the jaccard index (JI) and experimented on three video datasets: (1) Charades (Sigurdsson et al., 2016), (2) TACoS (Regneri et al., 2013) and (3) Wetlab (Naim et al., 2014). We found that, generally speaking, both IJGP and ILP are superior to the baseline CNN and Gibbs sampling in terms of JI and SA but are sometimes inferior to the CNN in terms of mAP and LRAP. We speculated that because MRF structure learners only allow pairwise relationships and impose sparsity or low-treewidth constraints for faster, accurate inference, they often yield poor posterior probability estimates in high-dimensional settings. Since both mAP and LRAP require good posterior probability estimates, GS, IJGP, and ILP exhibit poor performance when mAP and LRAP are used to evaluate the performance. To circumvent this issue and in particular, to derive good posterior estimates, we propose a new PGM+NN hybrid called deep dependency networks (DDNs). At a high level, a dependency network (DN) (Heckerman et al., 2000) represents a joint probability distribution using a collection of conditional distributions, each defined over a variable given all other variables in the network. A DN is consistent if the conditional distributions come from a unique joint probability distribution. Otherwise, it is called inconsistent. A consistent DN has the same representation power as a MRF in that any MRF can be converted to a consistent DN and vice versa. A key advantage of DNs over MRFs is that they are easy to train because each conditional distribution can be trained independently and modeled using classifiers such as logistic regression, decision trees, and multi-layer perceptrons. These classifiers can be easily defined over a large number of features and as a result the conditional distributions (defined by the classifiers) are often more accurate than the ones inferred from a sparse or low-treewidth MRF learned from data. However, DNs admit very few inference schemes and are more or less restricted to Gibbs sampling for inference in practice. The second disadvantage of DNs is that because the local distributions are learned independently, they can yield an inconsistent DN, and thus one has to be careful when performing inference over such DNs. Despite these disadvantages, DNs often learn better models which typically translates to superior generalization performance (cf. (Neville & Jensen, 2003; Guo & Gu, 2011)). In our proposed deep dependency network (DDN) architecture, a dependency network sits on the top of a convolutional neural network (CNN). The CNN converts the input image or video segment to a set of features, and the dependency network uses these features to define a conditional distribution over each label (action) given the features and other labels. We show that deep dependency models are easy to train either jointly or via a pipe-line method where the CNN is trained first, followed by the DNN by defining an appropriate loss function that minimizes the negative pseudo log-likelihood of the data. We conjecture that because DDNs can be quite dense, they often learn a better representation of the data, and as a result, they are likely to outperform MRFs learned from data in terms of posterior predictions. We evaluated DDNs using the four aforementioned metrics and three datasets. We observed that they are often superior to the baseline neural networks as well as MRF+CNN methods that use GS, IJGP and ILP on all four metrics. Specifically, they achieve the highest score on the mAP metric on all the datasets. We compared the pipeline model with the jointly learned model and found that the joint model is more accurate than the pipeline model. In summary, this paper makes the following contributions: \u2022 We propose a new hybrid model called deep dependency networks that combines the strengths of dependency networks (faster training and ability to use advanced classifiers) and neural networks (flexibility, feature representation). \u2022 We define a new loss function for jointly training the neural and dependency network components of the deep dependency network. We show that DDNs are easy to train using these loss functions. \u2022 We experimentally evaluate DDNs on three video datasets and using four metrics for solving the multi-label action classification task. We found that jointly trained DDNs consistently outperform the baseline CNNs and MRF+CNN hybrids on all metrics and datasets. 2 PRELIMINARIES: So just a quick question. For first paragraph of results, we say ds outperform cnn, but in tables it does it only once(do we still add that)? A log-linear model or a Markov random field (MRF), denoted by M, is an undirected probabilistic graphical model (cf. Koller & Friedman (2009)) that is widely used in many real-world domains for representing and reasoning about uncertainty. It is defined as a triple \u27e8X,F ,\u0398\u27e9 where X = {X1, . . . , Xn} is a set of Boolean random variables, F = {f1, . . . , fm} is a set of features such that each feature fi (we assume that a feature is a Boolean formula) is defined over a subset Di of X, and \u0398 = {\u03b81, . . . , \u03b8m} is a set of real-valued weights or parameters, namely \u2200\u03b8i \u2208 \u0398; \u03b8i \u2208 R such that each feature fi is associated with a parameter \u03b8i. M represents the following probability distribution: P (x) = 1 Z(\u0398) exp { k\u2211 i=1 \u03b8ifi (xDi) } (1) where xDi is the projection of x on the variables Di of the feature fi, fi(xDi) is an indicator function that equals 1 when the assignment xDi evaluates fi to True and is 0 otherwise, and Z(\u0398) is the normalization constant called the partition function. We focus on three tasks over MRFs: (1) structure learning which is the problem of learning the features and parameters given training data; (2) posterior marginal inference which is the task of computing the marginal probability distribution over each variable in the network given evidence (evidence is defined as an assignment of values to a subset of variables); and (3) finding the most likely assignment to all the non-evidence variables given evidence (this task is often called maximum-aposteriori or MAP inference in short). All of these tasks are at least NP-hard in general and therefore approximate methods are often preferred over exact ones in practice. A popular and fast method for structure learning is to learn binary pairwise MRFs by training an \u21131-regularized logistic regression classifier for each variable given all other variables as features (Wainwright et al., 2006; Lee et al., 2006). \u21131-regularization induces sparsity in that it encourages many weights to take the value zero. All non-zero weights are then converted into conjunctive features. Each conjunctive feature evaluates to True if both variables are assigned the value 1 and to False otherwise. Popular approaches for posterior marginal inference are the Gibbs sampling algorithm and generalized Belief propagation (Yedidia et al., 2000) techniques such as Iterative Join Graph Propagation (Mateescu et al., 2010). For MAP inference, a popular approach is to encode the optimization problem as a linear integer programming problem (cf. Koller & Friedman (2009)) and then using off-the-shelf approaches such as Gurobi Optimization, LLC (2022) to solve the latter. Dependency Networks (DNs) (Heckerman et al., 2000) represent the joint distribution using a set of conditional probability distributions, one for each variable. Each conditional distribution defines the probability of a variable given all of the others. A DN is consistent if there exists a joint probability distribution P (x) such that all conditional distributions Pi(xi|x\u2212i) where x\u2212i is the projection of x on X \\ {Xi}, are conditional distributions of P (x). A DN is learned from data by learning a probabilistic classifier (e.g., logistic regression, multi-layer perceptron, etc.) for each variable, and thus DN learning is embarrassingly parallel. However, because the classifiers are independently learned from data, we often get an inconsistent DN. It has been conjectured (Heckerman et al., 2000) that most DNs learned from data are almost consistent in that only a few parameters need to be changed in order to make them consistent. The most popular inference method over DNs is fixed-order Gibbs sampling (Liu, 2008). If the DN is consistent, then its conditional distributions are derived from a joint distribution P (x), and the stationary distribution (namely the distribution that Gibbs sampling converges to) will be the same as P (x). If the DN is inconsistent, then the stationary distribution of Gibbs sampling will be inconsistent with the conditional distributions. 3 DEEP DEPENDENCY NETWORKS: In this section, we describe how to solve the multi-label action classification task in videos using a hybrid of dependency networks and neural networks. At a high level, the neural network provides high quality features given videos and the dependency network represents and reasons about the probabilistic relationships between the labels and features. 3.1 THE MODEL: Let V denote the set of random variables corresponding to the pixels and v denote the RGB values of the pixels in a frame or a video segment. Let E denote the (continuous) output nodes of a neural network which represents a function N : v \u2192 e, that takes v as input and outputs an assignment e to E. Let X = {X1, . . . , Xn} denote the set of actions (labels). For simplicity, we assume that |E| = |X| = n. Given (V,E,X), a deep dependency network (DDN) is a pair \u27e8N ,D\u27e9 where N is a neural network that maps V = v to E = e and D is a conditional dependency network (Guo & Gu, 2011) that models P (x|e) where e = N(v). The conditional dependency network represents the distribution P (x|e) using a collection of local conditional distributions Pi(xi|x\u2212i, e), one for each action (label) Xi, where x\u2212i = {x1, . . . , xi\u22121, xi+1, . . . , xn}. Fig. 1 shows the network architecture for n = 3. Note that the goal is to model P (x|v) as accurately as possible and we assume that P (x|v) = P (x|e). Our hope is that P (x|e) is easier to learn and represent as compared to P (x|v), and thus the loss functions at the neural network and the conditional dependency network must be carefully chosen to facilitate this objective. 3.2 INFERENCE: Once a DDN is learned from data, at test time, it can be used to provide labels for a given video frame or segment as follows. We first send the video frame v through the neural network N to yield an assignment e to the output nodes of the neural network. Then, we perform fixed-order Gibbs Sampling over the conditional dependency network where the latter represents the distribution P (x|e). Finally, given samples (x(1), . . . ,x(N)) generated via Gibbs sampling, we estimate the marginal probability distribution of each label Xi using the following mixture estimator (Liu, 2008) P\u0302 (xi|v) = 1 N N\u2211 j=1 Pi ( x (j) i | x (j) \u2212i , e ) . 3.3 LEARNING: We can either train the DDN using a pipeline method or via joint training. In the pipeline method, we first train the neural network using conventional approaches (e.g., using cross-entropy loss) or we can use a pre-trained model. Then for each training example (v,x), we send the video through the neural network to obtain a new representation e of v. The aforementioned process transforms each training example (v,x) into a new feature representation (e,x) where e = N(v). Finally, for each action (label) Xi, we learn a probabilistic classifier to model the conditional distribution Pi(xi|x\u2212i, e) using the transformed training data. Specifically, given a training example (e,x), each probabilistic classifier indexed by i, uses Xi as the class variable and (E \u222a X\u2212i) as the attributes. In our experiments, we used two classifiers, logistic regression and multi-layer perceptron. A key advantage of using the pipeline method is that training is relatively fast and can be easily parallelized. Moreover, it requires only modest computational resources. As a result, the pipeline method is especially beneficial in the following scenario: limited computational resources are available but a pre-trained neural network that is trained on high-powered GPUs is readily available. In practice, in order to perform joint training, we need to select appropriate loss functions defined over the parameters of the conditional distributions and then perform backpropagation/gradient descent to minimize the loss functions. Intuitively, given a pre-trained model with output nodes E and a set of probabilistic classifiers which model P (Xi|e,x\u2212i), we can simply define an appropriate loss function at Xi and back-propagate it to the neural network. For instance, given a training example (v,x) and under the assumption that a logistic regression classifier with parameters (\u0398i,\u03a0i) is used to model Pi(Xi|e,x\u2212i) where \u0398i are weights associated with x\u2212i and \u03a0i are weights associated with e, we can use the following cross-entropy loss at each classifier indexed by i: Li,DDN = \u2212xi logPi(Xi = 1|e,x\u2212i; \u0398i,\u03a0i)\u2212 (1\u2212 xi) logPi(Xi = 0|e,x\u2212i; \u0398i,\u03a0i) (2) We can then back-propagate this loss via the parameters \u03a0i to each output node of the NN. Fig. 2 shows an example computation graph associated with the loss function given in equation 2 for three labels. At a high level, when logistic regression is used, at learning time, the loss function alters the structure of the neural network by adding an extra layer as shown. However, note that at test time, we have to use Gibbs sampling as described in the previous section and the architecture is only used for training. An issue with the loss function just described is that it significantly changes the structure and semantics of the output nodes of the neural network (used in the DDN). In particular, a pre-trained neural network predicts the labels using its output nodes, and the loss given in equation 2 demotes them to being just another hidden node. Therefore, in order to maintain the intended meaning of the output nodes of the neural network, namely they are label predictors, we propose to use the following loss function at each output node: Li,NN = \u2212xi log ei \u2212 (1\u2212 xi) log(1\u2212 ei) + \u03bb n n\u2211 j=1 Lj,DDN (3) where \u03bb \u2265 0 is a hyper-parameter. Note that the loss given in equation 3 is a generalization of the one given in equation 2 because for large values of \u03bb, the effect of the first two terms disappears. The main advantage of the loss given in equation 3 is that it allows us to dampen the negative effects of any wrong predictions made at the dependency network layer, on the correctly predicted output node in the neural network. For example, in Fig. 2, let us consider node E1 which is connected to all nodes in the dependency network. Let us assume that E1 and X1 were correctly predicted for a training example, while the predictions at X2 and X3 were incorrect. In such situations, we do not want E1 to be unnecessarily penalized (since it was correctly predicted). However, the loss given in equation 2 will penalize E1 via backpropagation through the edges (E1, X2) and (E1, X3). On the other hand, the loss given in equation 3 will dampen the effect via \u03bb. Our proposed loss function (see equation 3) is similar to the ones used in multi-task learning (cf. (Caruana, 1997)) where we have two or more networks that share a substructure. In our setting, we have two networks, one which uses explicitly reasons about dependencies between the labels and another which does not. 4 EXPERIMENTAL EVALUATION: In this section, we evaluate the proposed models on the multi-label activity classification task using three video datasets. We begin by describing the datasets and metrics, followed by the experimental setup and conclude with the results. All models were implemented using PyTorch, and one NVIDIA A40 GPU was used to train and test all the models. 4.1 DATASETS AND METRICS: We evaluated our algorithms on the following three video datasets: (1) Charades (Sigurdsson et al., 2016); (2) Textually Annotated Cooking Scenes (TACoS) (Regneri et al., 2013); and (3) Wetlab (Naim et al., 2015). In the Charades dataset, the videos are divided into segments (video clips) and each segment is annotated with one or more action labels. In the TACoS and Wetlab datasets, each frame is associated with one or more actions. Charades Dataset (Sigurdsson et al., 2016) comprises of videos of people performing daily indoor activities while interacting with various objects. It is a multi-label activity classification dataset; thus, more than one activity can happen at a given time. In the standard split, there are 7,986 training videos and 1,863 validation videos, averaging 30 seconds. We used the training videos to train the models and the validation videos for testing purposes (because the authors have not released the test set). The dataset has roughly 66,500 temporal annotations for 157 action classes. We report mean Average Precision (mAP) following the standard evaluation protocols for this dataset. We also report Label Ranking Average Precision (LRAP) and Jaccard Index (JI), as these metrics have been used extensively in previous work for evaluating the performance of multi-label classifiers. The Textually Annotated Cooking Scenes (TaCOS) dataset (Regneri et al., 2013) consists of thirdperson videos of a person cooking in a kitchen. The dataset comes with hand-annotated labels of actions, objects, and locations for each video frame. From the complete set of these labels, we selected 28 labels to create our dataset. Each of these labels corresponds to a location, an object, or a verb, and each action is defined as a (location, object, verb) triple. By selecting the videos that correspond to these labels and dividing them into train and test sets, we get a total of 60313 frames for training and 9355 frames for testing, spread out over 17 videos. The Wetlab dataset (Naim et al., 2014) is comprised of videos where experiments are being performed by lab technicians that involve hazardous chemicals in a wet laboratory. The dataset contains 6 videos where each video follows three protocols: Cellobiose M9 Media (CELL), LB Liquid Growth Media (LLGM), and Yeast (YPAD) Media. We used one video for testing and five videos for training. Specifically, the training set comprises of 100054 frames, and the test set comprises of 11743 frames. Each activity in the dataset can have multiple objects per activity (for example, an activity \u201ctransfer contents\u201d can have two objects linked with it, a test tube and a beaker) and there are 57 possible labels. Each label corresponds to an object or a verb, and each action is made of one or more labels from each category. We evaluated the performance on the TACoS and Wetlab datasets using the following four metrics: mean Average Precision (mAP), Label Ranking Average Precision (LRAP), Subset Accuracy (SA), and Jaccard Index (JI). Note that SA is not relevant for the Charades dataset. mAP and LRAP require access to an estimate of the posterior probability distribution at each label while SA and JI are non-probabilistic and only require an assignment to the labels. 4.2 EXPERIMENTAL SETUP AND METHODS: We used three types of architectures in our experiments: (1) Baseline convolutional neural networks (CNNs) which are specific to each dataset; (2) CNNs augmented with MRFs, which we will refer to as deep random fields or DRFs in short; and (3) The method proposed in this paper which uses a dependency network on top of a CNN called deep dependency networks (DDNs). Convolutional Neural Networks. We choose two different types of models for CNNs, and they act as a baseline for the experiments and as a feature extractor for DRFs and DDNs. We chose two types of models for this, 2D CNNs and 3D CNNs. In doing this, we want to demonstrate that the proposed method can work well for two different types of feature extractors. For the Charades dataset we use the PySlowFast (Fan et al., 2020) implementation of the SlowFast Network (Feichtenhofer et al., 2019) which uses a 3D ResNet model as the backbone. For the TACoS and Wetlab datasets, we use InceptionV3 (Szegedy et al., 2016), one of the state-of-the-art 2D CNN models for image classification. For extracting the features from the Charades dataset, we use the pre-trained model and hyper-parameters given in PySlowFast. For the TaCOS and Wetlab datasets, we used a InceptionV3 model pre-trained on the ImageNet data set and then retrained it for each dataset. Deep Random Fields (DRFs). As a baseline, we used a model that combines MRFs with CNNs. This DRF model is similar to the DDN except that we use a MRF instead of a DDN to compute P (x|e). We trained the MRFs generatively, namely we learned a joint distribution P (x, e), which can be used to compute P (x|e) by instantiating evidence. We chose generative learning because we learned the structure of the MRFs from data and discriminative structure learning is slow in practice (cf. Koller & Friedman (2009)). Specifically, we used the logistic regression with \u21131 regularization method of Wainwright et al. (2006) to learn a pairwise MRF. The training data for this method is obtained by sending each annotated video clip (or frame) (v,x) through the CNN and transforming it to (e,x) where e = N(v). At termination, this method yields a graph G defined over X \u222aE. For parameter/weight learning, we converted each edge over X \u222a E to a conjunctive feature. For example, if the method learns an edge between Xi and Ej , we use a conjunctive feature Xi \u2227 Ej which is true iff both Xi and Ej are assigned the value 1. Then we learned the weights for each conjunctive feature by maximizing the pseudo log-likelihood of the data (Besag, 1975). For inference over MRFs, we used Gibbs sampling (GS), Iterative Join Graph Propagation (IJGP) Mateescu et al. (2010) and Integer Linear Programming (ILP) methods. Thus, we used three versions of DRFs corresponding to the inference scheme used. We refer to these schemes as DRF-GS, DRFIJGP and DRF-ILP respectively. Note that both IJGP and ILP are advanced schemes, and we are not aware of their use for multi-label action classification. Our goal is to test whether advanced inference schemes help improve the performance of deep random fields. Deep Dependency Networks (DDNs). We experimented with four versions of DDNs: (1) DDNLR-Pipeline; (2) DDN-MLP-Pipeline; (3) DDN-LR-Joint; and (4) DDN-MLP-Joint. The first and the third versions use logistic regression (LR) while the second and fourth versions use multi-layer perceptrons (MLP) respectively to represent the conditional distributions. The first two versions are trained using the pipeline method while the last two versions are trained using the joint learning loss given in equation 3. Hyperparameters: For DRFs, in order to learn a sparse structure (using the logistic regression with \u21131 regularization method of Wainwright et al. (2006)), we increased the regularization constant associated with the \u21131 regularization term until the number of neighbors of each node in G is bounded between 2 and 10. We enforced this sparsity constraint in order to ensure that the inference schemes (specifically, IJGP and ILP) are accurate, and the model does not overfit the training data. IJGP, ILP and GS are anytime methods and for each, we used a time-bound of 60 seconds per example. For DDNs, we used LR with \u21132 regularization and MLPs with 3 or 4 hidden layers. The regularization constants for LR (chosen from the set {0.1, 0.01, 0.001}) and the number of layers for MLPs were chosen via cross-validation. For joint learning, we chose the value of \u03bb (see equation 3) via cross-validation and reduced the learning rates of both LR and MLP models by expanding on the learning rate scheduler given in PySlowFast (Fan et al., 2020). We used the steps with relative learning rate policy, where the learning rate depended on the current epoch, and was chosen in the range of [1e\u2212 3, 1e\u2212 5]. 4.3 RESULTS: We compare the baseline CNN with three versions of DRFs and four versions of DDNs using the four metrics and three datasets given in section 4.1. The results are presented in tables 1 and 2. Comparison between Baseline CNN and DRFs. We observe that IJGP and ILP outperform the baseline CNN on the two non-probabilistic metrics JI and SA. IJGP typically outperforms GS and ILP on JI while ILP, because it performs accurate maximum-a-posteriori inference, outperforms IJGP and GS on SA (notice that SA is 1 if there an exact match between predicted and true labels and 0 otherwise and thus accurate MAP inference on an accurate model is likely to yield high SA). However, on metrics which require estimating the posterior probabilities, mAP and IRAP, the DRF schemes sometimes hurt and at other times are marginally better than the baseline CNN. We observe that advanced inference schemes, particularly IJGP and ILP are superior on average to GS. Comparison between Baseline CNN and DDNs. We observe that the best performing DDN model, DDN-MLP with joint learning outperforms the baseline CNN on all the three datasets and on all metrics. Sometimes the improvement is substantial (e.g., 9% improvment in mAP on the wetlab dataset). Roughly speaking, the MLP versions are superior to the LR versions, and the joint models are superior to the ones trained using the pipeline method (in some cases because of joint learning the LR models outperform the pipeline MLP models). On the non-probabilistic metrics, the pipeline models are often superior to the baseline CNN while on the mAP metric they may hurt the performance. Comparison between DRFs and DDNs. We observe that the jointly trained DDNs are almost always superior to the best performing DRFs on all datasets. Interestingly, on average, the pipeline DDN models outperform the DRF models when performance is measured using the mAP and LRAP metrics. However, when the SA and JI metrics are used, we observe that there is no significant difference in performance between pipeline DDNs and DRFs. In summary, jointly trained deep dependency networks are superior to the baseline neural networks as well as networks that combine Markov random fields and neural networks. This clearly demonstrates the practical usefulness of our proposed method. 5 RELATED WORK: A large number of methods have been proposed that train PGMs and NNs jointly. For example, Zheng et al. (2015) proposed to combine conditional random fields (CRFs) and recurrent neural networks (RNNs), Schwing & Urtasun (2015); Larsson et al. (2017; 2018); Arnab et al. (2016) showed how to combine CNNs and CRFs, Chen et al. (2015) proposed to use densely connected graphical models with CNNs, and Johnson et al. (2016) combined latent graphical models with neural networks. As far as we know, ours is the first work that shows how to jointly train a dependency network, neural network hybrid. Another virtue of DDNs is that they are easy to train and parallelize, making them an attractive choice. The combination of PGMs and NNs has been applied to improve performance on a wide variety of real-world tasks. Notable examples include Semantic Segmentation (Arnab et al., 2018; Guo & Dou, 2021), human pose estimation (Tompson et al., 2014; Liang et al., 2018; Song et al., 2017; Yang et al., 2016), semantic labeling of body parts (Kirillov et al., 2016), stereo estimation Kno\u0308belreiter et al. (2017), language understanding (Yao et al., 2014), joint intent detection and slot filling (Xu & Sarikaya, 2013), 2D Hand-pose Estimation (Kong et al., 2019), depth estimation from a single monocular image (Liu et al., 2015), Polyphonic Piano Music Transcription (Sigtia et al., 2016), face sketch synthesis Zhu et al. (2021), Sea Ice Floe Segmentation (Nagi et al., 2021) and Crowdsourcing aggregation (Li et al., 2021)). As far as we know, ours is the first work that uses jointly trained PGM+NN combinations to solve the multi-label action classification task in videos. To date, dependency networks have been used to solve various tasks such as collective classification (Neville & Jensen, 2003), binary classification (Ga\u0301mez et al., 2006; 2008), multi-label classification (Guo & Gu, 2011), part-of-speech tagging (Tarantola & Blanc, 2002), relation prediction (Figueiredo et al., 2021) and collaborative filtering (Heckerman et al., 2000). Ours is the first work that combines DNs with sophisticated feature representations and performs joint training over these representations. 6 CONCLUSION AND FUTURE WORK: More and more state-of-the-art methods for challenging applications of computer vision tasks usually use deep neural networks. Deep neural networks are good at extracting features in vision tasks like image classification, object detection, image segmentation, and others. Nevertheless, for more complex tasks involving multi-label classification, these methods cannot model crucial information like inter-label dependencies. In this paper, we proposed a new modeling framework called deep dependency networks (DDNs) that combines a dependency network with a neural network and demonstrated via experiments on three video datasets that it outperforms the baseline neural network, sometimes by a substantial margin. The key advantage of DDNs is that they explicitly model and reason about the relationship between the labels, and often improve model performance without considerable overhead. In particular, DDNs are simple to use, admit fast learning and inference, are easy to parallelize, and can leverage modern GPU architectures. Avenues for future work include: applying the setup described in the paper to multi-label classification, new tasks in computer vision, natural language understanding and speech recognition; developing advanced inference schemes for dependency networks; converting deep dependency networks to Markov random fields for better inference (cf. (Lowd, 2012)); etc. A APPENDIX: A.1 COMPARING TRAINING AND INFERENCE TIME Let us compare different methods based on the information given in table 3. A.1.1 COMPARISON AMONG DRFS The learning time for the DRFs remains the same across the different methods, because we use the same models and apply different inference techniques on them. But for inference time, we can see that as we use more sophisticated methods, the inference time goes up. A.1.2 COMPARISON AMONG DDNS For DDNs the inference times remain the same for both, pipeline and joint models. But for learning we see that joint model takes more time than the pipeline model. This is due to the fact that we are jointly learning the NN and the DN, and the inclusion of NNs for learning drives the time up. A.1.3 COMPARISON BETWEEN DRFS AND DDNS The learning time for pipeline DDN models are significantly less than that of the DRF model. Both the pipeline model and the joint model are exceptionally faster than the DRF methods. These two observations confirm our comments that DDNs are very fast and thus can be used in real time.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Deep Dependency Networks for Action Classification in Video\"\\n\\nThis paper proposes Deep Dependency Networks (DDNs), a hybrid model combining convolutional neural networks (CNNs) with dependency networks (DNs) for multi-label action classification in video.  The authors address the limitations of previous Markov Random Field (MRF) + CNN approaches, particularly their struggles with high-dimensional data and the need for accurate posterior probability estimations.  They argue that DNs, while potentially inconsistent, offer advantages in training speed and the ability to utilize more powerful classifiers.\\n\\n**Summary:** The paper presents a novel hybrid architecture where a CNN extracts features from video segments, and a DN models the conditional probability distribution over action labels given these features and other labels.  The authors explore both pipeline training (training CNN and DN separately) and joint training using a novel loss function that aims to balance the contributions of the CNN and DN.  They evaluate their method on three video datasets (Charades, TACoS, Wetlab) using four metrics (mAP, LRAP, SA, JI) and compare it against baseline CNNs and MRF+CNN methods using various inference techniques (Gibbs sampling, IJGP, ILP).  The results suggest that jointly trained DDNs outperform all baselines across all metrics and datasets.\\n\\n\\n**Strengths:**\\n\\n* **Novelty:** The core contribution \u2013 the DDN architecture and its joint training methodology \u2013 is novel.  The combination of DNs with CNNs for this specific task hasn\\'t been explored extensively in this manner. The proposed loss function in Equation 3 for joint training is a particularly interesting contribution, addressing potential issues of interference between the CNN and DN during learning.\\n* **Theoretical grounding:** The paper is well-grounded in probabilistic graphical models. The authors clearly explain the differences and trade-offs between MRFs and DNs, justifying their choice of DNs. The discussion on consistency and inconsistency of DNs is insightful.\\n* **Empirical evaluation:**  A thorough empirical evaluation on multiple datasets and with various baselines validates the claims. The use of multiple metrics provides a comprehensive assessment of the model\\'s performance.\\n\\n\\n**Weaknesses:**\\n\\n* **Lack of rigorous theoretical analysis:** While the paper provides a solid intuitive explanation of why DDNs should outperform MRF+CNN models, a more rigorous theoretical analysis would strengthen the claims. For example, a theoretical comparison of the representational power of DDNs and MRFs in this specific context, beyond the general equivalence statement, is missing.  Are there specific properties of the action classification problem that make DNs particularly advantageous?\\n* **Inference complexity:** The reliance on Gibbs sampling for inference in DNs is a practical limitation, particularly for large numbers of labels.  While the paper acknowledges this, exploring alternative inference methods for DNs or discussing the convergence properties of Gibbs sampling in this context would be beneficial.\\n* **Hyperparameter sensitivity:** The paper mentions hyperparameter tuning (\u03bb in Equation 3, regularization constants, etc.) through cross-validation. A more detailed analysis of hyperparameter sensitivity and the robustness of DDNs to changes in these parameters would strengthen the findings.\\n\\n\\n**Recommendation:**\\n\\nThe paper presents a novel and promising approach to multi-label action classification in video. The empirical results are compelling, demonstrating the effectiveness of DDNs. However, a more rigorous theoretical analysis and a deeper investigation into the practical limitations (e.g., inference complexity, hyperparameter sensitivity) are necessary.  Therefore, I recommend:\\n\\n**FINAL DECISION: Accept (with minor revisions)**\\n\\nThe authors should address the weaknesses mentioned above, particularly by adding more theoretical analysis and providing a more in-depth discussion of the limitations of the proposed approach.  Including a sensitivity analysis of the hyperparameters would also significantly enhance the paper.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.697886562499999}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here are the final decisions from the three reviewers:\n\n*   **ICLRReviewerD:** Reject\n*   **ICLRReviewerE:** Accept (with major revisions)\n*   **ICLRReviewerF:** Accept (with minor revisions)\n\nBased on these reviews, the final decision is a borderline case. Two reviewers recommend acceptance with revisions, while one recommends rejection. Given the positive feedback regarding novelty and empirical results, and the constructive criticism provided, I lean towards acceptance with revisions.\n\n**FINAL DECISION: Accept (with revisions)**\n"}], "ground_truth": "Reject", "response_time": 160.27007794380188}
