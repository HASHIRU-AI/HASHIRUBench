{"paper_id": "b8f2YGWebo", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Self-supervised Continual Learning based on Batch-mode Novelty Detection\n\nNone: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: "}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper and provide a final decision. First, I need to check the available agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {\n    \"CriticalReviewer\": {\n      \"description\": \"A highly critical reviewer with expertise in continual learning and novelty detection. Focuses on identifying weaknesses and potential flaws in the methodology and experiments.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    },\n    \"BalancedReviewer\": {\n      \"description\": \"A balanced reviewer with expertise in continual learning. Provides a fair assessment of the paper, highlighting both strengths and weaknesses.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    },\n    \"PositiveReviewer\": {\n      \"description\": \"An enthusiastic reviewer who focuses on the positive aspects of the paper, highlighting its contributions and potential impact.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {'CriticalReviewer': {'description': 'A highly critical reviewer with expertise in continual learning and novelty detection. Focuses on identifying weaknesses and potential flaws in the methodology and experiments.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}, 'BalancedReviewer': {'description': 'A balanced reviewer with expertise in continual learning. Provides a fair assessment of the paper, highlighting both strengths and weaknesses.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}, 'PositiveReviewer': {'description': 'An enthusiastic reviewer who focuses on the positive aspects of the paper, highlighting its contributions and potential impact.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I already have three agents that fit the description: `PositiveReviewer`, `BalancedReviewer`, and `CriticalReviewer`. Now I need to check the budget before invoking them.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 0.21310211500000023,\n    \"current_remaining_expense_budget\": 9.786897885\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 0.21310211500000023, 'current_remaining_expense_budget': 9.786897885}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The expense budget is 9.78, and each agent invocation costs 0.00017. I can invoke all three agents without exceeding the budget. I will now invoke each agent with the paper abstract and ask for a review and recommendation.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'PositiveReviewer', 'prompt': 'Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Paper Title: Self-supervised Continual Learning based on Batch-mode Novelty Detection. Abstract: None: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: Please finish the review and give the FINAL DECISION line.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Self-supervised Continual Learning based on Batch-mode Novelty Detection\n\nThis paper presents a novel and exciting approach to continual learning (CL) by unifying it with novelty detection (ND).  The authors propose a self-supervised method that tackles both ND and CL simultaneously, addressing a crucial limitation in existing CL methods that rely on labeled data for new tasks.  The core idea of sequentially adding binary classifiers to a base model, trained in a batch-mode fashion, is particularly ingenious and shows great promise.\n\n**Strengths:**\n\n* **Novelty and Significance:** The unification of ND and CL in a self-supervised framework is a significant contribution. This addresses a critical practical limitation of many CL approaches \u2013 the reliance on labeled data for new tasks, which is often unavailable in real-world scenarios.  The proposed method elegantly circumvents this issue by learning new classes one at a time, leveraging the novelty detection mechanism for unsupervised class separation and then adding a specialized binary classifier.  This incremental approach contrasts sharply with existing methods that tackle multiple classes per task and greatly enhances the practicality of CL.\n\n* **Technical Soundness:** The methodology is well-explained and the technical details are presented clearly. The use of the Mahalanobis distance in the gradient space for novelty detection is a sound approach, and the incorporation of a self-supervised binary classifier to refine the detection process is a clever innovation.  The batch-mode training and inference strategy further enhances the robustness and accuracy of the classifier, addressing the potential for error accumulation in sequential classification.  The algorithm descriptions (Algorithms 1-4) are detailed and easily understandable.\n\n* **Empirical Validation:** The experimental results are comprehensive and convincingly demonstrate the effectiveness of the proposed method across various datasets (MNIST, CIFAR-10, CIFAR-100, TinyImageNet). The consistent outperformance of state-of-the-art CL methods, particularly in the single-head evaluation, is impressive and highlights the significant advantages of the unified framework. The ablation studies further solidify the contribution by showing the impact of key design choices, such as batch size and IDD/OOD mixture ratios. The figures effectively visualize the key concepts and results.\n\n\n* **Clear Writing and Presentation:** The paper is well-written and easy to follow. The authors effectively motivate the problem, present their approach, and interpret the results. The figures and tables are clear and well-integrated into the text.\n\n\n**Weaknesses:**\n\n* **Scalability:** While the incremental approach is advantageous, a thorough discussion of the scalability of the method to a very large number of classes is warranted. The accumulation of binary classifiers could potentially lead to computational overhead and increased memory consumption.  Addressing this concern with either theoretical analysis or further experiments would strengthen the paper.\n\n* **Comparison with Unsupervised CL methods:**  The paper focuses heavily on comparisons with supervised CL methods. A more direct comparison with existing unsupervised or self-supervised CL methods would provide a clearer picture of the relative advantages of the proposed approach.\n\n* **Generalization to other types of novelty:** The current novelty detection method relies on gradient analysis.  Discussion on the applicability of this method to different types of novelty (e.g., concept drift, adversarial examples) would be beneficial.\n\n\n**Recommendation:**\n\nThis paper presents a highly novel and impactful contribution to the field of continual learning. The proposed method elegantly addresses a major challenge in CL by unifying it with novelty detection in a self-supervised manner.  The experimental results are compelling and demonstrate the significant advantages of the approach.  While minor improvements could be made to address the scalability and comparison with other unsupervised CL methods, the overall contribution is substantial enough to warrant acceptance.\n\n**Final Decision: Accept**\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.785984645\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"PositiveReviewer\",\n  \"prompt\": \"Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Paper Title: Self-supervised Continual Learning based on Batch-mode Novelty Detection. Abstract: None: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: Self-supervised Continual Learning based on Batch-mode Novelty Detection\\n\\nThis paper presents a novel and exciting approach to continual learning (CL) by unifying it with novelty detection (ND).  The authors propose a self-supervised method that tackles both ND and CL simultaneously, addressing a crucial limitation in existing CL methods that rely on labeled data for new tasks.  The core idea of sequentially adding binary classifiers to a base model, trained in a batch-mode fashion, is particularly ingenious and shows great promise.\\n\\n**Strengths:**\\n\\n* **Novelty and Significance:** The unification of ND and CL in a self-supervised framework is a significant contribution. This addresses a critical practical limitation of many CL approaches \u2013 the reliance on labeled data for new tasks, which is often unavailable in real-world scenarios.  The proposed method elegantly circumvents this issue by learning new classes one at a time, leveraging the novelty detection mechanism for unsupervised class separation and then adding a specialized binary classifier.  This incremental approach contrasts sharply with existing methods that tackle multiple classes per task and greatly enhances the practicality of CL.\\n\\n* **Technical Soundness:** The methodology is well-explained and the technical details are presented clearly. The use of the Mahalanobis distance in the gradient space for novelty detection is a sound approach, and the incorporation of a self-supervised binary classifier to refine the detection process is a clever innovation.  The batch-mode training and inference strategy further enhances the robustness and accuracy of the classifier, addressing the potential for error accumulation in sequential classification.  The algorithm descriptions (Algorithms 1-4) are detailed and easily understandable.\\n\\n* **Empirical Validation:** The experimental results are comprehensive and convincingly demonstrate the effectiveness of the proposed method across various datasets (MNIST, CIFAR-10, CIFAR-100, TinyImageNet). The consistent outperformance of state-of-the-art CL methods, particularly in the single-head evaluation, is impressive and highlights the significant advantages of the unified framework. The ablation studies further solidify the contribution by showing the impact of key design choices, such as batch size and IDD/OOD mixture ratios. The figures effectively visualize the key concepts and results.\\n\\n\\n* **Clear Writing and Presentation:** The paper is well-written and easy to follow. The authors effectively motivate the problem, present their approach, and interpret the results. The figures and tables are clear and well-integrated into the text.\\n\\n\\n**Weaknesses:**\\n\\n* **Scalability:** While the incremental approach is advantageous, a thorough discussion of the scalability of the method to a very large number of classes is warranted. The accumulation of binary classifiers could potentially lead to computational overhead and increased memory consumption.  Addressing this concern with either theoretical analysis or further experiments would strengthen the paper.\\n\\n* **Comparison with Unsupervised CL methods:**  The paper focuses heavily on comparisons with supervised CL methods. A more direct comparison with existing unsupervised or self-supervised CL methods would provide a clearer picture of the relative advantages of the proposed approach.\\n\\n* **Generalization to other types of novelty:** The current novelty detection method relies on gradient analysis.  Discussion on the applicability of this method to different types of novelty (e.g., concept drift, adversarial examples) would be beneficial.\\n\\n\\n**Recommendation:**\\n\\nThis paper presents a highly novel and impactful contribution to the field of continual learning. The proposed method elegantly addresses a major challenge in CL by unifying it with novelty detection in a self-supervised manner.  The experimental results are compelling and demonstrate the significant advantages of the approach.  While minor improvements could be made to address the scalability and comparison with other unsupervised CL methods, the overall contribution is substantial enough to warrant acceptance.\\n\\n**Final Decision: Accept**\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.785984645}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Paper Title: Self-supervised Continual Learning based on Batch-mode Novelty Detection. Abstract: None: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: Please finish the review and give the FINAL DECISION line.', 'agent_name': 'BalancedReviewer'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Self-supervised Continual Learning based on Batch-mode Novelty Detection\n\nThis paper presents a novel approach to continual learning (CL) that integrates novelty detection (ND) within a self-supervised framework.  The core idea is to treat each new class as a single task, detected via a gradient-based novelty detector and learned using a sequentially added binary classifier.  The method employs a batch-mode processing strategy to enhance the separation of new and old classes in feature space.  Experiments on several benchmark datasets demonstrate competitive performance compared to state-of-the-art supervised CL methods.\n\n\n**Strengths:**\n\n* **Novelty and Contribution:** The integration of ND and CL in a self-supervised manner is a significant contribution.  Many CL methods rely on task labels, which are often unavailable in real-world scenarios. The proposed method addresses this limitation directly.\n* **Self-Supervised Learning:**  The self-supervised nature of the approach is a major strength, aligning with the growing interest in reducing the reliance on labeled data in CL.\n* **Batch-Mode Processing:** The proposed batch-mode training and inference strategy seems intuitively sound and empirically effective in improving class separation and, consequently, the accuracy of the binary classifiers.\n* **Empirical Results:**  The experimental results are comprehensive, covering multiple datasets and demonstrating competitive performance against existing state-of-the-art methods, especially considering the unsupervised nature of the approach.\n* **Clear Methodology:** The methodology is described clearly and the algorithms are presented in a logical manner.  The figures and diagrams help in visualizing the approach and understanding the technical details.\n\n**Weaknesses:**\n\n* **Scalability:**  The sequential addition of binary classifiers raises concerns about scalability.  While the memory overhead per classifier is claimed to be small, the overall memory requirements could become substantial as the number of classes increases.  A deeper analysis of this aspect is needed, including a discussion of the practical limitations.\n* **Theoretical Justification:** The paper lacks a strong theoretical foundation for the proposed method.  While the empirical results are promising, a more rigorous theoretical analysis would strengthen the paper significantly. For example, a theoretical understanding of why the batch-mode approach improves class separation would be beneficial.\n* **Comparison with other Self-Supervised CL Methods:** The comparison with other self-supervised CL methods is limited.  A more thorough comparison with relevant works in this area is necessary to better position the contributions of this paper.\n* **Assumption of Pure Batches:** The method relies on the assumption that batches predominantly contain samples from a single class. While this is argued to be reasonable in certain real-world scenarios (like video streams), it might not always hold true, and the robustness of the method to violations of this assumption needs further investigation.\n* **Limited Ablation Studies:** While ablation studies are included, they are somewhat limited in scope. More comprehensive ablation studies could further validate the importance of individual components of the proposed method.\n\n\n**Constructive Feedback:**\n\n* Provide a more detailed discussion on the scalability limitations of the approach and propose potential solutions to mitigate these limitations (e.g., pruning strategies, knowledge distillation).\n* Include a more thorough comparison with existing self-supervised CL methods.  A table summarizing the key differences and performance comparisons would be beneficial.\n* Develop a more comprehensive theoretical analysis to justify the effectiveness of the proposed method.\n* Investigate the robustness of the method to noisy data and scenarios where the assumption of pure batches is violated.\n* Conduct more extensive ablation studies, exploring variations in hyperparameters, network architectures, and data augmentation strategies.\n\n\n**Recommendation:**\n\nThe paper presents a novel and interesting approach to self-supervised continual learning.  The experimental results are compelling, demonstrating strong performance. However, the lack of a strong theoretical foundation and concerns about scalability require further attention.  Therefore, I recommend a **conditional accept**, pending a significant revision that addresses the weaknesses mentioned above, particularly the scalability analysis and a more thorough theoretical justification.  The authors should also strengthen the comparison with other self-supervised CL methods.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.785071405\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Paper Title: Self-supervised Continual Learning based on Batch-mode Novelty Detection. Abstract: None: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: Please finish the review and give the FINAL DECISION line.\",\n  \"agent_name\": \"BalancedReviewer\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: Self-supervised Continual Learning based on Batch-mode Novelty Detection\\n\\nThis paper presents a novel approach to continual learning (CL) that integrates novelty detection (ND) within a self-supervised framework.  The core idea is to treat each new class as a single task, detected via a gradient-based novelty detector and learned using a sequentially added binary classifier.  The method employs a batch-mode processing strategy to enhance the separation of new and old classes in feature space.  Experiments on several benchmark datasets demonstrate competitive performance compared to state-of-the-art supervised CL methods.\\n\\n\\n**Strengths:**\\n\\n* **Novelty and Contribution:** The integration of ND and CL in a self-supervised manner is a significant contribution.  Many CL methods rely on task labels, which are often unavailable in real-world scenarios. The proposed method addresses this limitation directly.\\n* **Self-Supervised Learning:**  The self-supervised nature of the approach is a major strength, aligning with the growing interest in reducing the reliance on labeled data in CL.\\n* **Batch-Mode Processing:** The proposed batch-mode training and inference strategy seems intuitively sound and empirically effective in improving class separation and, consequently, the accuracy of the binary classifiers.\\n* **Empirical Results:**  The experimental results are comprehensive, covering multiple datasets and demonstrating competitive performance against existing state-of-the-art methods, especially considering the unsupervised nature of the approach.\\n* **Clear Methodology:** The methodology is described clearly and the algorithms are presented in a logical manner.  The figures and diagrams help in visualizing the approach and understanding the technical details.\\n\\n**Weaknesses:**\\n\\n* **Scalability:**  The sequential addition of binary classifiers raises concerns about scalability.  While the memory overhead per classifier is claimed to be small, the overall memory requirements could become substantial as the number of classes increases.  A deeper analysis of this aspect is needed, including a discussion of the practical limitations.\\n* **Theoretical Justification:** The paper lacks a strong theoretical foundation for the proposed method.  While the empirical results are promising, a more rigorous theoretical analysis would strengthen the paper significantly. For example, a theoretical understanding of why the batch-mode approach improves class separation would be beneficial.\\n* **Comparison with other Self-Supervised CL Methods:** The comparison with other self-supervised CL methods is limited.  A more thorough comparison with relevant works in this area is necessary to better position the contributions of this paper.\\n* **Assumption of Pure Batches:** The method relies on the assumption that batches predominantly contain samples from a single class. While this is argued to be reasonable in certain real-world scenarios (like video streams), it might not always hold true, and the robustness of the method to violations of this assumption needs further investigation.\\n* **Limited Ablation Studies:** While ablation studies are included, they are somewhat limited in scope. More comprehensive ablation studies could further validate the importance of individual components of the proposed method.\\n\\n\\n**Constructive Feedback:**\\n\\n* Provide a more detailed discussion on the scalability limitations of the approach and propose potential solutions to mitigate these limitations (e.g., pruning strategies, knowledge distillation).\\n* Include a more thorough comparison with existing self-supervised CL methods.  A table summarizing the key differences and performance comparisons would be beneficial.\\n* Develop a more comprehensive theoretical analysis to justify the effectiveness of the proposed method.\\n* Investigate the robustness of the method to noisy data and scenarios where the assumption of pure batches is violated.\\n* Conduct more extensive ablation studies, exploring variations in hyperparameters, network architectures, and data augmentation strategies.\\n\\n\\n**Recommendation:**\\n\\nThe paper presents a novel and interesting approach to self-supervised continual learning.  The experimental results are compelling, demonstrating strong performance. However, the lack of a strong theoretical foundation and concerns about scalability require further attention.  Therefore, I recommend a **conditional accept**, pending a significant revision that addresses the weaknesses mentioned above, particularly the scalability analysis and a more thorough theoretical justification.  The authors should also strengthen the comparison with other self-supervised CL methods.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.785071405}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'CriticalReviewer', 'prompt': 'Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Paper Title: Self-supervised Continual Learning based on Batch-mode Novelty Detection. Abstract: None: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: Please finish the review and give the FINAL DECISION line.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Self-supervised Continual Learning based on Batch-mode Novelty Detection\n\nThis paper proposes a unified framework for continual learning (CL) and novelty detection (ND) using a sequence of binary classifiers trained in a batch-mode fashion.  The core idea is to treat each new class as a single task and leverage a self-supervised approach, avoiding the need for labeled data for new classes. While the presented results show promising performance on several benchmark datasets, the paper suffers from significant weaknesses that raise concerns about its novelty, methodology, and generalizability.\n\n**Strengths:**\n\n* **Addresses an important problem:** The paper tackles the challenging problem of self-supervised continual learning, a crucial aspect for real-world applications where labeled data is scarce and tasks evolve dynamically.\n* **Empirical results:** The reported accuracy on several benchmark datasets appears competitive, at least within the specific single-head evaluation setup used.\n\n**Weaknesses:**\n\n1. **Lack of Novelty:** The core methodology is a rather straightforward combination of existing techniques.  The use of a sequence of binary classifiers for incremental learning is not novel; similar approaches have been explored before, albeit often in supervised settings.  The batch-mode novelty detection, while presented as a key contribution, appears to be a minor modification of existing gradient-based novelty detection methods.  The paper fails to convincingly demonstrate a significant advancement over existing CL and ND approaches.\n\n2. **Methodological Flaws and Unclear Assumptions:**\n    * **Batch Purity Estimation:** The algorithm relies heavily on the accurate estimation of batch purity.  The proposed L2 distance-based method is simplistic and makes strong assumptions about the data distribution (e.g., well-separated clusters of IDD and OOD samples).  This assumption is unlikely to hold in more complex real-world scenarios. The paper does not sufficiently address the robustness of this crucial step.\n    * **Gradient-based Novelty Detection:** While using gradients for novelty detection is a valid approach, the reliance on Mahalanobis distance in the gradient space requires careful consideration of the covariance matrix estimation, especially when dealing with high-dimensional data. The paper lacks detail on how this is handled robustly.\n    * **Single-Head Evaluation:** The paper exclusively focuses on single-head evaluation. This significantly limits the generalizability of the proposed method, as many real-world applications would require multi-head architectures to avoid catastrophic forgetting more effectively.\n    * **Memory Management:** The paper mentions memory overhead, but the actual memory requirements and scaling limitations are not properly addressed.  How does the memory usage scale with the number of learned classes?  What are the practical limitations of this approach in terms of memory capacity?\n\n3. **Insufficient Analysis and Ablation Studies:**\n    * **Ablation studies are too limited:** While the paper includes some ablation studies, they are insufficient to fully understand the impact of different design choices. For example, the effect of the batch size and the IDD/OOD mixing ratio should be explored more thoroughly.\n    * **Comparison to state-of-the-art is weak:** The comparison with existing continual learning methods lacks rigor.  The paper needs a more detailed and comprehensive comparison against relevant state-of-the-art methods, using consistent evaluation metrics and experimental settings.  The claim of outperforming state-of-the-art is not convincingly supported.\n    * **Qualitative Analysis:**  The paper lacks a proper qualitative analysis of the learned representations and the decision boundaries.  Visualizations showing the feature distributions and how the binary classifiers separate the classes would significantly strengthen the paper.\n\n4. **Writing Quality:** The writing style is somewhat convoluted and difficult to follow in places.  The methodology section is particularly dense and could benefit from clearer explanations and improved organization.\n\n\n**Recommendation:**\n\nReject.\n\nWhile the paper attempts to address a relevant problem, its methodological limitations, lack of novelty, and insufficient analysis prevent it from making a significant contribution to the field. The core ideas are not novel enough, and the claimed improvements over state-of-the-art methods are not convincingly demonstrated.  Addressing the weaknesses outlined above, particularly the robustness of the batch purity estimation and a thorough comparison with existing methods, would be necessary for consideration in a future submission.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.784158165\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"CriticalReviewer\",\n  \"prompt\": \"Please provide a detailed review of the following paper, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Paper Title: Self-supervised Continual Learning based on Batch-mode Novelty Detection. Abstract: None: Continual learning (CL) plays a key role in dynamic systems in order to adapt to new tasks, while preserving previous knowledge. Most existing CL approaches focus on learning new knowledge in a supervised manner, while leaving the data gathering phase to the novelty detection (ND) algorithm. Such presumption limits the practical usage where new data needs to be quickly learned without being labeled. In this paper, we propose a unified approach of CL and ND, in which each new class of the out-of-distribution (ODD) data is first detected and then added to previous knowledge. Our method has three unique features: (1) a unified framework seamlessly tackling both ND and CL problems; (2) a self-supervised method for model adaptation, without the requirement of new data annotation; (3) batch-mode data feeding that maximizes the separation of new knowledge vs. previous learning, which in turn enables high accuracy in continual learning. By learning one class at each step, the new method achieves robust continual learning and consistently outperforms state-of-the-art CL methods in the singlehead evaluation on MNIST, CIFAR-10, CIFAR-100 and TinyImageNet datasets. 1 INTRODUCTION: Machine learning methods have been widely deployed in dynamic applications, such as drones, self-driving vehicles, surveillance, etc. Their success is built upon carefully handcrafted deep neural networks (DNNs), big data collection and expensive model training. However, due to the unforeseeable circumstances of the environment, these systems will inevitably encounter the input samples that fall out of the distribution (OOD) of their original training data, leading to their instability and performance degradation. Such a scenario has inspired two research branches: (1) novelty detection (ND) or one-class classification and (2) continual learning (CL) or life-long learning. The former one aims to make the system be able to detect the arrival of OOD data. The latter one studies how to continually learn the new data distribution while preventing catastrophic forgetting Goodfellow et al. (2013) from happening to prior knowledge. While there exists a strong connection between these two branches, current practices to solve them are heading toward quite different directions. ND methods usually output all detected OOD samples as a single class while CL methods package multiple classes into a single task for learning. Such a dramatic difference in problem setup prevents researchers to form an unified algorithm from ND to CL, which is necessary for dynamic systems in reality. One particular challenge of multi-class learning in CL is the difficulty in data-driven (i.e., selfsupervised or unsupervised) separation of new and old classes, due to their overlap in the feature space. Without labeled data, the model can be struggling to find a global optimum that can successfully separate the distribution of all classes. Consequently, the model either ends up with the notorious issue of catastrophic forgetting, or fails to learn the new data. To overcome this challenge, previous methods either introduce constraints in model adaptation in order to protect prior knowledge when learning a new task Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017), or expand the network structure to increase the model capacity for new knowledge Rusu et al. (2016); Yoon et al. (2017). However, the methods with constraints may not succeed when the knowledge distribution of a new task is far from prior knowledge distribution. On the other hand, the methods with a dynamic network may introduce too much overhead when the amount of new knowledge keeps increasing. In this context, our aims of this work are: (1) Connecting ND and CL into one method for dynamic applications, (2) Completing ND and CL without the annotation of OOD data, and (3) Improving the robustness and accuracy of CL in the single-head evaluation. We propose a self-supervised approach for one-class novelty detection and continual learning, with the following contributions: \u2022 A unified framework that continually detects the arrival of the OODs, extracts and learns the OOD features, and merges the OOD into the knowledge base of previous IDDs. More specially, we train a tiny binary classifier for each new OOD class as the feature extractor. The binary classifier and the pre-trained IDD model is sequentially connected to form a \u201cN + 1\u201d classifier, where \u201cN\u201d represents prior knowledge which contains N classes and \u201c1\u201d refers to the newly arrival OOD. This CL process continues as \u201cN + 1+ 1+ 1...\u201d (i.e, one-class CL), as demonstrated in this work. \u2022 A batch-mode training and inference method that fully utilizes the context of the input and maximizes the feature separation between OOD and previous IDDs, without using data labels. This method helps achieve the high accuracy in OOD detection and prediction in a scenario where IDDs and OODs are streaming into the system, such as videos and audios. \u2022 Comprehensive evaluation on multiple benchmarks, such as MNIST, CIFAR-10, CIFAR100, and TinyImageNet. Our proposed method consistently achieves robust and a high single-head accuracy after learning a sequence of new OOD classes one by one. 2 BACKGROUND: Most continual learning methods belong to the supervised type, where the input tasks are welllabeled. To mitigate catastrophic forgetting, three directions have been studied in the community: (1) Regularization methods Zeng et al. (2019); Aljundi et al. (2018); Li & Hoiem (2018); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Zenke et al. (2017); Ahn et al. (2019), which aim to penalize the weight shifting towards the new task. This is realized by introducing a new loss constraint to protect the most important weights for previous tasks. Many metrics to measure the importance of weights are proposed, such as the Fisher information matrix, distillation loss and training trajectory. (2) Rehearsal-based methods Lopez-Paz & Ranzato (2017); Chaudhry et al. (2018); Rolnick et al. (2019); Aljundi et al. (2019); Cha et al. (2021), which maintain a small buffer to store the samples from previous tasks. To prevent the drift of prior knowledge, these samples are replayed during the middle of the training routine on the new task. Some rehearsal-based methods are combined with the regularization methods to improve their performance. (3) Expansion-based methods Rusu et al. (2016); Yoon et al. (2017); Schwarz et al. (2018); Li et al. (2019); Hung et al. (2019), which aim to protect previous knowledge by progressively adding new network branches for the new task. These methods are especially useful when the knowledge base of previous tasks and that of the new task are overlapping. Although these three supervised methods improve the performance of continual learning, we argue that the capability to automatically detect the task shifting and learn the new knowledge (i.e., self-supervised or unsupervised) is the most preferred solution by a realistic system, since an expensive annotation process of the new task is impractical in the field. There also exist self-supervised CL methods Rao et al. (2019); Smith et al. (2021), which focus on the generation of the class-discriminative representations from the unlabelled sequence of tasks. However, their demonstration so far are still limited to relatively simple and low-resolution datasets, such as MNIST and CIFAR-10. 3 METHODOLOGY:  3.1 TERMINOLOGY: Algorithm 1 Unification of One-class Novelty Detection and Continual Learning Input: IDD memory budget XmemIDD = {Xmem1IDD , . . . , X memN IDD }, IDD sequence XIDD ={ X1IDD, . . . , X N IDD } OOD sequence XOOD ={ X0OOD, . . . , X t OOD } , trained modelM0 1: Mcurrent \u2190M0 2: for i = 0 to len(XOOD) do 3: XcurrentOOD \u2190 XOOD[i] 4: Xmixture \u2190 XIDD +XcurrentOOD 5: DIDD \u2190 IDD estimator(XmemIDD ,Mcurrent) 6: XpredOOD,Bi \u2190 noveltyDetector(Mcurrent, XmemIDD ,DIDD, Xmixture) 7: XmemIDD .append(X pred OOD) 8: Mcurrent \u2190Mcurrent + Bi //Merge Model 9: end for 10: returnMcurrent Previous CL methods use a taskbased setup, where each task consists of multiple classes of training samples. The model is trained to learn each task (i.e., multiple classes) sequentially. Different from previous methods, our proposed solution is an unified system that leverages the output from the novelty detector for CL. Therefore, we embrace the one-class per task setup as follows: As shown in Algorithm 1, the system is continuously exposed to a stream of mixed input that contains both in-distribution data (IDD) XIDD = { X1IDD, . . . , X N IDD } and one of the out-of-distribution (OOD) data XcurrentOOD \u2208 XOOD ={ X0OOD, . . . , X t OOD } , where each Xi corresponds to a single class in the dataset. XIDD denotes the IDD set containing all the classes that the system already recognizes. XOOD contains the OOD classes that the system is currently facing and will encounter in the future. The primary task is to filter out XcurrentOOD from XIDD through the novelty detection engine, while learning the features of XcurrentOOD for continual learning. Once XcurrentOOD is successfully detected and learned, we move X current OOD from XOOD into XIDD, and randomly draw a new OOD class from XOOD as the newXcurrentOOD . This process continues until XOOD is empty and all classes have been learned by the model, i.e., becoming IDDs. For simplicity, we denote the initial IDD classes and the first OOD class as X 0IDD and X0OOD, respectively. When the system has successfully learned X0OOD, we denote the updated IDD set as X 1IDD and the next OOD as X1OOD. Then Si = { X iIDD, XiOOD } represents the status when the system just finishes learning Xi\u22121OOD. From the perspective of task-based learning, eachX i OOD can be considered as a single task. Our proposed method will detect and learn only one class at each time. 3.2 ONE-CLASS NOVELTY DETECTION AND CONTINUAL LEARNING: Our method is built upon the recently proposed work on \u201cSelf-supervised gradient-based novelty detector\u201d Sun et al. (2022). The contributions of this method are two-fold. First, given a pre-trained modelM0 that is able to classify X 0IDD, a statistical analysis evaluating the Mahalanobis distance in the gradient space is developed to threshold the OOD. Second, to further boost the performance, they introduce a self-supervised binary classifier, denoted as B0, which guides the label selection process to generate the gradients so that the Mahalanobis distance between the IDD and OOD data is further maximized. The primary OOD detector, which is based on the Mahalanobis distance, interleaves with the binary classifier. As more data stream into the system, the OOD detection accuracy gradually improves through this closed-loop interaction. This method can be naturally converted into a one-class CL solution. Upon the successful detection of X0OOD, the binary classifier B0 is well-trained to recognize X0OOD from previous X 0IDD. Intuitively, we can deploy B0 upstream from the pre-trained IDD modelM0 to filter out the newly learned X0OOD, and leave the predicted X 0IDD samples to the downstream M0 for further classification. This merged modelM1 = {M0 + B0} will become the new baseline to detect the next X1OOD. Continuously, we can keep adding a new binary classifier Bi to the previously merged modelMi = {Mi\u22121 + Bi\u22121} for every new XiOOD upon its detection. Eventually, we obtain this chain structure (Fig. 1) where the latest binary classifier is the most knowledgeable one to recognize all XIDD and XOOD. During the inference phase, its job is to filter out the XtOOD from a mixture input that contains {X tIDD, XtOOD}. The remaining inputs X tIDD = { X t\u22121IDD, X t\u22121 OOD } will be sent downstream to the next binary classifier to sequentially recognize the Xt\u22121OOD. This sequence continues until only X 0IDD is left and arrives at the original modelM0 for final classification. Algorithm 2 Gradient-based Novelty Detection 1: function noveltyDetector (M,XmemIDD ,DIDD, Xmix) 2: Initialize Bnew 3: Xpuremix \u2190 batchPurityEstimator(Xmix, XmemIDD ,M) 4: repeat 10 iterations 5: XpredIDD \u2190 [ ], X pred OOD \u2190 [ ] 6: for each x \u2208 Xpuremix do 7: if Bnew not trained then 8: cM0 \u2190 c pred M0 , cBi \u2190 c pred Bi 9: else 10: cM0 \u2190 ccustM0 , cBi \u2190 1\u2212 c pred Bi 11: end if \u2207fM(x)\u2190 \u2207cM0 fM0(x)\u2225 . . . \u2225\u2207cBi fBi(x) 12: noveltyScore l\u2190 [ ] 13: for each (\u00b5\u0302c, \u03a3\u0302c) in DIDD do score c\u2190 (\u2207fM(x)\u2212\u00b5\u0302c)T \u03a3\u0302c \u22121 (\u2207fM(x)\u2212\u00b5\u0302c) noveltyScore l.append(score c) 14: end for 15: noveltyScore\u2190 min(noveltyScore l) 16: if noveltyScore < threshold then 17: XpredIDD.append(x) 18: else 19: XpredOOD.append(x) 20: end if 21: end for 22: Re-initial & Train Bnew using {XpredIDD, X pred OOD} 23: return X predIDD,Bnew 24: end function There are two advantages of this oneclass learning method. First, after we merge a new binary classifier to the existing structure, all weights are frozen and isolated from future training towards the new OODs. This prevents the knowledge shifting and thus, minimizes catastrophic forgetting. Second, each binary classifier only induces a small memory overhead since its task is simple enough to be accomplished with very few layers of neurons. Compared with previous dynamic methods which either add an indeterminate amount of neurons to each layer or make new branches using sub-modules, our method requires much lower memory. On the other hand, such a sequential learning model leaves two questions to be answered: (1) How to port the original Mahalanobis distance method into this merged model so that it can detect the next OOD? (2) How to achieve a high inference accuracy of the binary classifier, such that the testing samples can go through multiple steps in this sequence and reach the final classifier without losing the accuracy. We will address each problem in the subsection 3.3 and 3.4, respectively. 3.3 GRADIENT-BASED NOVELTY DETECTOR: The stepping stone of the previous novelty detector (Algorithm 2, Algorithm 3) is to characterize the IDD and OOD in the gradient space. If a model M has been trained to classify N classes in X trainIDD = { Xtrain 1IDD , . . . , X train N IDD } , the gradients collected from each XiIDD \u2208 XIDD will form a class-wise multi-variant distribution DIDD = { D1IDD, . . . , D N IDD } . Each DiIDD \u223c N (\u00b5\u0302i, \u03a3\u0302i) corresponds to a Gaussian distribution for a particular class i, and future IDDs X valIDD = { Xval 1IDD , . . . , X val N IDD } will be within the range of corresponding DiIDD. On the contrary, due to the low confidence towards the OOD, the modelM will adjust itself to fit the OOD properly by back-propagating the gradients with abnormal magnitude and direction. Any deviant fromDIDD will be considered as abnormal and therefore, a distance metric can be utilized to measure the novelty confidence. They propose to use the Mahalanobis Distance as the novelty confidence score Sun et al. (2022): Mx = (\u2207cfM(x)\u2212 \u00b5\u0302c)T \u03a3\u0302\u22121c (\u2207cfM(x)\u2212 \u00b5\u0302c) (1) To collect the gradient \u2207cfM(x), they further utilize two types of labels for back-propagation: cpredictedM and c custom M . cpredictedM = argmax c\u2208XIDD (Softmax(fM(X; \u0398))) (2) ccustomM = argmin c\u2208XIDD (Softmax(fM(X; \u0398))) (3) Measured by the softmax output from the trained model M, cpredictedM and ccustomM refer to the most and least possible class the input belongs to, respectively, leading to mild and aggressive backpropagated gradients. Since the goal is to detect the abnormal gradient from the OOD, using ccustomM for OOD maximizes weight update to the model and thus, the gradients will be even more likely to fall out of the DIDD. On the other hand, using cpredictedM for IDDs helps their gradients stay within DIDD and thus, reduces the possibility of false alarm. To guide the label selection, they introduce a binary classifier into the system to predict the samples. Based on the prediction, the primary ND engineM collects the gradients with either cpredictedM or ccustomM to measure the Mahalanobis distance towards DIDD. To make the above algorithm compatible with the unified ND model, we set up the following scenario for explanation. Assume the system just finishes the detection and learning on XiOOD and is in the status of Si+1 = { X i+1IDD, X i+1 OOD } . The latest binary classifier Bi, which can distinguish XiOOD and X iIDD, is placed upstream from the previously merged modelMi to form a new model Mi+1 = {Mi + Bi}. At this point,Mi+1 becomes the new baseline to detect Xi+1OOD. It is a sequential structure composed of i+1 number of the binary classifier, i.e., one for each learned OOD, and one original neural networkM0 that predicts N classes and is placed at the end of the chain. With these notations mentioned above, we introduce our solution as follows: Gradient Collection: To make this merged modelMi+1 = {Mi + Bi} capable of detecting the upcomingXi+1OOD, we need to first conduct theD i+1 IDD estimation onX i+1 IDD = { X iIDD, XiOOD } from bothMi andBi. Only then we can start evaluating how far theXi+1OOD deviates from the distributions inDi+1IDD. Given an input x, we propose to collect the gradients\u2207cfBi(x) and\u2207cfMi(x) separately from both Bi andMi. Then we concatenate them together\u2207cfBi+Mi(x) = \u2207cfBi(x)\u2225\u2207cfMi(x). The overall gradient dimension becomes \u2207fMi+1 = \u2207fM0\u2225(\u2207fB0\u2225 . . . \u2225\u2207fBi), where \u2207fM0 is the gradient collected from the original neural network, and \u2207fB0...i are the gradients from each binary classifier in the chain. Class Selection: Similar as cpredictedM and c custom M to \u2207cfM(x), here we introduce c predicted Bi and ccustomBi to control the gradient\u2207cfBi(x). Since there are only two possible predictions: IDD (label 0) and OOD (label 1), we can simply make the original prediction as cpredictedBi and use the flipped result as ccustomBi cpredictedBi = fBi(X; \u0398) ; c custom Bi = 1\u2212 fBi(X; \u0398) (4) Ideally we prefer to use the label cpredictedBi for all the samples X \u2208 { X iIDD, XiOOD } , so that the gradient\u2207cfBi(x) will be minimized. ForXi+1OOD, using ccustomBi will make their gradients stand out even more and thus, easier to be detected. Algorithm 3 Gradient-based Evaluation of IDD Distribution 1: function IDD estimator (XmemIDD ,Mi) 2: DIDD \u2190 [ ] 3: for each XmemIDD in XmemIDD do 4: \u2207cfMi(xk) = \u2207cfM0(xk)\u2225 . . . \u2225\u2207cfBi(xk) 5: \u00b5\u0302c \u2190 1Nc \u2211 yk=c \u2207cfMi(xk) 6: \u03a3\u0302c \u2190 1Nc \u2211 yk=c (\u2207cfMi(xk) \u2212 \u00b5\u0302c)(\u2207cfMi(xk) \u2212 \u00b5\u0302c) \u22a4 where (xk, yk) \u2208 XmemIDD 7: DIDD.append((\u00b5\u0302c, \u03a3\u0302c)) 8: end for 9: return DIDD 10: end function Memory for DIDD Evaluation: The extra dimension from \u2207fBi requires the preestimated distributions in DIDD to be re-evaluated. For each class in X i+1IDD, we assign a memory at a constant size to re-evaluate DIDD after the expansion of the gradient dimension. In addition, the newly learned XiOOD becomes a new IDD class from the perspective ofMi+1 and thus, DIDD needs to includeDiIDD. Before we use Mi+1 to detect the next OOD, DIDD should include the distribution estimation of N + i+1 classes, in the dimension of\u2207fMi+1 . Furthermore, a small memory for each class helps improve the ND accuracy. Previous methods only rely on the predicted IDD and OOD from the novelty detector engine to train the binary classifier, with limited training accuracy and convergence speed when the prediction is not accurate enough. Instead, we propose to directly use the pre-stored IDD samples as the IDD training dataset. For the OOD, we still use the predicted OOD from the ND routine. Such a solution makes the training of the binary classifier more stable, by reducing mislabeled OODs. After the system converges towards current OOD, those predicted OOD samples, selected by the novelty detector with the assistance from the binary classifier, will be stored in the memory as the representation of this newly learned class, which will be used for DIDD evaluation and future training on the new binary classifier. Except for the first N classes which have the pre-labeled dataset, all the samples from the new coming OODs are self-selected by the engine itself. 3.4 BATCH-MODE TRAINING AND INFERENCE: The success of the sequential classification, from a sequence of binary classifiers, relies on the high inference accuracy of each binary classifier. Even with very minor accuracy degradation from each of them, the accumulated accuracy will drop exponentially. To achieve such a goal, we propose to not only consider the sample itself but the context of the sample as well. If a testing sample is within the cluster of other samples that all come from the same class, its prediction will be biased towards that class as well. This inspires the idea of the batch-mode training and inference: Based on the features, we will cluster the samples in one class into a batch first, before feeding them to the engine. During the training phase, we introduce the new loss function as follows: L(X ) = 1 N N\u22121\u2211 i=0 L(f(Xi), i) (5) where each Xi refers to a single batch with samples of class i only. Different from traditional training method where the feed-forward operation is conducted on a single batch that contains randomly selected samples fromN classes, we sendN individual one-class batches to the classifier all at once and calculate their average loss. Due to the nature of the BatchNorm layer in the neural network, we find that the batch average from each class can be better separated and thus, the boundary of the classes is easier to be learned. To prove the effectiveness of the batch-mode training, we evaluate the feature distribution of IDD and OOD using both the traditional and batch-mode method with CIFAR-10 dataset on a binary classifier, where IDD contains five classes and OOD contains one class. The batch size is at 32 samples. As shown in Fig. 2(a)(b), the result from batch-mode training is less intertwined, promising a higher chance to detect OOD. We exploit this property to improve the accuracy of each binary classifier. One challenges in this approach is that onlyXIDD classes are available for creating the pure batches, but the OOD samples are mixed with IDD in the incoming stream, not pre-labeled. Therefore, a prefiltering operation is necessary to detect and prepare the OOD from the data stream. We propose to first divide the input stream into small consecutive batches and use a purity metric to localize the batches with the highest OOD percentage. In reality, the assumption of IDD/OOD batches is quite feasible. For instance, in a video or audio stream, once OOD data appear, there will be multiple, continuous samples of OOD, rather than one glitching sample only. Regarding the purity metric, we propose to compare the mean of the features from the testing batch with the pre-estimated features of each class in XIDD. More specifically, assuming the system is in status Si = { X iIDD, XiOOD } , there are totally N + i classes in X iIDD. Therefore, we expect the next input stream will contain a mixture of XiOOD batch and N + i kinds of batches from previous classes. We use M0 to filter out the batches that contain the first N IDD classes, by comparing the L2 distance between the batch features \u03c8M0(Xtest) and the X IDD0 features \u03c8M0(X IDD0 ), both extracted from M0. Since M0 is trained by X IDD0 , if Xtest batch is dominated by the samples from X IDD0 , then the L2 distance between them should not be large. On the other hand, if the batch is mixed with samples from another class, then the L2 distance will increase which helps us set a threshold to separate them. The selected batches will be sent to the upstream of M0, which is B0, for another round of filtering to remove the batches that are dominated by X0OOD. This procedure continues along the sequential structure from bottom to top until all the batches containing theN+ i classes are filtered out. The remaining batches then only contain the new OOD XiOOD. Algorithm 4 Unsupervised Estimation of Batch Purity 1: function batchPurityEstimator (Xmix,XmemIDD ,M) 2: Divide Xmix sequence into batches with size 32 [XB0, XB1, . . . , XBt] 3: Xpuremix \u2190 [ ] 4: for each XB \u2208 [XB0, XB1, . . . , XBt] do 5: for eachm along the chain [M0,B0, . . . ,Bi] do 6: if m isM0 then 7: XrefIDD \u2190 select X 0IDD from XmemIDD 8: else if m is Bk where k \u2208 [0, i] then 9: XrefIDD \u2190 select XkOOD from XmemIDD 10: end if 11: L2\u2190 d(\u03c8m(XB), \u03c8m(XrefIDD)) 12: if L2 < thresholdIDD then 13: Xpuremix .append(XB) 14: else if L2 > thresholdOOD then 15: Continue 16: else Break 17: end if 18: end for 19: end for 20: return Xpuremix 21: end function (a) Frame-based feature distribution (b) Batch-based feature distribution Figure 2: The feature distribution of IDD and OOD using (a) traditional training and (b) batch-mode training. Figure 3: L2 distance-based batch purity estimation using M0 and B0\u22123 on IDD:{ X 0IDD, X0OOD, . . . , X3OOD } and OOD: X4OOD. All data are collected using CIFAR-10 dataset. Fig. 3 illustrates the process to separate the pure batches from the CIFAR-10 input stream that consists of five types of IDDs ( { X 0IDD, X0OOD, . . . , X3OOD } ) and one OOD (X4OOD), using M0 and B0\u22123. The gray and white area corresponds to the batches with 100% purity and mixture data, respectively. Starting fromM0 (the red curve), X 0IDD pure batches are collected by comparing the L2 distance with low threshold. The batches above threshold are sent to B0 to find the X0OOD pure batches. This process continues until the stream data reaches B3 and all IDD and OOD pure batches are successfully separated. 4 EXPERIMENTS: To prove the efficacy of our proposed method, we conduct several one-class ND and CL experiments using MNIST Deng (2012), CIFAR-10, CIFAR-100 Krizhevsky & Hinton (2009) and TinyImageNet Le & Yang (2015). All experiments are implemented using PyTorch Paszke et al. (2019) on NVIDIA GeForce RTX 2080 platform. 4.1 EXPERIMENTAL SETUP: Input Sequence and Memory Budget: Different from the multiple-class based setup, our method is evaluated after every exposure to a new OOD class. For every new XiOOD, we mix it with the same amount of randomly selected samples from X iIDD. This mixed input stream is then sent to the system for ND and CL. The input stream consists of the batches from N + i classes in X iIDD and from theXiOOD current class. Each batch is at the size of 32 frames for all the experiments. We also create the transition phases to mimic the input change from one class to another. Each transition phase last three batches long. The ratios of the mixture between the previous class and the next class are 1/4, 1/2 and 3/4. This transition setup is used in batch purity evaluation. For each dataset, the size of XiOOD and X iIDD samples is shown in the first row of Table 1. XiOOD samples are selected from the training dataset while the X iIDD are selected from the testing dataset. The reason for using testing rather than training on X iIDD is because previously trained binary classifiers have already seen the training data of X iIDD and we need to avoid any unfair evaluation in the current iteration. The second row of Table 1 presents the memory budget for X0IDD. Network Structure and Training: For fair comparisons with previous methods, we select the structure of M0 as shown in Table 1, Row 3. The binary classifier has three convolution layers, one BatchNorm layer, and a Sigmoid classifier. This structure is used in all experiments. ForM0 training, standard Stochastic Gradient Decent is used with momentum 0.9 and weight decay 0.0005. The number of epochs is listed in Table 1, Row 4. The initial learning rate is set to 0.1 and is divided by 10 after reaching the 50% and 75% milestones. For the binary classifier, we train it in 100 epochs with the Adam optimizer Kingma & Ba (2014) where the initial learning rate is set to 0.0002 and the decay rate is controlled by \u03b21 = 0.5 and \u03b22 = 0.999. To estimate the novelty in the gradient space, we collect the gradients from the last convolution layer ofM0 and the second from the last convolution layer of each binary classifier. 4.2 IN-DEPTH ANALYSIS: All experiments are conducted in an unsupervised manner, which means all new OOD classes are not manually labeled, but purely rely on the prediction from the novelty detector engine. Compared with traditional CL algorithms which learn multiple classes in one shot, our method learns one class at each time and takes more steps to achieve the same learning goal. To test how many classes can be learned before the accuracy starts to drop, we test our algorithm using CIFAR-100 by training a baseline model using 10 classes and then continually feeding 20 new classes to the system one class after another. The inset from the Figure 4(a) illustrates the accuracy curve from the actual testing result (red points) plus the extrapolation (dashed line). This curve proves that our method is able to stably learn multiple steps; the accuracy eventually drops as the error in each binary classifier accumulates through the sequential process. Therefore, we design our further experiment as follows: For CIFAR-100 and Tiny-ImageNet, we divide them into 10 tasks where each task contains 10 and 20 classes, respectively. After the baseline training with the first task, we test the algorithm performance by feeding the next task as the incoming OOD. Once all the classes from the new task are learned one by one, we terminate the current iteration and retrain a new baseline model using all the previous tasks. This new baseline will then be used for learning the next available task. This process continues till all the tasks have been tested. For each experiment on CIFAR-10 and MNIST, we train two baseline models using the first two classes and the first five classes to mimic the 5-tasks (5T) and 2-tasks (2T) setup that used by other methods. We then feed the remaining classes to the baseline model to test the performance. As shown in Fig. 4, the single-head accuracy of all the experiments stay at a high value after consecutively detecting and learning new classes from each checkpoint, which proves that our batch-mode method successfully boosts the performance of the binary classifier. For CIFAR-10 and MNIST, our method significantly improves the state-of-the-art even though it is unsupervised. For TinyImageNet, the accuracy is less stable due to the increased complexity of the dataset, but overall the performance is still robust after learning 20 new classes. 5 ABLATION STUDY: We conduct four ablation studies to test how the performance is influenced by various input stream patterns using CIFAR-10 and CIFAR-100 dataset. First, we conduct two experiments by feeding the input with 75%/25% and 90%/10% of IDD/OOD mixture. As shown in Figs. 5(a)(b), the single-head accuracy of both experiments are worse than previous experiments using 50%/50% of IDD/OOD mixture. This is because fewer OOD samples increase the difficulty in OOD separation, especially the unsupervised training of the binary classifier. Second, we test our model with two smaller batch size for batch purity estimation. Figs. 5(c)(d) present the performance after dividing the input stream into smaller batches at the size of 16 and 8. The smaller the batch size is, the worse performance our model achieves. With fewer samples in each batch, the average feature estimated from the trained model becomes more diverse, which exacerbates the error in the purity estimation engine. 6 CONCLUSION: In this paper, we propose an unified framework for one-class novelty detection and continual learning, by using a sequence of binary classifiers with the batch-mode technique. We demonstrate that our method successfully detects and learns consecutive OOD classes in an unsupervised setup, achieving a stable single-head accuracy without triggering catastrophic forgetting. For instance, our method reaches 97.08% on CIFAR-10 in continual learning, better than the state-of-the-art. The performance on all other datasets is also among the top list, without the need of manually labeled training data. The success of this approach promises high stabilization, high learning accuracy, and practical usage by various dynamic systems. A APPENDIX: Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: Self-supervised Continual Learning based on Batch-mode Novelty Detection\\n\\nThis paper proposes a unified framework for continual learning (CL) and novelty detection (ND) using a sequence of binary classifiers trained in a batch-mode fashion.  The core idea is to treat each new class as a single task and leverage a self-supervised approach, avoiding the need for labeled data for new classes. While the presented results show promising performance on several benchmark datasets, the paper suffers from significant weaknesses that raise concerns about its novelty, methodology, and generalizability.\\n\\n**Strengths:**\\n\\n* **Addresses an important problem:** The paper tackles the challenging problem of self-supervised continual learning, a crucial aspect for real-world applications where labeled data is scarce and tasks evolve dynamically.\\n* **Empirical results:** The reported accuracy on several benchmark datasets appears competitive, at least within the specific single-head evaluation setup used.\\n\\n**Weaknesses:**\\n\\n1. **Lack of Novelty:** The core methodology is a rather straightforward combination of existing techniques.  The use of a sequence of binary classifiers for incremental learning is not novel; similar approaches have been explored before, albeit often in supervised settings.  The batch-mode novelty detection, while presented as a key contribution, appears to be a minor modification of existing gradient-based novelty detection methods.  The paper fails to convincingly demonstrate a significant advancement over existing CL and ND approaches.\\n\\n2. **Methodological Flaws and Unclear Assumptions:**\\n    * **Batch Purity Estimation:** The algorithm relies heavily on the accurate estimation of batch purity.  The proposed L2 distance-based method is simplistic and makes strong assumptions about the data distribution (e.g., well-separated clusters of IDD and OOD samples).  This assumption is unlikely to hold in more complex real-world scenarios. The paper does not sufficiently address the robustness of this crucial step.\\n    * **Gradient-based Novelty Detection:** While using gradients for novelty detection is a valid approach, the reliance on Mahalanobis distance in the gradient space requires careful consideration of the covariance matrix estimation, especially when dealing with high-dimensional data. The paper lacks detail on how this is handled robustly.\\n    * **Single-Head Evaluation:** The paper exclusively focuses on single-head evaluation. This significantly limits the generalizability of the proposed method, as many real-world applications would require multi-head architectures to avoid catastrophic forgetting more effectively.\\n    * **Memory Management:** The paper mentions memory overhead, but the actual memory requirements and scaling limitations are not properly addressed.  How does the memory usage scale with the number of learned classes?  What are the practical limitations of this approach in terms of memory capacity?\\n\\n3. **Insufficient Analysis and Ablation Studies:**\\n    * **Ablation studies are too limited:** While the paper includes some ablation studies, they are insufficient to fully understand the impact of different design choices. For example, the effect of the batch size and the IDD/OOD mixing ratio should be explored more thoroughly.\\n    * **Comparison to state-of-the-art is weak:** The comparison with existing continual learning methods lacks rigor.  The paper needs a more detailed and comprehensive comparison against relevant state-of-the-art methods, using consistent evaluation metrics and experimental settings.  The claim of outperforming state-of-the-art is not convincingly supported.\\n    * **Qualitative Analysis:**  The paper lacks a proper qualitative analysis of the learned representations and the decision boundaries.  Visualizations showing the feature distributions and how the binary classifiers separate the classes would significantly strengthen the paper.\\n\\n4. **Writing Quality:** The writing style is somewhat convoluted and difficult to follow in places.  The methodology section is particularly dense and could benefit from clearer explanations and improved organization.\\n\\n\\n**Recommendation:**\\n\\nReject.\\n\\nWhile the paper attempts to address a relevant problem, its methodological limitations, lack of novelty, and insufficient analysis prevent it from making a significant contribution to the field. The core ideas are not novel enough, and the claimed improvements over state-of-the-art methods are not convincingly demonstrated.  Addressing the weaknesses outlined above, particularly the robustness of the batch purity estimation and a thorough comparison with existing methods, would be necessary for consideration in a future submission.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.784158165}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here are the final decisions from the three agents:\n\n*   **PositiveReviewer:** Accept\n*   **BalancedReviewer:** Conditional Accept\n*   **CriticalReviewer:** Reject\n\nBased on these reviews, I will provide a final decision. Considering the concerns raised by the CriticalReviewer regarding novelty and methodological flaws, and the BalancedReviewer's conditional accept pending revisions, I lean towards a more conservative decision.\n\nFINAL DECISION: Reject\n"}], "ground_truth": "Reject", "response_time": 262.2691400051117}
