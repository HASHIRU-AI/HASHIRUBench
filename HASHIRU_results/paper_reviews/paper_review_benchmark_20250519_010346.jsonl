{"paper_id": "9BXSGPfRhX", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s\n\n1 INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS. B ADDITIONAL RESULTS: Higher-Shot Results on COCO-ARShift In Table 2 of the main paper, we show that CoRPN\u2019s outperform the DeFRCN Qiao et al. (2021) baseline consistently on 1, 2, and 3 shots. Table 8 shows that this performance improvement persists in 5, 10, and 30 shots. In this scenario where more support instances are available, CoRPN\u2019s consistently outperform the DeFRCN Qiao et al. (2021) baseline. Additional Baseline on ARShift In Table 1 of the main paper, we mainly compare our method against the state-of-the-art method DeFRCN Qiao et al. (2021) on our VOC-ARShift. In Table 9 we also evaluate an additional baseline FSCE Sun et al. (2021). Note that other recently published methods represented by FSCE underperform our DeFRCN baseline by significant margins on our ARShift splits as well, so we do not include these other methods in our main paper. Results on Base classes Here we provide the detection results on base classes after base classes training (stage 1) and after fine-tuning (stage 2). As shown in Table 7, the performance of our CoRPN\u2019s + DeFRCN on base classes is comparable with DeFRCN Qiao et al. (2021), while we achieve large improvements on novel test classes as shown in Table 1 of the main paper. After fine-tuning, our CoRPN\u2019s also do not forget base classes. Table 7 shows that the performance of our CoRPN\u2019s + DeFRCN after finetuning is still comparable with the baseline. C IMPLEMENTATION AND EVALUATION DETAILS Implementation Details and Hyperparameters. For ease of comparison, we use the same values for all shared training and fine-tuning hyperparameters (batch size, learning rate, momentum, weight decay, etc.) as the baselines Wang et al. (2020) and Qiao et al. (2021). CoRPN\u2019s have the following additional hyperparameters: the number of RPN\u2019s, the cooperation loss threshold \u03d5, the diversity loss trade-off \u03bbd, and the cooperation loss trade-off \u03bbc. For COCO and LVIS, we directly used hyperparameter sets that worked well on PASCAL VOC. In Table 4, we report CoRPN\u2019s detection results. We find that hyperparameters selected from PASCAL VOC are generalizable to the more challenging COCO and LVIS benchmark. Results reported in Table 4 are obtained with 5 RPNs, \u03d5 = 0.3, \u03bbc = 1, \u03bbd = 0.025 for TFAWang et al. (2020), and \u03bbc = 2, \u03bbd = 0.05 for DeFRCN Qiao et al. (2021). Selection Procedure \u2013 Cumulative Variance. We summarized the hyperparameter selection criteria for PASCAL VOC in the experiment section in the main paper. The second criterion is the cumulative variance in the RPN\u2019s response matrix to foreground boxes. Specifically, given a set of M anchor boxes, the N RPN\u2019s produce an N by M matrix of probabilities F = [f1, f2, . . . , fN ]T . We run a principal component analysis (PCA) on F with N components and compare the cumulative percentage of variance explained by each component. We would like the variance to be distributed across components. Evaluation. As mentioned in the main paper, for a fair comparison, we use the standard evaluation procedure for all the compared models. We also compare against other approaches with the same novel class instances and test images. Authors of methods compared in this submission have helped us ensure the performance we report is a proper reflection of their methods, and we will acknowledge properly in any final version. In the standard evaluation procedure, when a test image comes in, the model has no assumption on what category the image contains Wang et al. (2020). The detector\u2019s classifier is (|Cb|+|Cn|)-way, detecting objects from a joint space of both base and novel categories. In the main paper, we marked results with * if they were re-evaluated under the standard procedure, and with ** if the original work used the standard procedure, but the results were not reported and so they were evaluated by us. Specifically, below we include the details on how we obtained the results (with special marks) in the main paper; other results (without special marks) Wang et al. (2020); Pe\u0301rez-Ru\u0301a et al. (2020); Yang et al. (2020); Sun et al. (2021); Li et al. (2021a); Zhu et al. (2021) including the concurrent work Sun et al. (2021); Li et al. (2021a); Zhu et al. (2021) are from the original papers. \u2022 We fine-tuned and evaluated MPSR using the publicly released code and the pre-trained detection model for base classes Wu et al. (2020). \u2022 We re-evaluated FSOD Fan et al. (2020) and CoAE Hsieh et al. (2019) under the standard procedure, using the publicly released code and the pre-trained model Fan et al. (2020); Hsieh et al. (2019). FSOD uses a class-agnostic 2-way classifier that determines if an object is foreground or background. At inference time, FSOD produces a balanced number of proposals per novel category and collects these proposals for NMS. We adapt FSOD under the standard procedure such that there are a balanced number of proposals for all base and novel categories. For a fair comparison, we also fine-tuned FSOD with the same novel category instance(s) as in TFA Wang et al. (2020). \u2022 We re-evaluated CoAE under the standard evaluation, using the publicly released code and a pre-trained model Hsieh et al. (2019). For each test image containing a certain category, CoAE samples support image(s) from this category and collects boxes based on the support image(s). At inference time, instead of providing each test image with support feature(s) from this category, we provide each test image support feature(s) from all base and novel categories. We then collect boxes from all categories and evaluate them. For a fair comparison, we also fine-tuned CoAE with the same novel category instance(s) as in TFA Wang et al. (2020). D TRAINING & INFERENCE TIME AND MEMORY: The only architectural difference between CoRPN\u2019s and the baseline RPNs of TFA Wang et al. (2020) and DeFRCNQiao et al. (2021) is that CoRPN\u2019s have multiple RPN classifiers. Note that we only duplicate the RPN classifiers, but not the entire RPN. In the baselines, the RPN\u2019s classifier is a 1\u00d71 convolutional layer, with input channels as the number of feature channels (256) and output channels as the number of cell anchors (3). Compared with the baselines, CoRPN\u2019s with 5 RPN\u2019s have four additional RPN classifiers and thus consist of 256\u00d712 additional parameters. In our experiments, we find that CoRPN\u2019s with 5 RPN\u2019s increase the training time by only 3%, with roughly the same inference time and same memory footprint, compared with the baselines. E RESULTS ON CONVENTIONAL SPLITS: In Table 4 of the main paper, we show that CoRPN\u2019s perform comparably or even better on conventional splits on 1, 2, and 3 shots on COCO. In this section we present more conventional split results. Table 10 shows PASCAL VOC conventional split results where our CoRPN\u2019s perform comparably with the baselines. Table 11 shows the results on COCO 5, 10, and 30 shots, where our method also obtains similar performance to the baseline. F ADDITIONAL ANALYSIS: We present some additional ablation studies for our CoRPN\u2019s. Note that these ablations are performed on the conventional split. Even on conventional splits, our CoRPN\u2019s outperform these naive strategies. Our CoRPN\u2019s will likely achieve more performance improvement on our ARShift splits. Bootstrapping. Table 1 in the main paper shows that our CoRPN\u2019s outperform naive ensembles. Here we further compare CoRPN\u2019s with a bootstrapping baseline in Table 12. We construct the bootstrapping baseline by using multiple RPN classifiers, the number of which is the same as CoRPN\u2019s. Instead of selecting the most confident RPN to get gradients, we randomly select an RPN to get gradients during training. There are no additional loss terms in training these RPN classifiers. In the fine-tuning stage and the inference time, we use the same selection procedure as CoRPN\u2019s (i.e., using the most certain RPN). Diversity loss in Dvornik et al. (2019). In Table 12, we also compare our CoRPN\u2019s with Dvornik et al. (2019), which utilizes a pairwise cosine similarity based diversity loss. Our CoRPN\u2019s also outperform this baseline with a considerable margin. Larger RPN. We compare CoRPN\u2019s with a baseline with larger RPN sub-networks in Table 13. We find that using larger RPN sub-networks does not improve performance, suggesting that the advantage of CoRPN\u2019s is not simply the result of using more parameters. In CoRPN\u2019s, all RPN classifiers share the same RPN feature extractor, as shown in Figure 4 (main paper). We enlarge the feature dimension in the RPN. We implemented two options: a large RPN where the RPN feature extractor\u2019s output channels increase from 256 to 272, and a larger RPN where the output channels double to 512. We also modified the RPN classifier and bounding box regressor to take in larger features for both options. Table 13 shows that simply enlarging the model capacity and the anchor density of the original RPN cannot improve the few-shot detection performance. G ADDITIONAL QUALITATIVE RESULTS: In Figure 4 we show some additional qualitative result of our CoRPN\u2019s model on COCO. Notably, our model could successfully discover novel objects omitted by the baseline, and also successfully classify some novel objects that cause confusion for the baseline method. H DATA SPLITS: PASCAL VOC Split. For our proposed PASCAL VOC-ARShift split, the base classes include \u201cbicycle\u201d, \u201cboat\u201d, \u201ccar\u201d, \u201ccat\u201d, \u201cchair\u201d, \u201cdiningtable\u201d, \u201cdog\u201d, \u201cperson\u201d, \u201csheep\u201d, \u201ctvmonitor\u201d, \u201cbird\u201d, \u201cpottedplant\u201d, \u201ccow\u201d, \u201cmotorbike\u201d, and \u201csofa\u201d; the novel classes include \u201ctrain\u201d, \u201cbottle\u201d, \u201caeroplane\u201d, \u201chorse\u201d, and \u201cbus\u201d. COCO Split. For our proposed COCO-ARShift split, the base classes include the 20 VOC classes in COCO; the novel classes include \u201chot dog\u201d, \u201ctennis racket\u201d, \u201cfire hydrant\u201d, \u201claptop\u201d, \u201csuitcase\u201d, \u201cfrisbee\u201d, \u201cteddy bear\u201d, \u201cbowl\u201d, \u201ckite\u201d, and \u201celephant\u201d. LVIS Splits. For our proposed LVIS-ARShift splits, the base classes include the 20 VOC classes in LVIS; the novel classes of the first split include \u201cLoafer (type of shoe)\u201d, \u201cbatter (food)\u201d, \u201ccabin car\u201d, \u201ccylinder\u201d, \u201cegg roll\u201d, \u201cliquor\u201d, \u201cnailfile\u201d, \u201cplow (farm equipment)\u201d, \u201cvinegar\u201d, and \u201cyoke (animal equipment)\u201d; the novel classes of the second split include \u201cbroach\u201d, \u201cburrito\u201d, \u201ccargo ship\u201d, \u201ccrayon\u201d, \u201cincense\u201d, \u201cpeeler (tool for fruit and vegetables)\u201d, \u201cpin (non jewelry)\u201d, \u201croller skate\u201d, \u201ctinsel\u201d, and \u201cvodka\u201d; the novel classes of the third split include \u201cax\u201d, \u201cbeaker\u201d, \u201cferry\u201d, \u201cfish (food)\u201d, \u201cfunnel\u201d, \u201cincense\u201d, \u201cneedle\u201d, \u201cspace shuttle\u201d, \u201cstepladder\u201d, and \u201cvulture\u201d."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper and provide a final decision.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in object detection, focusing on technical correctness and novelty.', 'agent_name': 'ReviewerAgent1', 'base_model': 'gpt-4', 'system_prompt': 'You are an expert reviewer for ICLR. You have a strong background in object detection and are very critical of technical correctness and novelty. Provide a detailed review, including strengths and weaknesses, and a final recommendation (Accept/Reject).'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are an expert reviewer for ICLR. You have a strong background in few-shot learning and are interested in the practical impact and generalizability of the method. Provide a detailed review, including strengths and weaknesses, and a final recommendation (Accept/Reject).', 'description': 'Expert in few-shot learning, focusing on the practical impact and generalizability of the method.', 'agent_name': 'ReviewerAgent2', 'base_model': 'gpt-4'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are an expert reviewer for ICLR. You have broad expertise in computer vision and focus on the clarity, presentation, and overall contribution of the paper. Provide a detailed review, including strengths and weaknesses, and a final recommendation (Accept/Reject).', 'base_model': 'gpt-4', 'description': 'Broad expertise in computer vision, focusing on clarity, presentation, and overall contribution.', 'agent_name': 'ReviewerAgent3'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.98\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in object detection, focusing on technical correctness and novelty.\",\n  \"agent_name\": \"ReviewerAgent1\",\n  \"base_model\": \"gpt-4\",\n  \"system_prompt\": \"You are an expert reviewer for ICLR. You have a strong background in object detection and are very critical of technical correctness and novelty. Provide a detailed review, including strengths and weaknesses, and a final recommendation (Accept/Reject).\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.975\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert reviewer for ICLR. You have a strong background in few-shot learning and are interested in the practical impact and generalizability of the method. Provide a detailed review, including strengths and weaknesses, and a final recommendation (Accept/Reject).\",\n  \"description\": \"Expert in few-shot learning, focusing on the practical impact and generalizability of the method.\",\n  \"agent_name\": \"ReviewerAgent2\",\n  \"base_model\": \"gpt-4\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.97\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert reviewer for ICLR. You have broad expertise in computer vision and focus on the clarity, presentation, and overall contribution of the paper. Provide a detailed review, including strengths and weaknesses, and a final recommendation (Accept/Reject).\",\n  \"base_model\": \"gpt-4\",\n  \"description\": \"Broad expertise in computer vision, focusing on clarity, presentation, and overall contribution.\",\n  \"agent_name\": \"ReviewerAgent3\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.98}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.975}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.97}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have created three reviewer agents. Now I will ask each agent to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerAgent1', 'prompt': 'Review the following paper and provide a final recommendation (Accept/Reject). The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s. INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"Okay, I have reviewed the paper \"Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s.\" Here's a detailed breakdown of my assessment:\n\n**Summary**\n\nThe paper addresses the problem of unfairness in few-shot object detection due to aspect ratio distribution shifts between base and novel classes. The authors argue that standard few-shot training procedures can lead to RPNs biased towards the aspect ratios of the base classes, hindering the detection of novel objects with significantly different aspect ratios. To mitigate this, they propose \"Cooperating RPNs\" (CoRPNs), a method that trains multiple RPN classifiers with diversity and cooperation losses to improve the RPN's robustness to aspect ratio shifts. They introduce new \"ARShift\" benchmarks based on PASCAL VOC, MS-COCO, and LVIS, which are specifically designed to evaluate detector performance under severe aspect ratio distribution shifts. The paper demonstrates that CoRPNs achieve state-of-the-art performance on these ARShift benchmarks while remaining competitive on standard few-shot detection datasets.\n\n**Strengths**\n\n*   **Novel Problem Identification:** The paper clearly identifies and articulates a previously unaddressed problem in few-shot object detection \u2013 the unfairness caused by aspect ratio distribution shifts. This is a valuable contribution, as it highlights a limitation of existing methods and motivates the need for more robust RPNs.\n*   **ARShift Benchmarks:** The introduction of the ARShift benchmarks is a significant strength. These benchmarks provide a controlled environment for evaluating the performance of detectors under realistic aspect ratio distribution shifts, enabling a more thorough assessment of fairness.\n*   **Technical Approach (CoRPNs):** The proposed CoRPN architecture is well-motivated and technically sound. The diversity and cooperation losses are designed to encourage the RPN classifiers to specialize without overspecializing, leading to improved robustness and generalization. The selection of the most certain RPN at training time is also clever.\n*   **Comprehensive Experiments:** The paper presents a comprehensive set of experiments on both standard and ARShift benchmarks. The results demonstrate the effectiveness of CoRPNs in addressing aspect ratio unfairness while maintaining competitive performance on standard datasets. Ablation studies provide valuable insights into the contribution of each component of the CoRPN architecture. Comparisons to other state-of-the-art methods are also performed.\n*   **Clarity and Writing Quality:** The paper is well-written and easy to follow. The problem is clearly defined, the proposed approach is well-explained, and the experimental results are presented in a clear and organized manner. The figures are illustrative, and the tables are well-formatted.\n*   **Reproducibility:** The authors provide sufficient details about their experimental setup, training procedures, and hyperparameters, making the paper relatively reproducible.\n\n**Weaknesses**\n\n*   **Incremental Novelty:** While the problem addressed is novel and important, the technical solution (CoRPNs) might be considered somewhat incremental. Training multiple models with diversity-encouraging losses is a fairly common technique. The main novelty lies in its application to the RPN in the context of aspect ratio fairness. However, the performance gains on the ARShift datasets clearly justify the approach.\n*   **Computational Overhead:** The use of multiple RPN classifiers inevitably increases the computational overhead of the detector. While the paper mentions the training and inference time and memory overhead in the appendix, a more detailed analysis of the computational cost could be beneficial.\n*   **Hyperparameter Sensitivity:** The performance of CoRPNs depends on the proper tuning of the hyperparameters \u03bbd and \u03bbc (diversity and cooperation loss weights). The paper does not provide a detailed analysis of the sensitivity of the results to these hyperparameters.\n\n**Specific Comments and Questions:**\n\n*   It would be helpful to visualize the actual proposals generated by the different RPNs within the CoRPN. This would further support the claim that the RPNs are indeed specializing in different aspect ratios.\n*   Could the authors elaborate on the limitations of their approach? For example, are there certain types of aspect ratio distribution shifts that CoRPNs cannot effectively handle?\n*   The paper mentions that CoRPNs could be combined with other approaches for further improvements. Could the authors provide specific examples of such combinations and discuss the potential benefits?\n*   In the ablation study, it would be useful to see the performance of the baseline model trained with a similar number of parameters as the CoRPN model. This would help to isolate the effect of the CoRPN architecture from the effect of simply increasing the model capacity.\n\n**Final Recommendation**\n\nDespite the minor weaknesses, the strengths of this paper significantly outweigh its limitations. The paper addresses a novel and important problem in few-shot object detection, proposes a technically sound solution, and provides comprehensive experimental evidence to support its claims. The introduction of the ARShift benchmarks is a valuable contribution to the field. The paper is well-written and easy to follow. Therefore, I recommend **Accept**.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.96921307\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ReviewerAgent1\",\n  \"prompt\": \"Review the following paper and provide a final recommendation (Accept/Reject). The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s. INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'Okay, I have reviewed the paper \"Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s.\" Here\\'s a detailed breakdown of my assessment:\\n\\n**Summary**\\n\\nThe paper addresses the problem of unfairness in few-shot object detection due to aspect ratio distribution shifts between base and novel classes. The authors argue that standard few-shot training procedures can lead to RPNs biased towards the aspect ratios of the base classes, hindering the detection of novel objects with significantly different aspect ratios. To mitigate this, they propose \"Cooperating RPNs\" (CoRPNs), a method that trains multiple RPN classifiers with diversity and cooperation losses to improve the RPN\\'s robustness to aspect ratio shifts. They introduce new \"ARShift\" benchmarks based on PASCAL VOC, MS-COCO, and LVIS, which are specifically designed to evaluate detector performance under severe aspect ratio distribution shifts. The paper demonstrates that CoRPNs achieve state-of-the-art performance on these ARShift benchmarks while remaining competitive on standard few-shot detection datasets.\\n\\n**Strengths**\\n\\n*   **Novel Problem Identification:** The paper clearly identifies and articulates a previously unaddressed problem in few-shot object detection \u2013 the unfairness caused by aspect ratio distribution shifts. This is a valuable contribution, as it highlights a limitation of existing methods and motivates the need for more robust RPNs.\\n*   **ARShift Benchmarks:** The introduction of the ARShift benchmarks is a significant strength. These benchmarks provide a controlled environment for evaluating the performance of detectors under realistic aspect ratio distribution shifts, enabling a more thorough assessment of fairness.\\n*   **Technical Approach (CoRPNs):** The proposed CoRPN architecture is well-motivated and technically sound. The diversity and cooperation losses are designed to encourage the RPN classifiers to specialize without overspecializing, leading to improved robustness and generalization. The selection of the most certain RPN at training time is also clever.\\n*   **Comprehensive Experiments:** The paper presents a comprehensive set of experiments on both standard and ARShift benchmarks. The results demonstrate the effectiveness of CoRPNs in addressing aspect ratio unfairness while maintaining competitive performance on standard datasets. Ablation studies provide valuable insights into the contribution of each component of the CoRPN architecture. Comparisons to other state-of-the-art methods are also performed.\\n*   **Clarity and Writing Quality:** The paper is well-written and easy to follow. The problem is clearly defined, the proposed approach is well-explained, and the experimental results are presented in a clear and organized manner. The figures are illustrative, and the tables are well-formatted.\\n*   **Reproducibility:** The authors provide sufficient details about their experimental setup, training procedures, and hyperparameters, making the paper relatively reproducible.\\n\\n**Weaknesses**\\n\\n*   **Incremental Novelty:** While the problem addressed is novel and important, the technical solution (CoRPNs) might be considered somewhat incremental. Training multiple models with diversity-encouraging losses is a fairly common technique. The main novelty lies in its application to the RPN in the context of aspect ratio fairness. However, the performance gains on the ARShift datasets clearly justify the approach.\\n*   **Computational Overhead:** The use of multiple RPN classifiers inevitably increases the computational overhead of the detector. While the paper mentions the training and inference time and memory overhead in the appendix, a more detailed analysis of the computational cost could be beneficial.\\n*   **Hyperparameter Sensitivity:** The performance of CoRPNs depends on the proper tuning of the hyperparameters \u03bbd and \u03bbc (diversity and cooperation loss weights). The paper does not provide a detailed analysis of the sensitivity of the results to these hyperparameters.\\n\\n**Specific Comments and Questions:**\\n\\n*   It would be helpful to visualize the actual proposals generated by the different RPNs within the CoRPN. This would further support the claim that the RPNs are indeed specializing in different aspect ratios.\\n*   Could the authors elaborate on the limitations of their approach? For example, are there certain types of aspect ratio distribution shifts that CoRPNs cannot effectively handle?\\n*   The paper mentions that CoRPNs could be combined with other approaches for further improvements. Could the authors provide specific examples of such combinations and discuss the potential benefits?\\n*   In the ablation study, it would be useful to see the performance of the baseline model trained with a similar number of parameters as the CoRPN model. This would help to isolate the effect of the CoRPN architecture from the effect of simply increasing the model capacity.\\n\\n**Final Recommendation**\\n\\nDespite the minor weaknesses, the strengths of this paper significantly outweigh its limitations. The paper addresses a novel and important problem in few-shot object detection, proposes a technically sound solution, and provides comprehensive experimental evidence to support its claims. The introduction of the ARShift benchmarks is a valuable contribution to the field. The paper is well-written and easy to follow. Therefore, I recommend **Accept**.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.96921307}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Review the following paper and provide a final recommendation (Accept/Reject). The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s. INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.', 'agent_name': 'ReviewerAgent2'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s\"\n\n**Overall Recommendation: Accept**\n\n**Summary:**\n\nThis paper addresses an important and previously unremarked issue in few-shot object detection: the unfairness arising from aspect ratio distribution shift between base and novel classes. The authors demonstrate that existing RPNs, trained primarily on base classes, can be biased against novel classes with significantly different aspect ratios, leading to poor detection performance. To mitigate this, they propose CoRPN\u2019s (Cooperating RPN\u2019s), a novel approach that trains multiple RPN classifiers with cooperation and diversity losses. The paper introduces ARShift benchmarks designed to evaluate performance under significant aspect ratio distribution shifts and demonstrates that CoRPN\u2019s outperform state-of-the-art methods on these challenging benchmarks while maintaining competitive performance on conventional benchmarks.\n\n**Strengths:**\n\n*   **Novel Problem Identification:** The paper identifies and articulates a clear and previously overlooked problem in few-shot detection: aspect ratio distribution shift between base and novel classes, leading to RPN unfairness. This is a valuable contribution to the field.\n*   **Practical Significance:** The identified problem is not just theoretical. The paper argues convincingly that this aspect ratio shift is likely to occur in real-world few-shot detection applications, making the problem and proposed solution highly relevant.\n*   **Well-Designed Approach:** The proposed CoRPN approach is well-motivated and technically sound. The combination of cooperation and diversity losses provides a good balance between specialization and generalization across aspect ratios. The chosen OR strategy during evaluation is also clever.\n*   **Thorough Evaluation:** The paper provides a comprehensive evaluation of CoRPN\u2019s on both conventional and the newly introduced ARShift benchmarks. The results clearly demonstrate the effectiveness of CoRPN\u2019s in addressing aspect ratio unfairness. The ablation studies further strengthen the claims by demonstrating the necessity of both diversity and cooperation losses. Furthermore, the extensive comparisons with state-of-the-art methods showcase the strong performance of the proposed approach.\n*   **Clear and Well-Written:** The paper is well-structured and clearly written, making it easy to understand the problem, proposed solution, and experimental results.\n\n**Weaknesses:**\n\n*   **Complexity:** While the idea is clearly presented, the implementation does add some complexity. The practical impact of this added complexity in terms of memory and runtime should be thoroughly addressed. The appendix does partially address this, but it should be more prominent in the main paper.\n*   **Hyperparameter Sensitivity:** The method introduces several hyperparameters (\u03bbd, \u03bbc, \u03d5) that require tuning. The paper should provide more guidance on how to choose appropriate values for these hyperparameters or discuss their sensitivity. While the ablation study touches on this, a more comprehensive discussion would be beneficial.\n*   **Limited Theoretical Analysis:** While the paper is strong on empirical results, it lacks a deeper theoretical analysis of why CoRPN\u2019s work. Providing some theoretical insights into the benefits of cooperation and diversity losses would further strengthen the paper.\n\n**Detailed Comments and Questions:**\n\n*   **Novelty:** The main strength of the paper is the identification of a novel problem in few-shot detection. However, the proposed solution, while effective, builds upon existing concepts of ensemble learning and diversity regularization. The authors should more explicitly acknowledge the connection to these related areas.\n*   **ARShift Benchmark:** The creation of the ARShift benchmarks is a valuable contribution. However, the manual selection of classes for these benchmarks introduces a potential bias. The authors should discuss this limitation and consider alternative approaches for creating these benchmarks in future work.\n*   **Comparison with Ensemble of Experts:** The paper compares CoRPN\u2019s with a \"naive ensemble of experts.\" It's important to clarify what \"naive\" means in this context. Was this baseline properly optimized? Could a more sophisticated ensemble method have achieved better results? Providing more details about this baseline would strengthen the comparison.\n*   **OR Strategy Rationale:** The rationale behind the \"OR\" strategy for RPN selection at test time is not entirely clear. Why is taking the maximum confidence score better than other aggregation methods (e.g., averaging)? Providing more justification or exploring alternative aggregation strategies would be helpful.\n*   **Base Class Performance:** While the paper reports base class performance and emphasizes that CoRPN's don't forget base classes, it would be interesting to see if the improved fairness on novel classes comes at any cost to base class performance. Are there any cases where CoRPN's perform worse on base classes compared to the baseline?\n*   **Runtime and Memory Overhead:** What is the additional runtime and memory overhead introduced by using multiple RPN classifiers? A detailed analysis of these costs is crucial for assessing the practicality of CoRPN\u2019s, especially in resource-constrained environments. This is partially addressed in the Appendix, but should be brought up to the main paper.\n*   **Broader Impact:** While not a primary focus, discussing the potential broader impacts of this work, particularly regarding fairness in object detection across different object categories, would be beneficial.\n\n**Minor Suggestions:**\n\n*   In the Introduction, it would be helpful to briefly explain what RPN stands for.\n*   In Section 3.3, clarify the term \"OR strategy\" earlier in the text.\n*   Proofread the paper carefully to fix minor typos and grammatical errors.\n\n**Final Recommendation:**\n\nDespite the minor weaknesses mentioned above, the strengths of this paper far outweigh its limitations. The identification of aspect ratio distribution shift as a significant problem in few-shot detection, the well-designed CoRPN approach, and the thorough experimental evaluation make this a valuable contribution to the field. Therefore, I recommend **Accept**. The authors are encouraged to address the comments and suggestions above in the final version of the paper.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.96842614\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and provide a final recommendation (Accept/Reject). The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s. INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.\",\n  \"agent_name\": \"ReviewerAgent2\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s\"\\n\\n**Overall Recommendation: Accept**\\n\\n**Summary:**\\n\\nThis paper addresses an important and previously unremarked issue in few-shot object detection: the unfairness arising from aspect ratio distribution shift between base and novel classes. The authors demonstrate that existing RPNs, trained primarily on base classes, can be biased against novel classes with significantly different aspect ratios, leading to poor detection performance. To mitigate this, they propose CoRPN\u2019s (Cooperating RPN\u2019s), a novel approach that trains multiple RPN classifiers with cooperation and diversity losses. The paper introduces ARShift benchmarks designed to evaluate performance under significant aspect ratio distribution shifts and demonstrates that CoRPN\u2019s outperform state-of-the-art methods on these challenging benchmarks while maintaining competitive performance on conventional benchmarks.\\n\\n**Strengths:**\\n\\n*   **Novel Problem Identification:** The paper identifies and articulates a clear and previously overlooked problem in few-shot detection: aspect ratio distribution shift between base and novel classes, leading to RPN unfairness. This is a valuable contribution to the field.\\n*   **Practical Significance:** The identified problem is not just theoretical. The paper argues convincingly that this aspect ratio shift is likely to occur in real-world few-shot detection applications, making the problem and proposed solution highly relevant.\\n*   **Well-Designed Approach:** The proposed CoRPN approach is well-motivated and technically sound. The combination of cooperation and diversity losses provides a good balance between specialization and generalization across aspect ratios. The chosen OR strategy during evaluation is also clever.\\n*   **Thorough Evaluation:** The paper provides a comprehensive evaluation of CoRPN\u2019s on both conventional and the newly introduced ARShift benchmarks. The results clearly demonstrate the effectiveness of CoRPN\u2019s in addressing aspect ratio unfairness. The ablation studies further strengthen the claims by demonstrating the necessity of both diversity and cooperation losses. Furthermore, the extensive comparisons with state-of-the-art methods showcase the strong performance of the proposed approach.\\n*   **Clear and Well-Written:** The paper is well-structured and clearly written, making it easy to understand the problem, proposed solution, and experimental results.\\n\\n**Weaknesses:**\\n\\n*   **Complexity:** While the idea is clearly presented, the implementation does add some complexity. The practical impact of this added complexity in terms of memory and runtime should be thoroughly addressed. The appendix does partially address this, but it should be more prominent in the main paper.\\n*   **Hyperparameter Sensitivity:** The method introduces several hyperparameters (\u03bbd, \u03bbc, \u03d5) that require tuning. The paper should provide more guidance on how to choose appropriate values for these hyperparameters or discuss their sensitivity. While the ablation study touches on this, a more comprehensive discussion would be beneficial.\\n*   **Limited Theoretical Analysis:** While the paper is strong on empirical results, it lacks a deeper theoretical analysis of why CoRPN\u2019s work. Providing some theoretical insights into the benefits of cooperation and diversity losses would further strengthen the paper.\\n\\n**Detailed Comments and Questions:**\\n\\n*   **Novelty:** The main strength of the paper is the identification of a novel problem in few-shot detection. However, the proposed solution, while effective, builds upon existing concepts of ensemble learning and diversity regularization. The authors should more explicitly acknowledge the connection to these related areas.\\n*   **ARShift Benchmark:** The creation of the ARShift benchmarks is a valuable contribution. However, the manual selection of classes for these benchmarks introduces a potential bias. The authors should discuss this limitation and consider alternative approaches for creating these benchmarks in future work.\\n*   **Comparison with Ensemble of Experts:** The paper compares CoRPN\u2019s with a \"naive ensemble of experts.\" It\\'s important to clarify what \"naive\" means in this context. Was this baseline properly optimized? Could a more sophisticated ensemble method have achieved better results? Providing more details about this baseline would strengthen the comparison.\\n*   **OR Strategy Rationale:** The rationale behind the \"OR\" strategy for RPN selection at test time is not entirely clear. Why is taking the maximum confidence score better than other aggregation methods (e.g., averaging)? Providing more justification or exploring alternative aggregation strategies would be helpful.\\n*   **Base Class Performance:** While the paper reports base class performance and emphasizes that CoRPN\\'s don\\'t forget base classes, it would be interesting to see if the improved fairness on novel classes comes at any cost to base class performance. Are there any cases where CoRPN\\'s perform worse on base classes compared to the baseline?\\n*   **Runtime and Memory Overhead:** What is the additional runtime and memory overhead introduced by using multiple RPN classifiers? A detailed analysis of these costs is crucial for assessing the practicality of CoRPN\u2019s, especially in resource-constrained environments. This is partially addressed in the Appendix, but should be brought up to the main paper.\\n*   **Broader Impact:** While not a primary focus, discussing the potential broader impacts of this work, particularly regarding fairness in object detection across different object categories, would be beneficial.\\n\\n**Minor Suggestions:**\\n\\n*   In the Introduction, it would be helpful to briefly explain what RPN stands for.\\n*   In Section 3.3, clarify the term \"OR strategy\" earlier in the text.\\n*   Proofread the paper carefully to fix minor typos and grammatical errors.\\n\\n**Final Recommendation:**\\n\\nDespite the minor weaknesses mentioned above, the strengths of this paper far outweigh its limitations. The identification of aspect ratio distribution shift as a significant problem in few-shot detection, the well-designed CoRPN approach, and the thorough experimental evaluation make this a valuable contribution to the field. Therefore, I recommend **Accept**. The authors are encouraged to address the comments and suggestions above in the final version of the paper.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.96842614}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Review the following paper and provide a final recommendation (Accept/Reject). The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s. INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.', 'agent_name': 'ReviewerAgent3'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s\"\n\n**Overall Assessment:**\n\nThis paper presents a novel and well-motivated approach to address the problem of unfairness in few-shot object detection caused by aspect ratio distribution shift between base and novel classes. The proposed method, CoRPN's, leverages multiple cooperating RPN classifiers trained with diversity and cooperation losses to improve the robustness and fairness of the region proposal process. The experimental results on newly introduced ARShift benchmarks and conventional benchmarks demonstrate the effectiveness of the proposed approach. Overall, the paper is well-written, technically sound, and makes a valuable contribution to the field of few-shot object detection. I recommend **Accept**.\n\n**Strengths:**\n\n*   **Novel Problem Formulation:** The paper identifies and articulates a significant issue of aspect ratio distribution shift in few-shot object detection, which has been largely overlooked in previous works.\n*   **Well-Motivated Approach:** The proposed CoRPN's method is well-motivated and designed to address the identified problem. The use of multiple RPN classifiers with diversity and cooperation losses is a clever way to encourage specialization and collaboration among the RPNs.\n*   **Strong Experimental Results:** The paper presents comprehensive experimental results on both newly introduced ARShift benchmarks and conventional benchmarks. The results consistently demonstrate the superiority of CoRPN's over existing methods, especially in the challenging ARShift settings. The ablation studies provide further insights into the contribution of each component of the proposed method.\n*   **Clarity and Presentation:** The paper is well-written and easy to follow. The technical details are clearly explained, and the experimental results are presented in a concise and informative manner.\n*   **Reproducibility:** The paper provides sufficient details about the experimental setup, training procedure, and hyperparameters, which should facilitate the reproducibility of the results.\n\n**Weaknesses:**\n\n*   **Complexity:** The proposed CoRPN's method introduces additional complexity to the existing few-shot detection frameworks. While the performance gains are significant, the increased complexity might be a concern for some applications. However, the authors could address this by providing more analysis of the computational overhead and memory requirements of CoRPN's in the appendix.\n*   **Parameter Sensitivity:** The performance of CoRPN's might be sensitive to the choice of hyperparameters, such as the diversity loss weight (\u03bbd) and cooperation loss weight (\u03bbc). The paper could benefit from a more detailed discussion on how to choose these hyperparameters and their impact on the performance of CoRPN's.\n*   **Limited Theoretical Analysis:** The paper could benefit from a more theoretical analysis of the proposed diversity and cooperation losses. While the empirical results are strong, a theoretical understanding of why these losses work could further strengthen the paper.\n\n**Detailed Comments:**\n\n*   **Introduction:** The introduction clearly presents the problem of aspect ratio distribution shift and motivates the need for a more robust RPN. The contributions of the paper are clearly stated.\n*   **Related Work:** The related work section provides a comprehensive overview of the existing literature on object detection, few-shot object detection, and proposal generation. The authors clearly position their work within the existing literature.\n*   **Our Approach:** The proposed CoRPN's method is well-explained, and the motivations behind the diversity and cooperation losses are clear. The technical details of the method are presented in a concise and easy-to-understand manner.\n*   **Experiments:** The experimental setup is well-defined, and the evaluation metrics are appropriate. The results on the ARShift benchmarks clearly demonstrate the effectiveness of CoRPN's in addressing the aspect ratio distribution shift problem. The ablation studies provide valuable insights into the contribution of each component of the proposed method.\n*   **Appendix:** The appendix provides additional experimental results and details that support the approach in the main paper. The information in the appendix is well-organized and helpful for understanding the paper.\n\n**Recommendations for Improvement:**\n\n*   Include a more detailed analysis of the computational overhead and memory requirements of CoRPN's in the appendix.\n*   Provide a more detailed discussion on how to choose the hyperparameters (\u03bbd and \u03bbc) and their impact on the performance of CoRPN's.\n*   Consider adding a more theoretical analysis of the diversity and cooperation losses.\n\n**Final Recommendation:**\n\nI recommend **Accept** this paper. The paper presents a novel and well-motivated approach to address an important problem in few-shot object detection. The experimental results are strong, and the paper is well-written and easy to follow. The proposed CoRPN's method has the potential to significantly improve the performance and fairness of few-shot object detectors.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.96763921\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and provide a final recommendation (Accept/Reject). The paper title is: Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s. INTRODUCTION: Most state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN\u2019s predictions. Objects in large-scale object detection datasets (e.g. COCO Lin et al. (2014); LVIS Gupta et al. (2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as unfairness (the RPN does not deal fairly with different aspect ratios). Unfairness occurs because the RPN sees few or no examples of the novel classes during training Kang et al. (2019); Wang et al. (2020); Yan et al. (2019). To date, unfairness has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence that this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art few-shot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits. In few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN\u2019s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS Everingham et al. (2010); Lin et al. (2014); Kang et al. (2019); Wang et al. (2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution. Our contributions: (1) We show unfairness has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detector. (3) We show that performance improvements resulting from CoRPN\u2019s results from improved fairness. (4) Our CoRPN\u2019s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair. 2 RELATED WORK: Object Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category Endres & Hoiem (2010); van de Sande et al. (2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants Girshick (2015); Girshick et al. (2014); He et al. (2017); Ren et al. (2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants Bochkovskiy et al. (2020); Redmon & Farhadi (2017); Redmon et al. (2016); Redmon & Farhadi (2018), SSD Liu et al. (2016), point-based detectors such as CornerNet Law & Deng (2018) and ExtremeNet Zhou et al. (2019), and emerging transformer-based methods exemplified by DETR Carion et al. (2020)). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too. Few-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples Chen et al. (2018); Kang et al. (2019); Schwartz et al. (2019), and state-ofthe-art methods are usually serial Wang et al. (2019); Yan et al. (2019); Wang et al. (2020); Fan et al. (2020); Wu et al. (2020); Xiao & Marlet (2020); Yang et al. (2020); Li et al. (2021a); Hu et al. (2021); Zhang et al. (2021); Li et al. (2021b); Zhu et al. (2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA Wang et al. (2020) shows that a simple two-stage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning Wang et al. (2019); Yan et al. (2019); Hu et al. (2021); Zhang et al. (2021), metric learning Han et al. (2021); Wu et al. (2021); Yang et al. (2020), refinement Wu et al. (2020); Li et al. (2021b), feature reweighting Kang et al. (2019), semantic relations Zhu et al. (2021), augmentation Li & Li (2021); Zhang & Wang (2021), and margin loss Li et al. (2021a). Other work (Fan et al. (2021)) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods \u2013 the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020) \u2013 as our main baselines. Few-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks Kang et al. (2019); Wang et al. (2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly. Proposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which is usually seen as robust to few-shot issues because there are many base examples. Sun et al. (2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process Hsieh et al. (2019); Fan et al. (2020); Xiao & Marlet (2020); Osokin et al. (2020), as well as re-ranking proposals based on similarity with query images Hsieh et al. (2019); Fan et al. (2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories \u2013 current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN\u2019s without substantial loss of performance for the base classes. 3 OUR APPROACH: We believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN Qiao et al. (2021) and the widely used TFA Wang et al. (2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na\u0131\u0308ve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN\u2019s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses. 3.1 BACKGROUND: We use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. As shown in Figure 3, the training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights for novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU\u22650.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes. We adopt the widely-used Faster R-CNN Ren et al. (2015) as our base model. As shown in Figure 3, Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. After passing the ROI pooling layer, the predictor\u2019s head classifies and localizes each box. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA Wang et al. (2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN Qiao et al. (2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor. 3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT: In the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of box scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions. We manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na\u0131\u0308ve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN Qiao et al. (2021), this approach represented by \u2018DeFRCN Ensemble of Experts\u2019 performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN\u2019s, shows a large improvement over the baselines in Tables 1, 2 and 3. 3.3 LEARNING COOPERATING RPN\u2019S (CORPN\u2019S): We would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. We use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy \u2013 a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN\u2019s can specialize to a degree, though we do not allow the RPN\u2019s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model\u2019s fairness with respect to aspect ratio distribution. As Figure 3 shows, Faster R-CNN\u2019s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN\u2019s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN\u2019s. An RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed). We use the following strategy. For a specific anchor box i, each RPN j (of the N RPN\u2019s) outputs a raw score rji , indicating if the box is a foreground box or not: ri = [r 1 i , r 2 i , . . . , r N i ]. After applying a sigmoid, the jth RPN produces the foreground probability f ji = \u03c3(r j i ) for anchor box i. We choose the score from the j\u2217th RPN such that j\u2217 = argminj min{f j i , 1\u2212 f j i }, (1) namely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j\u2217th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN\u2019s. Other than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN\u2019s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj \u2217 CE + \u03bbdLdiv + \u03bbcLcoop, (2) where \u03bbd and \u03bbc are trade-off hyperparameters. 3.4 ENFORCING DIVERSITY: We do not want our RPN\u2019s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN\u2019s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN\u2019s. Given a set of NA anchor boxes, the N RPN\u2019s produce an N by NA matrix of probabilities F = [f1, f2, . . . , fN ]T . The covariance matrix \u03a3(F) is \u03a3(F) = E[(f j \u2212 E[f j ])(fk \u2212 E[fk])T ]. (3) We define the diversity loss Ldiv by the log determinant loss Ldiv := \u2212 log(det(\u03a3(F))). (4) By the diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training. 3.5 LEARNING TO COOPERATE: We also want the RPN\u2019s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss Li,jcoop := max{0, \u03d5\u2212 f j i }, (5) where \u03d5 is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN\u2019s probability assigning to a foreground box. If an RPN\u2019s response is below \u03d5, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN\u2019s. 4 EXPERIMENTS: Benchmarks. We propose a new base/novel split on both the PASCAL VOC Everingham et al. (2010), MS-COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split Kang et al. (2019); Wang et al. (2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix. Apart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks Kang et al. (2019); Wang et al. (2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained Russakovsky et al. (2015) ResNet-101 as the backbone, unless otherwise noted. Training Procedure. Our training and fine-tuning procedures are consistent with previous work TFA Wang et al. (2020) and DeFRCN Qiao et al. (2021). On PASCAL VOC, at phase 1 base classes training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters. Baselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN Qiao et al. (2021), and a widely-used previous work TFA Wang et al. (2020). Our approach incorporates CoRPN\u2019s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD Fan et al. (2020), MPSR Wu et al. (2020), FSDetView Xiao & Marlet (2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb|+ |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work Fan et al. (2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results. 4.1 MAIN RESULTS: Our evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. CoRPN\u2019s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN\u2019s greatly outperform the baseline TFA Wang et al. (2020), DeFRCN Qiao et al. (2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset Gupta et al. (2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN\u2019s outperform the state-of-the-art DeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN\u2019s improve the quality of detection in such scenarios. CoRPN\u2019s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN\u2019s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model. CoRPN\u2019s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation Hsieh et al. (2019); Fan et al. (2020) under the standard evaluation procedure. We believe CoRPN\u2019s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation Wu et al. (2020), incorporating metric learning Yang et al. (2020), or adding feature aggregation module Xiao & Marlet (2020) for further improvements. CoRPN\u2019s don\u2019t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes \u2013 there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes. 4.2 ABLATION STUDY: We investigate how the proposals of CoRPN\u2019s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN\u2019s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN\u2019s to improve fairness; (3) (in Section F of the appendix) CoRPN\u2019s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping. Specialization for aspect ratios emerges in CoRPN\u2019s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN\u2019s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN\u2019s learn a more flexible specialization, improving the performance. CoRPN\u2019s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN\u2019s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN\u2019s with the optimal hyperparameter. 5 CONCLUSION: We identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN\u2019s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN\u2019s with diversity and cooperation losses. A APPENDIX: This appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.\",\n  \"agent_name\": \"ReviewerAgent3\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Improving Aspect Ratio Distribution Fairness in Detector Pretraining via Cooperating RPN\u2019s\"\\n\\n**Overall Assessment:**\\n\\nThis paper presents a novel and well-motivated approach to address the problem of unfairness in few-shot object detection caused by aspect ratio distribution shift between base and novel classes. The proposed method, CoRPN\\'s, leverages multiple cooperating RPN classifiers trained with diversity and cooperation losses to improve the robustness and fairness of the region proposal process. The experimental results on newly introduced ARShift benchmarks and conventional benchmarks demonstrate the effectiveness of the proposed approach. Overall, the paper is well-written, technically sound, and makes a valuable contribution to the field of few-shot object detection. I recommend **Accept**.\\n\\n**Strengths:**\\n\\n*   **Novel Problem Formulation:** The paper identifies and articulates a significant issue of aspect ratio distribution shift in few-shot object detection, which has been largely overlooked in previous works.\\n*   **Well-Motivated Approach:** The proposed CoRPN\\'s method is well-motivated and designed to address the identified problem. The use of multiple RPN classifiers with diversity and cooperation losses is a clever way to encourage specialization and collaboration among the RPNs.\\n*   **Strong Experimental Results:** The paper presents comprehensive experimental results on both newly introduced ARShift benchmarks and conventional benchmarks. The results consistently demonstrate the superiority of CoRPN\\'s over existing methods, especially in the challenging ARShift settings. The ablation studies provide further insights into the contribution of each component of the proposed method.\\n*   **Clarity and Presentation:** The paper is well-written and easy to follow. The technical details are clearly explained, and the experimental results are presented in a concise and informative manner.\\n*   **Reproducibility:** The paper provides sufficient details about the experimental setup, training procedure, and hyperparameters, which should facilitate the reproducibility of the results.\\n\\n**Weaknesses:**\\n\\n*   **Complexity:** The proposed CoRPN\\'s method introduces additional complexity to the existing few-shot detection frameworks. While the performance gains are significant, the increased complexity might be a concern for some applications. However, the authors could address this by providing more analysis of the computational overhead and memory requirements of CoRPN\\'s in the appendix.\\n*   **Parameter Sensitivity:** The performance of CoRPN\\'s might be sensitive to the choice of hyperparameters, such as the diversity loss weight (\u03bbd) and cooperation loss weight (\u03bbc). The paper could benefit from a more detailed discussion on how to choose these hyperparameters and their impact on the performance of CoRPN\\'s.\\n*   **Limited Theoretical Analysis:** The paper could benefit from a more theoretical analysis of the proposed diversity and cooperation losses. While the empirical results are strong, a theoretical understanding of why these losses work could further strengthen the paper.\\n\\n**Detailed Comments:**\\n\\n*   **Introduction:** The introduction clearly presents the problem of aspect ratio distribution shift and motivates the need for a more robust RPN. The contributions of the paper are clearly stated.\\n*   **Related Work:** The related work section provides a comprehensive overview of the existing literature on object detection, few-shot object detection, and proposal generation. The authors clearly position their work within the existing literature.\\n*   **Our Approach:** The proposed CoRPN\\'s method is well-explained, and the motivations behind the diversity and cooperation losses are clear. The technical details of the method are presented in a concise and easy-to-understand manner.\\n*   **Experiments:** The experimental setup is well-defined, and the evaluation metrics are appropriate. The results on the ARShift benchmarks clearly demonstrate the effectiveness of CoRPN\\'s in addressing the aspect ratio distribution shift problem. The ablation studies provide valuable insights into the contribution of each component of the proposed method.\\n*   **Appendix:** The appendix provides additional experimental results and details that support the approach in the main paper. The information in the appendix is well-organized and helpful for understanding the paper.\\n\\n**Recommendations for Improvement:**\\n\\n*   Include a more detailed analysis of the computational overhead and memory requirements of CoRPN\\'s in the appendix.\\n*   Provide a more detailed discussion on how to choose the hyperparameters (\u03bbd and \u03bbc) and their impact on the performance of CoRPN\\'s.\\n*   Consider adding a more theoretical analysis of the diversity and cooperation losses.\\n\\n**Final Recommendation:**\\n\\nI recommend **Accept** this paper. The paper presents a novel and well-motivated approach to address an important problem in few-shot object detection. The experimental results are strong, and the paper is well-written and easy to follow. The proposed CoRPN\\'s method has the potential to significantly improve the performance and fairness of few-shot object detectors.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.96763921}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "All three reviewers recommend accepting the paper.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Reject", "response_time": 157.86338782310486}
