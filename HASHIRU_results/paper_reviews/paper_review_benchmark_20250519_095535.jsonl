{"paper_id": "XZRmNjUMj0c", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search\n\n1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks (Elsken et al., 2019; Wistuba et al., 2019; Ren et al., 2020). NAS approaches automate the architecture design process and can achieve higher accuracy compared to human-designed architectures (Liu et al., 2019; Xie et al., 2019; Cai et al., 2019). However, the early NAS methods, such as reinforcement NAS (Zoph & Le, 2016), came with the price of expensive computation costs since every searched architecture needs to be trained from scratch, which makes the total search time unacceptable. To reduce the search cost of earlier NAS methods, the weight sharing technique has been proposed (Yu et al., 2020; Chen et al., 2020), among which the one-shot NAS method has attracted a lot of attention recently (Bender et al., 2018; Li et al., 2020). The one-shot NAS method is known as cost-efficient as it requires training a supernet only once. A supernet is a stack of basic blocks, each of which contains multiple choices. A candidate network architecture (defined as subnet) can be formed by selecting one choice for each block in the supernet, and its corresponding weights can be inherited from the supernet. During the architecture searching stage, candidate architectures are evaluated on the validation dataset and the best architecture, i.e., the architecture with the highest validation accuracy, is updated in every searching epoch of Evolutionary Algorithm (EA) (Real et al., 2019). Surprisingly, although training is commonly deemed as a lengthy and energy-consuming task, the architecture searching stage in one-shot NAS is much more costly (Cai et al., 2020) than training a supernet. The reason is that a new searching stage should be performed whenever a different searching scenario is given, e.g., different hardware constraints, learning tasks, and workloads, while the trained supernet can be reused. Hence, the numerous inferences on the subnets can take a much longer time than training a supernet only once. According to (You et al., 2020), searching can be 10 GPU days longer than supernet training when 10 different constraints/platforms are required. To tackle this problem, our work first makes a key observation that, for the first few continuous blocks of the candidate architecture (defined as continuous shallow blocks), their optimal choices can be determined at an early search epoch. Based on this observation, we propose to freeze the choices of continuous shallow blocks at the early search epoch, which means these choices will not be changed during the remaining search epochs. This strategy elaborately \u201ccreates\u201d redundant computations in the continuous shallow blocks since all candidates will share exactly the same architecture, inherited weights, and input validation data for the shallow blocks during later search epochs. Then we leverage such redundancy and propose a simple yet effective data reuse scheme to save large amounts of computations, thus further reducing time and energy cost. Specifically, we propose to reuse the last output of the continuous shallow blocks instead of re-computing it repeatedly throughout the remaining searching stage. Interestingly, we further discover that the freezing strategy may in turn help to determine the optimal choices of the subsequent blocks earlier. Such phenomenon enables us to keep freezing the choices of blocks progressively after the initial freezing, which will create more redundant computations of the blocks that possess the same architecture (choice), thus more computations can be saved. With the proposed freezing technique, the intermediate data (the last output of the continuous shallow blocks) of a certain subnet can be stored and reused during the evaluation of other subnets. However, as the searching stage requires to evaluate a subnet with a large batch (e.g., batch size = 5000) of input samples, storing the intermediate data of only one subnet may cause serious memory issues (Sec.3.5). Inspired by the Importance Sampling technique employed in many training methods (Zeng et al., 2021), we propose to sample the \u201cimportant\u201d input data that contribute more to distinguish the evaluation accuracy of the candidate subnets. More importantly, we empirically demonstrate that the important samples are shared across different subnets. Therefore, it only requires to store the intermediate data of important samples for one certain subnet, and then reuse it for all other subnets. We evaluate the proposed method on multiple benchmarks trained with the state-of-the-art approaches on the ImageNet dataset (Krizhevsky et al., 2012). The experimental results indicate superb performance in improving the search efficiency while maintaining the search performance with only 0.1% searching accuracy loss. Our contributions can be summarized as follows: \u00b7 We observe that, in the one-shot NAS evolutionary searching stage, the optimal architecture of shallow blocks is determined at the early searching stage. \u00b7 We propose to freeze the choices of continuous shallow blocks for all candidates at the early stage, and progressively freeze the choices of the subsequent blocks in the later stage. This approach creates a great amount of redundant computations, which provide us a good opportunity to reuse the intermediate data and reduce the searching time. \u00b7 To alleviate memory capacity issue for storing intermediate data, we leverage the concept of importance sampling and propose a distinguish-based sampling method to reduce the size of the intermediate data. \u00b7 We conduct extensive experiments on different benchmarks with our proposed methods. The evaluation results show that our method can achieve up to 55% time saving and 51% energy saving with 0.1% accuracy loss. 2 REVIEW OF ONE-SHOT NAS: Different from the traditional neural network training that aims to optimize weights given a network architecture, NAS seeks to optimize both weight and architecture at the same time. Conventional NAS methods (Zoph & Le, 2016; Baker et al., 2016; Zhong et al., 2018; Zela et al., 2018) have tried to solve these two optimization problems at a nested approach. However, these methods are usually prohibitively expensive because each architecture sampled from the search space has to be trained from the scratch and evaluated separately. Recent works (Bender et al., 2018; Pham et al., 2018; Cai et al., 2019; 2018) have proposed a weight sharing strategy to reduce high costs of architecture and weight searching procedure in conventional NAS. As one of the most popular weight sharing techniques, one-shot NAS achieves unprecedented search efficiency by decoupling the whole searching process into two stages: supernet training (Fig.1 (a))and subnet searching (Fig.1 (b)). One-shot NAS encodes the search space into a supernet and trains it only once. Then it allows sampled architectures directly inherit the weights from the supernet. As shown in Fig.1 (a), the commonly used chain-structured supernet (Suganuma et al., 2017) is a stack of blocks, and each block consists of multiple choices. Each choice can be different operations, e.g., a 3\u00d73 / 5\u00d75 convolutional layer, a max-pooling layer or an identity layer. We further define the first few blocks in the supernet as shallow blocks, while the subsequent blocks are defined as deep blocks. The second step of one-shot NAS is the searching stage which employs EA to find the subnet architecture with the highest validation accuracy. Each subnet can be obtained by selecting one choice for each block in the supernet, and its weights are inherited from the supernet. The overall workflow is shown in Fig.1 (b). At each search epoch, also known as generation, there are N candidate subnets in the candidate pool sampled from the supernet and evaluated using validation dataset. Then the candidate subnets are ranked together with the previous top candidates according to the validation accuracy. Candidates with Top-K validation accuracy are selected and then evolved into N new subnets by performing mutation and crossover operations (Real et al., 2019). The above process is repeated until reaching the maximum number of search epochs E, and the subnet with the highest accuracy can be obtained. The architecture of this subnet will be regarded as the optimal architecture since the validation accuracy obtained by using inherited weights is highly predictive on the accuracy obtained by training from scratch (Bender et al., 2018). Recently, many studies focused on improving the efficiency and accuracy of the supernet training stage by introducing various subnet sampling methods (Guo et al., 2020; Chu et al., 2021; You et al., 2020). Nevertheless, none of them considered the efficiency of the searching stage. The searching stage can be more time-consuming compared with supernet training (Cai et al., 2020). This is because the supernet only need to be trained once while numerous searching processes are required to search the optimal architecture for various deployment scenarios, e.g., different hardware platforms, workloads. Therefore, it is essential to improve the search efficiency. NASA (Ma et al., 2021) is the first work that focuses on accelerating the search process, and a NAS accelerator was proposed that utilizes network fusion based on the computation sharing and data reuse within a search generation. Different from their approach, we propose an algorithm-level optimization for one-shot NAS evolutionary search and exploit data sharing from both within generation and across generation levels, which achieves significant improvement on the searching efficiency. 3 METHODOLOGY: This section first introduces several key observations during the evolutionary searching phase, based on which the progressive choice freezing evolutionary search (PCF-ES) is then proposed, along with how it saves the computations during the searching stage. Finally, to mitigate the memory issue of our method, the distinguish-based importance sampling method is proposed to reduce the size of the intermediate data. 3.1 OBSERVATION: MAJORITY CHOICE IN CONTINUOUS SHALLOW BLOCKS: Figure 2 illustrates how the choices of blocks evolve across generations during the evolutionary search in SPOS (Guo et al., 2020). The right-up subgraphs record the choice evolution in shallow blocks. Especially, we take the 1st, the 2nd, and the 3rd blocks as the examples to show their choice evolution across the search generations, respectively. The right-bottom subgraphs are the choice evolution in deep blocks, and the 15th, the 16th, and the 17th blocks are used as the examples. Each curve in the subgraph indicates the percentage of subnets among all candidate subnets in a certain generation that select a certain choice for the block. As an example shown in the left part of the figure, at the i-th generation, the percentage of the blue choice in the 1st block equals to 80% if 40 out of 50 subnets select the blue choice. As can be seen in the right-up subgraphs, there is one choice whose percentage increases dramatically and surpasses all the other choices in the early generations (i \u2264 10, highlighted by the left of the blue dotted line). We define such choice as the majority choice. We further observed that the majority choice will be selected by more subnets and become even more dominating for the shallow blocks in the following generations. However, this is not the case for the deep blocks. In the right-bottom subgraphs, we observed that either the majority choice emerges at very later generations, or multiple curves intertwine with each other, thus there is no obvious majority choice for deep blocks. The above observations indicate that candidate subnets tend to vote majority choice in the shallow blocks at the early searching stage, while for deeper blocks the majority choice is unclear. 3.2 RELATION BETWEEN THE MAJORITY CHOICE AND THE OPTIMAL CHOICE: Majority choice is the choice that most of the candidate subnets \u201dthink\u201d it can lead to better accuracy during evolution. Intuitively, majority choice should be representative of optimal choice. To quantitatively analyze the relationship between the majority choice and the optimal choice, we first define a binary categorical choice distance D, where D equals to 0 if two choices are the same otherwise D equals to 1. Then we calculate four types of D for the choices in the continuous shallow blocks as follows: the average distance D between the non-majority choices and the block choices in the first generation (i.e., initial block choices), the average distance D between the non-majority choices and the block choices in the last generation (i.e., optimal block choices), the average distance D between the majority choices and the block choices in the first generation, and the average distance D between the majority choices and the block choices in the last generation. Here, all D denotes the averaged distance across all shallow blocks. Note that the non-majority choices are all the choices of shallow blocks when their average percentage is below an empirically predetermined threshold (0.7) at the early searching stage. The majority choices are obtained once we observe their average percentage rising above 0.7. The results of non-majority choices and majority choices are shown in Figure 3 (a) and (b), respectively. The x-axis (y-axis) represents the distance between the nonmajority/majority choices and the block choices of the first (last) generation. Each point represents an independent searching process with random seeds. As can be seen, for non-majority choices (Figure 3 (a)), the points are scattered in a symmetrical way, which means a certain non-majority choice can be either close or far away from the optimal choice. In contrast, for majority choices (Figure 3 (b)), most points are located at the right-bottom of the plane, which indicates that the majority choices are very close to the final optimal choices. 3.3 KEY INSIGHTS: CHOICE FREEZING: The above comparative experiment shows that majority choices are very close to optimal choices and can be safely used to approximate optimal choices. This provide a solid foundation for us to freeze the majority choices in the shallow blocks when they appear in the early searching stage. The motivation of freezing the block choices is illustrated in Fig.4. As shown in Fig 4 (a), assuming there are two candidate subnets to be evaluated sequentially at the current searching epoch, the choices of the first two blocks CB1,CB2 of Net1 and Net2 are {blue, red} and {yellow, red} respectively. Since the choices of these two shallow blocks are not totally the same (even though the choices of CB2 are the same), the intermediate feature maps (FM1, FM2) in Net1 and Net2 will be different. Hence, to obtain the evaluation results for both subnets, the computations of all the blocks are required. However, as shown in Figure 4 (b), if the choices in CB1 and CB2 are already majority choices in the previous searching epoch and we managed to freeze them at the i-th search generation, Net1 will have the same choices for CB1 and CB2 as Net2, so does Net3 to Net50 in the current generation and all other subnets in the following generations. Since one-shot NAS uses the same validation dataset as input, the FM1 and FM2 of these subnets will be exactly the same and the computations for CB1 and CB2 will become redundant. Therefore, one can simply store the FM2 and reuse it for all the subsequent evaluation of candidate subnets, with a significant computation savings of all frozen blocks. Note that the stored FMs can be reused both within and across generations by every subnet evaluation process, which brings more computation savings than NASA (Ma et al., 2021). 3.4 PROGRESSIVE CHOICE FREEZING EVOLUTIONARY SEARCH: To further explore the behaviour of the choices of deeper blocks, we keep monitoring the choice percentage of every subsequent block once the previous blocks are frozen in the candidate subnet. As shown in Figure 5, we conduct three independent search processes where the first 3, 6, and 8 continuous block choices are frozen according to the majority choice percentage threshold. For example, the orange curve represents the choice percentage of the 7th block B7 if we freeze the first 6 continuous blocks at the 5th generation (marked by the dot line). It can be seen that, for all three blocks in the figure, the choice percentage increases drastically after previous blocks are fixed. Since higher choice percentage means higher possibility that a majority choice is observed, freezing the previous block choices will help freeze the latter block choices. Based on this observation, we propose to freeze more block choices progressively when the previous ones are already frozen. The overall idea is implemented in our proposed Progressive Choice Freezing Evolutionary Search (PCF-ES) algorithm in Alg. 1. The population of the first generation is randomly generated (Line Algorithm 1 Progressive Choice Freezing Evolutionary Search Input: supernet weight WA, population size P , architecture constraints C, max generation T , validation dataset Dv , choice percentage threshold H , mutation rate r, number of blocks N , monitor size f Output: architecture with best validation performance 1: Random initialize P0 2: c = 0 3: for i = 1 : T do 4: if c = 0 then 5: for j = N : 1 do 6: average choice percentage Mj from Bj to B1 7: block ratio Brj from Bj to B1 8: if M > H and Brj > 0.5 then 9: a1\u223cjTop\u2212K = a 1\u223cj maj 10: c = 1 11: break 12: end if 13: end for 14: else 15: for x = f : 1 do 16: average choice percentage Mf from Bj+1 to Bj+f 17: if Mf > H and Brf > 0.5 then 18: a1\u223c(j+x)Top\u2212K = a 1\u223c(j+x) maj 19: end if 20: end for 21: end if 22: Pmutation = Mutation(Topk, r, C) 23: Pcrossover = Crossover(Topk,C) 24: P = Pmutation \u222a Pcrossover 25: Acc = Inference(WA, Dv, P ) 26: end for 27: Return architecture with highest accuracy 1) and c is a flag to check if there are block choices that have been frozen (Line 4). When c is 0 which implies no block choice has been frozen, we calculate and examine the average choice percentage of the first j blocks at each search generation, and j starts from N (the number of blocks in a subnet) to 1 because we aim to freeze as many block choices as possible at the beginning (Lines 5-6). Note that the average choice percentage Mj is the mean of choice percentage from block B1 to Bj . One obvious downside of the average choice percentage is that it cannot indicate the choice percentage of each block, thus the choices of some blocks with relatively low choice percentage may be mistakenly frozen when the averaged choice percentage across the investigated shallow blocks exceeds the threshold. We employ another metric, i.e., block ratio, to avoid the aggressive choice frozen. block ratio measures the ratio of blocks whose choice percentage exceeds the threshold. For example, if the average choice percentage of the first 10 blocks exceeds the threshold, among which only 7 of these blocks exceeds the threshold, then the block ratio is 70%. Only when the block ratio is above the block ratio threshold, we allow the block choices to be frozen, which ensures that most of the frozen block choices reach the required choice percentage. If both metrics, i.e., average choice percentage and block ratio, are above the thresholds (Line 8), choices of the first j blocks in all Top-K subnets are changed to the current majority choices (Line 9) and become frozen. Accordingly, we set flag c = 1 when the first j blocks are frozen. For the rest of the searching process, these block choices will not be affected by the mutation and crossover operations. On the other hand, if any of the two metrics is below the threshold and the flag c = 0, the algorithm jumps to population generation and generates new candidates normally (Lines 22-24). If the first j blocks can be frozen, we keep exploring chances to freeze more block choices during the later searching stage. The monitor size f is the maximum number of blocks we should monitor at each generation in the latter searching stage (Line 15). The purpose of using monitor size here is to prevent aggressive block freezing. Specifically, since the search space is already remarkably narrowed down in the initial freezing step (up to 8 continuous shallow blocks can be frozen at a time, while a supernet usually has a total of 20 blocks), keep freezing too many subsequent blocks (even if their average choice percentage and block ratio are above the threshold) can make the search space so small that finally leads to inaccurate search results. In the progressively freezing phase, the average choice percentage Mf is the mean of choice percentage from block Bj+1 to Bj+f (Line 16). If the Mf is above the threshold, blocks Bj+1 to Bj+f in Top-K subnets will be frozen based on the current majority choice. After all candidate subnets are generated, they are evaluated on the validation dataset and ranked based on the validation accuracy (Line 25). We keep track of the Top-K subnets and return the Top-1 subnet architecture when maximum generation is reached or all block choices are frozen. 3.5 DISTINGUISH-BASED IMPORTANCE SAMPLING: To achieve the potential computation savings, it\u2019s necessary to store the intermediate feature maps, e.g. FM2 in Fig.4, of all the input samples in the validation dataset, the size of which may exceed the memory capacity of the GPUs in some circumstances. For instance, for each input sample, the intermediate data size of the second block of subnets in SPOS on ImageNet is approximately 0.8 MB. If the validation dataset contains more than 5000 input images, the memory storage of all the intermediate feature maps could exceed a GPU\u2019s memory capacity with 4GB main memory. To tackle this problem, we first leverage Importance Sampling (Alain et al., 2015) to compress the intermediate data. The idea of importance sampling comes from the fact that not all the input samples contribute to distinguishing the evaluation accuracy of candidate subnets. For example, for images which are easy to be classified by most subnets, they barely contribute to distinguish the performance of the searched subnets. Inspired by the importance sampling during training (Zeng et al., 2021), we follow the similar principle to sample the important data for the subnets evaluation. Let x denotes the input data, p denotes the uniform distribution referred from random sample, q denotes the distribution adopted by importance sampling and f(x) denotes the evaluation accuracy of input data x. The unbiased estimation of evaluation accuracy can be obtained by Eq.(1). Ep[f(x)] = \u222b p(x)f(x)dx = Eq[ p(x) q(x) f(x)] (1) if q(x) > 0 whenever p(x) > 0. Moreover, the estimation variance of the distribution q is minimized when q(x)\u2217 = 1 Z p(x)||f(x)||2, and Z = \u222b p(x)||f(x)||2dx (2) In our design, we use the cross-entropy loss to approximate the evaluation accuracy and then calculate the important sampling distribution q(x)\u2217 based on Eq.(2). Although we can obtain the important samples for a certain subnet using the above method, we cannot reuse the intermediate data of these important samples for other subnet\u2019s evaluation unless the important samples are shared across different subnets. Therefore, we quantitatively analyze the importance of samples for different subnets after the shallow blocks are frozen. As shown in Fig.6, we present the importance distribution of 500 important samples (x-axis) for 50 different subnets (y-axis). The color represents the importance of a sample. It can be found that the sample with high importance (deep blue) for one subnet is also important for other subnets. Overall, the importance distribution of a certain subnet (a certain row) is similar to the importance distribution of other subnets (other rows). As a result, it is only necessary to store the intermediate data of important samples for one subnet and then reuse it for other subnets. 4 EXPERIMENT:  4.1 EXPERIMENT SETUP: Our experiments are conducted on widely used ImageNet(Krizhevsky et al., 2012), of which the validation dataset contains 50000 images at 1000 categories. Both supernet training and subnet searching processes are performed on Tesla P100 GPUs with 16 GB memory. We employ the chain structure supernet adopted with three benchmarks: SPOS (Guo et al., 2020), FairNAS (Chu et al., 2021) and GreedyNAS (You et al., 2020), whose supernets contain 20, 19 and 21 blocks, respectively. The supernets are trained with the parameter settings in the original papers. The maximum search generation is 20 and the population size is 50. At each generation, subnets with top-10 validation accuracy are kept as parent networks. 4.2 EXPERIMENT RESULTS: The search results on different benchmarks and search methods are shown in Table 1. Evolutionary represents the original evolutionary algorithm used in (Guo et al., 2020), which is regarded as the baseline in our study. Choice Frozen Evolutionary Search (CF-ES) represents the method that only freezes shallow blocks and does not monitor the deep blocks for further freezing. Thanks to the reduced computations for the frozen shallow blocks, CF-ES method achieves averagely 19% time saving and 18% energy saving while maintaining Top-1 and Top-5 accuracy compared to the baseline. Progressively Choice Freezing Evolutionary Search (PCF-ES) is our proposed method that freezes both shallow and deep blocks during the searching, which achieves much more latency reduction with little accuracy loss. Specifically, PCF-ES reduces 38% \u223c 55% search latency and 37% \u223c 51% GPU energy consumption in three benchmarks. To understand where the speedup exactly comes from, we further breakdown the execution time at each generation when using different searching methods. As shown in Fig.7, the searching time (latency) of each generation is close to each other in the original evolutionary search, while CF-ES takes less time since the 6th generation because shallow blocks are frozen. The latency of PCF-ES begins to decrease from the 6th generation and keeps dropping progressively until it becomes 0 in the 13th generation, during which all blocks are frozen. In other words, it takes only 13th searching epochs to find the optimal architecture when using PCF-ES method. Fig.8 shows the Top-1 searching accuracy during the searching stage for different approaches and benchmarks. It can be observed that, for all benchmarks, all three methods have the same accuracy at the early generations when no choices are frozen. Compare to other two methods, PCF-ES shows very little accuracy loss in SPOS and FairNAS, and can always terminates earlier. To determine appropriate hyper-parameters, i.e., choice percentage threshold, block ratio threshold, and monitor size, used in our algorithm, we conduct sensitivity analysis for all three types of thresholds. Note that the proper thresholds we obtained using one dataset are effective for different searching scenarios. We first study the effects of selecting the choice percentage threshold for dif- ferent benchmarks are shown in Fig.9. The monitor size and block ratio are empirically initialized to be 3 and 0.5. As can be seen, for SPOS, when choosing the smallest threshold (T = 0.6), the searching terminates at the earliest (at the 10th generation) with relatively low accuracy. This is because some shallow blocks are mistakenly frozen. Similar behavior can be observed when setting threshold as 0.3 for FairNAS and GreedyNAS. To avoid the blocks being frozen too aggressively, a proper threshold should be picked. For example, when setting threshold as 0.7 for SPOS and 0.4 for FairNAS and GreedyNAS, the searching process maintains relatively high accuracy while terminating early as well. These thresholds are the optimal thresholds that ensure both high accuracy and low latency at the same time. The effects of selecting different block ratios are not shown here since we observed similar results to the effects of the choice percentage threshold. We empirically set block ratio as 0.5 in all three benchmarks. Fig.10 shows the searching accuracy and saved computations with different monitor sizes. For SPOS and GreedyNAS, the accuracy fluctuate with the increase of the monitor size. However, for FairNAS, the accuracy drops significantly when the monitor size is greater than 2. This is because too many deep blocks are frozen in this case. The saved computation increases steadily in all benchmarks as the number of monitored blocks increases. Fig.11 shows the validation accuracy of the optimal architecture searched by PCF-ES with different importance sampling rate (the ratio of sampled data to total data). We observe that when the sample rate is higher than 40%, the accuracy impact is negligible. However, the accuracy drops rapidly if the sample rate further decreases. 5 CONCLUSION: In this work, we profiled the evolutionary searching process of one-shot NAS and observed the key observations regarding the choice of blocks. Motivated by the majority choices that appears at the shallow blocks at the early searching stage, we proposed a progressively choice freezing evolutionary search to narrow the search space and reduce the searching time. As our evaluation results show, our method reduces averagely 46% of searching latency and 43% of energy consumption for all benchmarks while incurs only 0.1% accuracy loss. A APPENDIX: A.1 OBSERVATION As introduced in Section 3.1, the majority choices of shallow blocks can be determined in the early search generations while deep blocks cannot. Here we present the choice evolution of more blocks in two other one-shot NAS benchmarks: FairNAS and GreedyNAS. As shown in Fig. 12. It can be found that the majority choices of shallow blocks appears in the early search stage and their percentage increases steadily. On the contrary, the majority choice in deep blocks can not be decided during the search process. A.2 PROGRESSIVELY CHOICE FREEZING In section 3.4, we show that in SPOS, the choice percentage of the subsequent blocks increases rapidly after the previous blocks are frozen. We further conduct the same experiments for other two benchmarks. As shown in Fig. 13, for all six searching processes in (a) and (b), the choice percentage of subsequent blocks increases dramatically after freezing the previous blocks. A.3 THE EFFECTS OF QUANTIZATION 50% 73.4 73.4 73.1 73.0:  30% 73.1 73.1 72.8 72.8:  10% 73.1 73.1 73.0 72.8: In an attempt to achieve more memory savings, we tested another compression technique, i.e., quantization, on our sampled dataset. Quantization is a commonly used compression method that converts a floating-point value to a fixed-point value with fewer bits. Here the quantization is applied on the intermediate data before storing them into the memory. The validation accuracy with different importance sampling rate and quantization bit for SPOS benchmark is shown in Table 2. It can be seen that the search accuracy maintains when the intermediate data are quantized to 16 bit, but drops considerably when further quantized to lower bit. For the configuration of PCF-ES method, we use 40% sample rate and 16-bit quantization for the best searching performance."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper and provide a final decision.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerAlpha', 'base_model': 'gpt-4', 'description': 'Experienced ICLR reviewer with a focus on efficiency and NAS.', 'system_prompt': 'You are an experienced ICLR reviewer. You have a strong background in Neural Architecture Search (NAS) and efficient deep learning. You are detail-oriented and provide constructive feedback. Focus on the novelty, clarity, and impact of the work. Provide a detailed review including strengths and weaknesses, and a final recommendation (Accept/Reject).'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent ReviewerAlpha already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"ReviewerAlpha\",\n  \"base_model\": \"gpt-4\",\n  \"description\": \"Experienced ICLR reviewer with a focus on efficiency and NAS.\",\n  \"system_prompt\": \"You are an experienced ICLR reviewer. You have a strong background in Neural Architecture Search (NAS) and efficient deep learning. You are detail-oriented and provide constructive feedback. Focus on the novelty, clarity, and impact of the work. Provide a detailed review including strengths and weaknesses, and a final recommendation (Accept/Reject).\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent ReviewerAlpha already exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems I made a mistake and the agent already exists. I will try creating the other two agents and then proceed with the reviews.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerBeta', 'description': 'ICLR reviewer specializing in evolutionary algorithms and hardware constraints.', 'system_prompt': 'You are an ICLR reviewer specializing in evolutionary algorithms and their application to neural architecture search. You are also knowledgeable about hardware constraints and energy efficiency. Provide a detailed review including strengths and weaknesses, and a final recommendation (Accept/Reject).', 'base_model': 'gpt-4'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent ReviewerBeta already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"ReviewerBeta\",\n  \"description\": \"ICLR reviewer specializing in evolutionary algorithms and hardware constraints.\",\n  \"system_prompt\": \"You are an ICLR reviewer specializing in evolutionary algorithms and their application to neural architecture search. You are also knowledgeable about hardware constraints and energy efficiency. Provide a detailed review including strengths and weaknesses, and a final recommendation (Accept/Reject).\",\n  \"base_model\": \"gpt-4\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent ReviewerBeta already exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'ICLR reviewer focusing on practical applications and memory efficiency.', 'agent_name': 'ReviewerGamma', 'system_prompt': 'You are an ICLR reviewer focusing on the practical applications of neural architecture search, especially in resource-constrained environments. You are an expert in memory efficiency and importance sampling techniques. Provide a detailed review including strengths and weaknesses, and a final recommendation (Accept/Reject).', 'base_model': 'gpt-4'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent ReviewerGamma already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"ICLR reviewer focusing on practical applications and memory efficiency.\",\n  \"agent_name\": \"ReviewerGamma\",\n  \"system_prompt\": \"You are an ICLR reviewer focusing on the practical applications of neural architecture search, especially in resource-constrained environments. You are an expert in memory efficiency and importance sampling techniques. Provide a detailed review including strengths and weaknesses, and a final recommendation (Accept/Reject).\",\n  \"base_model\": \"gpt-4\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent ReviewerGamma already exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems all three agents already exist. I will now proceed to ask each agent to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerAlpha', 'prompt': 'Review the following ICLR 2023 paper. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search\\n\\n1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks (Elsken et al., 2019; Wistuba et al., 2019; Ren et al., 2020). NAS approaches automate the architecture design process and can achieve higher accuracy compared to human-designed architectures (Liu et al., 2019; Xie et al., 2019; Cai et al., 2019). However, the early NAS methods, such as reinforcement NAS (Zoph & Le, 2016), came with the price of expensive computation costs since every searched architecture needs to be trained from scratch, which makes the total search time unacceptable. To reduce the search cost of earlier NAS methods, the weight sharing technique has been proposed (Yu et al., 2020; Chen et al., 2020), among which the one-shot NAS method has attracted a lot of attention recently (Bender et al., 2018; Li et al., 2020). The one-shot NAS method is known as cost-efficient as it requires training a supernet only once. A supernet is a stack of basic blocks, each of which contains multiple choices. A candidate network architecture (defined as subnet) can be formed by selecting one choice for each block in the supernet, and its corresponding weights can be inherited from the supernet. During the architecture searching stage, candidate architectures are evaluated on the validation dataset and the best architecture, i.e., the architecture with the highest validation accuracy, is updated in every searching epoch of Evolutionary Algorithm (EA) (Real et al., 2019). Surprisingly, although training is commonly deemed as a lengthy and energy-consuming task, the architecture searching stage in one-shot NAS is much more costly (Cai et al., 2020) than training a supernet. The reason is that a new searching stage should be performed whenever a different searching scenario is given, e.g., different hardware constraints, learning tasks, and workloads, while the trained supernet can be reused. Hence, the numerous inferences on the subnets can take a much longer time than training a supernet only once. According to (You et al., 2020), searching can be 10 GPU days longer than supernet training when 10 different constraints/platforms are required. To tackle this problem, our work first makes a key observation that, for the first few continuous blocks of the candidate architecture (defined as continuous shallow blocks), their optimal choices can be determined at an early search epoch. Based on this observation, we propose to freeze the choices of continuous shallow blocks at the early search epoch, which means these choices will not be changed during the remaining search epochs. This strategy elaborately \u201ccreates\u201d redundant computations in the continuous shallow blocks since all candidates will share exactly the same architecture, inherited weights, and input validation data for the shallow blocks during later search epochs. Then we leverage such redundancy and propose a simple yet effective data reuse scheme to save large amounts of computations, thus further reducing time and energy cost. Specifically, we propose to reuse the last output of the continuous shallow blocks instead of re-computing it repeatedly throughout the remaining searching stage. Interestingly, we further discover that the freezing strategy may in turn help to determine the optimal choices of the subsequent blocks earlier. Such phenomenon enables us to keep freezing the choices of blocks progressively after the initial freezing, which will create more redundant computations of the blocks that possess the same architecture (choice), thus more computations can be saved. With the proposed freezing technique, the intermediate data (the last output of the continuous shallow blocks) of a certain subnet can be stored and reused during the evaluation of other subnets. However, as the searching stage requires to evaluate a subnet with a large batch (e.g., batch size = 5000) of input samples, storing the intermediate data of only one subnet may cause serious memory issues (Sec.3.5). Inspired by the Importance Sampling technique employed in many training methods (Zeng et al., 2021), we propose to sample the \u201cimportant\u201d input data that contribute more to distinguish the evaluation accuracy of the candidate subnets. More importantly, we empirically demonstrate that the important samples are shared across different subnets. Therefore, it only requires to store the intermediate data of important samples for one certain subnet, and then reuse it for all other subnets. We evaluate the proposed method on multiple benchmarks trained with the state-of-the-art approaches on the ImageNet dataset (Krizhevsky et9al., 2012). The experimental results indicate superb performance in improving the search efficiency while maintaining the search performance with only 0.1% searching accuracy loss. Our contributions can be summarized as follows: \u00b7 We observe that, in the one-shot NAS evolutionary searching stage, the optimal architecture of shallow blocks is determined at the early searching stage. \u00b7 We propose to freeze the choices of continuous shallow blocks for all candidates at the early stage, and progressively freeze the choices of the subsequent blocks in the later stage. This approach creates a great amount of redundant computations, which provide us a good opportunity to reuse the intermediate data and reduce the searching time. \u00b7 To alleviate memory capacity issue for storing intermediate data, we leverage the concept of importance sampling and propose a distinguish-based sampling method to reduce the size of the intermediate data. \u00b7 We conduct extensive experiments on different benchmarks with our proposed methods. The evaluation results show that our method can achieve up to 55% time saving and 51% energy saving with 0.1% accuracy loss. 2 REVIEW OF ONE-SHOT NAS: Different from the traditional neural network training that aims to optimize weights given a network architecture, NAS seeks to optimize both weight and architecture at the same time. Conventional NAS methods (Zoph & Le, 2016; Baker et al., 2016; Zhong et al., 2018; Zela et al., 2018) have tried to solve these two optimization problems at a nested approach. However, these methods are usually prohibitively expensive because each architecture sampled from the search space has to be trained from the scratch and evaluated separately. Recent works (Bender et al., 2018; Pham et al., 2018; Cai et al., 2019; 2018) have proposed a weight sharing strategy to reduce high costs of architecture and weight searching procedure in conventional NAS. As one of the most popular weight sharing techniques, one-shot NAS achieves unprecedented search efficiency by decoupling the whole searching process into two stages: supernet training (Fig.1 (a))and subnet searching (Fig.1 (b)). One-shot NAS encodes the search space into a supernet and trains it only once. Then it allows sampled architectures directly inherit the weights from the supernet. As shown in Fig.1 (a), the commonly used chain-structured supernet (Suganuma et al., 2017) is a stack of blocks, and each block consists of multiple choices. Each choice can be different operations, e.g., a 3\u00d73 / 5\u00d75 convolutional layer, a max-pooling layer or an identity layer. We further define the first few blocks in the supernet as shallow blocks, while the subsequent blocks are defined as deep blocks. The second step of one-shot NAS is the searching stage which employs EA to find the subnet architecture with the highest validation accuracy. Each subnet can be obtained by selecting one choice for each block in the supernet, and its weights are inherited from the supernet. The overall workflow is shown in Fig.1 (b). At each search epoch, also known as generation, there are N candidate subnets in the candidate pool sampled from the supernet and evaluated using validation dataset. Then the candidate subnets are ranked together with the previous top candidates according to the validation accuracy. Candidates with Top-K validation accuracy are selected and then evolved into N new subnets by performing mutation and crossover operations (Real et al., 2019). The above process is repeated until reaching the maximum number of search epochs E, and the subnet with the highest accuracy can be obtained. The architecture of this subnet will be regarded as the optimal architecture since the validation accuracy obtained by using inherited weights is highly predictive on the accuracy obtained by training from scratch (Bender et al., 2018). Recently, many studies focused on improving the efficiency and accuracy of the supernet training stage by introducing various subnet sampling methods (Guo et al., 2020; Chu et al., 2021; You et al., 2020). Nevertheless, none of them considered the efficiency of the searching stage. The searching stage can be more time-consuming compared with supernet training (Cai et al., 2020). This is because the supernet only need to be trained once while numerous searching processes are required to search the optimal architecture for various deployment scenarios, e.g., different hardware platforms, workloads. Therefore, it is essential to improve the search efficiency. NASA (Ma et al., 2021) is the first work that focuses on accelerating the search process, and a NAS accelerator was proposed that utilizes network fusion based on the computation sharing and data reuse within a search generation. Different from their approach, we propose an algorithm-level optimization for one-shot NAS evolutionary search and exploit data sharing from both within generation and across generation levels, which achieves significant improvement on the searching efficiency. 3 METHODOLOGY: This section first introduces several key observations during the evolutionary searching phase, based on which the progressive choice freezing evolutionary search (PCF-ES) is then proposed, along with how it saves the computations during the searching stage. Finally, to mitigate the memory issue of our method, the distinguish-based importance sampling method is proposed to reduce the size of the intermediate data. 3.1 OBSERVATION: MAJORITY CHOICE IN CONTINUOUS SHALLOW BLOCKS: Figure 2 illustrates how the choices of blocks evolve across generations during the evolutionary search in SPOS (Guo et al., 2020). The right-up subgraphs record the choice evolution in shallow blocks. Especially, we take the 1st, the 2nd, and the 3rd blocks as the examples to show their choice evolution across the search generations, respectively. The right-bottom subgraphs are the choice evolution in deep blocks, and the 15th, the 16th, and the 17th blocks are used as the examples. Each curve in the subgraph indicates the percentage of subnets among all candidate subnets in a certain generation that select a certain choice for the block. As an example shown in the left part of the figure, at the i-th generation, the percentage of the blue choice in the 1st block equals to 80% if 40 out of 50 subnets select the blue choice. As can be seen in the right-up subgraphs, there is one choice whose percentage increases dramatically and surpasses all the other choices in the early generations (i \u2264 10, highlighted by the left of the blue dotted line). We define such choice as the majority choice. We further observed that the majority choice will be selected by more subnets and become even more dominating for the shallow blocks in the following generations. However, this is not the case for the deep blocks. In the right-bottom subgraphs, we observed that either the majority choice emerges at very later generations, or multiple curves intertwine with each other, thus there is no obvious majority choice for deep blocks. The above observations indicate that candidate subnets tend to vote majority choice in the shallow blocks at the early searching stage, while for deeper blocks the majority choice is unclear. 3.2 RELATION BETWEEN THE MAJORITY CHOICE AND THE OPTIMAL CHOICE: Majority choice is the choice that most of the candidate subnets \u201dthink\u201d it can lead to better accuracy during evolution. Intuitively, majority choice should be representative of optimal choice. To quantitatively analyze the relationship between the majority choice and the optimal choice, we first define a binary categorical choice distance D, where D equals to 0 if two choices are the same otherwise D equals to 1. Then we calculate four types of D for the choices in the continuous shallow blocks as follows: the average distance D between the non-majority choices and the block choices in the first generation (i.e., initial block choices), the average distance D between the non-majority choices and the block choices in the last generation (i.e., optimal block choices), the average distance D between the majority choices and the block choices in the first generation, and the average distance D between the majority choices and the block choices in the last generation. Here, all D denotes the averaged distance across all shallow blocks. Note that the non-majority choices are all the choices of shallow blocks when their average percentage is below an empirically predetermined threshold (0.7) at the early searching stage. The majority choices are obtained once we observe their average percentage rising above 0.7. The results of non-majority choices and majority choices are shown in Figure 3 (a) and (b), respectively. The x-axis (y-axis) represents the distance between the nonmajority/majority choices and the block choices of the first (last) generation. Each point represents an independent searching process with random seeds. As can be seen, for non-majority choices (Figure 3 (a)), the points are scattered in a symmetrical way, which means a certain non-majority choice can be either close or far away from the optimal choice. In contrast, for majority choices (Figure 3 (b)), most points are located at the right-bottom of the plane, which indicates that the majority choices are very close to the final optimal choices. 3.3 KEY INSIGHTS: CHOICE FREEZING: The above comparative experiment shows that majority choices are very close to optimal choices and can be safely used to approximate optimal choices. This provide a solid foundation for us to freeze the majority choices in the shallow blocks when they appear in the early searching stage. The motivation of freezing the block choices is illustrated in Fig.4. As shown in Fig 4 (a), assuming there are two candidate subnets to be evaluated sequentially at the current searching epoch, the choices of the first two blocks CB1,CB2 of Net1 and Net2 are {blue, red} and {yellow, red} respectively. Since the choices of these two shallow blocks are not totally the same (even though the choices of CB2 are the same), the intermediate feature maps (FM1, FM2) in Net1 and Net2 will be different. Hence, to obtain the evaluation results for both subnets, the computations of all the blocks are required. However, as shown in Figure 4 (b), if the choices in CB1 and CB2 are already majority choices in the previous searching epoch and we managed to freeze them at the i-th search generation, Net1 will have the same choices for CB1 and CB2 as Net2, so does Net3 to Net50 in the current generation and all other subnets in the following generations. Since one-shot NAS uses the same validation dataset as input, the FM1 and FM2 of these subnets will be exactly the same and the computations for CB1 and CB2 will become redundant. Therefore, one can simply store the FM2 and reuse it for all the subsequent evaluation of candidate subnets, with a significant computation savings of all frozen blocks. Note that the stored FMs can be reused both within and across generations by every subnet evaluation process, which brings more computation savings than NASA (Ma et al., 2021). 3.4 PROGRESSIVE CHOICE FREEZING EVOLUTIONARY SEARCH: To further explore the behaviour of the choices of deeper blocks, we keep monitoring the choice percentage of every subsequent block once the previous blocks are frozen in the candidate subnet. As shown in Figure 5, we conduct three independent search processes where the first 3, 6, and 8 continuous block choices are frozen according to the majority choice percentage threshold. For example, the orange curve represents the choice percentage of the 7th block B7 if we freeze the first 6 continuous blocks at the 5th generation (marked by the dot line). It can be seen that, for all three blocks in the figure, the choice percentage increases drastically after previous blocks are fixed. Since higher choice percentage means higher possibility that a majority choice is observed, freezing the previous block choices will help freeze the latter block choices. Based on this observation, we propose to freeze more block choices progressively when the previous ones are already frozen. The overall idea is implemented in our proposed Progressive Choice Freezing Evolutionary Search (PCF-ES) algorithm in Alg. 1. The population of the first generation is randomly generated (Line Algorithm 1 Progressive Choice Freezing Evolutionary Search Input: supernet weight WA, population size P , architecture constraints C, max generation T , validation dataset Dv , choice percentage threshold H , mutation rate r, number of blocks N , monitor size f Output: architecture with best validation performance 1: Random initialize P0 2: c = 0 3: for i = 1 : T do 4: if c = 0 then 5: for j = N : 1 do 6: average choice percentage Mj from Bj to B1 7: block ratio Brj from Bj to B1 8: if M > H and Brj > 0.5 then 9: a1\u223cjTop\u2212K = a 1\u223cj maj 10: c = 1 11: break 12: end if 13: end for 14: else 15: for x = f : 1 do 16: average choice percentage Mf from Bj+1 to Bj+f 17: if Mf > H and Brf > 0.5 then 18: a1\u223c(j+x)Top\u2212K = a 1\u223c(j+x) maj 19: end if 20: end for 21: end if 22: Pmutation = Mutation(Topk, r, C) 23: Pcrossover = Crossover(Topk,C) 24: P = Pmutation \u222a Pcrossover 25: Acc = Inference(WA, Dv, P ) 26: end for 27: Return architecture with highest accuracy 1) and c is a flag to check if there are block choices that have been frozen (Line 4). When c is 0 which implies no block choice has been frozen, we calculate and examine the average choice percentage of the first j blocks at each search generation, and j starts from N (the number of blocks in a subnet) to 1 because we aim to freeze as many block choices as possible at the beginning (Lines 5-6). Note that the average choice percentage Mj is the mean of choice percentage from block B1 to Bj . One obvious downside of the average choice percentage is that it cannot indicate the choice percentage of each block, thus the choices of some blocks with relatively low choice percentage may be mistakenly frozen when the averaged choice percentage across the investigated shallow blocks exceeds the threshold. We employ another metric, i.e., block ratio, to avoid the aggressive choice frozen. block ratio measures the ratio of blocks whose choice percentage exceeds the threshold. For example, if the average choice percentage of the first 10 blocks exceeds the threshold, among which only 7 of these blocks exceeds the threshold, then the block ratio is 70%. Only when the block ratio is above the block ratio threshold, we allow the block choices to be frozen, which ensures that most of the frozen block choices reach the required choice percentage. If both metrics, i.e., average choice percentage and block ratio, are above the thresholds (Line 8), choices of the first j blocks in all Top-K subnets are changed to the current majority choices (Line 9) and become frozen. Accordingly, we set flag c = 1 when the first j blocks are frozen. For the rest of the searching process, these block choices will not be affected by the mutation and crossover operations. On the other hand, if any of the two metrics is below the threshold and the flag c = 0, the algorithm jumps to population generation and generates new candidates normally (Lines 22-24). If the first j blocks can be frozen, we keep exploring chances to freeze more block choices during the later searching stage. The monitor size f is the maximum number of blocks we should monitor at each generation in the latter searching stage (Line 15). The purpose of using monitor size here is to prevent aggressive block freezing. Specifically, since the search space is already remarkably narrowed down in the initial freezing step (up to 8 continuous shallow blocks can be frozen at a time, while a supernet usually has a total of 20 blocks), keep freezing too many subsequent blocks (even if their average choice percentage and block ratio are above the threshold) can make the search space so small that finally leads to inaccurate search results. In the progressively freezing phase, the average choice percentage Mf is the mean of choice percentage from block Bj+1 to Bj+f (Line 16). If the Mf is above the threshold, blocks Bj+1 to Bj+f in Top-K subnets will be frozen based on the current majority choice. After all candidate subnets are generated, they are evaluated on the validation dataset and ranked based on the validation accuracy (Line 25). We keep track of the Top-K subnets and return the Top-1 subnet architecture when maximum generation is reached or all block choices are frozen. 3.5 DISTINGUISH-BASED IMPORTANCE SAMPLING: To achieve the potential computation savings, it\u2019s necessary to store the intermediate feature maps, e.g. FM2 in Fig.4, of all the input samples in the validation dataset, the size of which may exceed the memory capacity of the GPUs in some circumstances. For instance, for each input sample, the intermediate data size of the second block of subnets in SPOS on ImageNet is approximately 0.8 MB. If the validation dataset contains more than 5000 input images, the memory storage of all the intermediate feature maps could exceed a GPU\u2019s memory capacity with 4GB main memory. To tackle this problem, we first leverage Importance Sampling (Alain et al., 2015) to compress the intermediate data. The idea of importance sampling comes from the fact that not all the input samples contribute to distinguishing the evaluation accuracy of candidate subnets. For example, for images which are easy to be classified by most subnets, they barely contribute to distinguish the performance of the searched subnets. Inspired by the importance sampling during training (Zeng et al., 2021), we follow the similar principle to sample the important data for the subnets evaluation. Let x denotes the input data, p denotes the uniform distribution referred from random sample, q denotes the distribution adopted by importance sampling and f(x) denotes the evaluation accuracy of input data x. The unbiased estimation of evaluation accuracy can be obtained by Eq.(1). Ep[f(x)] = \u222b p(x)f(x)dx = Eq[ p(x) q(x) f(x)] (1) if q(x) > 0 whenever p(x) > 0. Moreover, the estimation variance of the distribution q is minimized when q(x)\u2217 = 1 Z p(x)||f(x)||2, and Z = \u222b p(x)||f(x)||2dx (2) In our design, we use the cross-entropy loss to approximate the evaluation accuracy and then calculate the important sampling distribution q(x)\u2217 based on Eq.(2). Although we can obtain the important samples for a certain subnet using the above method, we cannot reuse the intermediate data of these important samples for other subnet\u2019s evaluation unless the important samples are shared across different subnets. Therefore, we quantitatively analyze the importance of samples for different subnets after the shallow blocks are frozen. As shown in Fig.6, we present the importance distribution of 500 important samples (x-axis) for 50 different subnets (y-axis). The color represents the importance of a sample. It can be found that the sample with high importance (deep blue) for one subnet is also important for other subnets. Overall, the importance distribution of a certain subnet (a certain row) is similar to the importance distribution of other subnets (other rows). As a result, it is only necessary to store the intermediate data of important samples for one subnet and then reuse it for other subnets. 4 EXPERIMENT:  4.1 EXPERIMENT SETUP: Our experiments are conducted on widely used ImageNet(Krizhevsky et al., 2012), of which the validation dataset contains 50000 images at 1000 categories. Both supernet training and subnet searching processes are performed on Tesla P100 GPUs with 16 GB memory. We employ the chain structure supernet adopted with three benchmarks: SPOS (Guo et al., 2020), FairNAS (Chu et al., 2021) and GreedyNAS (You et al., 2020), whose supernets contain 20, 19 and 21 blocks, respectively. The supernets are trained with the parameter settings in the original papers. The maximum search generation is 20 and the population size is 50. At each generation, subnets with top-10 validation accuracy are kept as parent networks. 4.2 EXPERIMENT RESULTS: The search results on different benchmarks and search methods are shown in Table 1. Evolutionary represents the original evolutionary algorithm used in (Guo et al., 2020), which is regarded as the baseline in our study. Choice Frozen Evolutionary Search (CF-ES) represents the method that only freezes shallow blocks and does not monitor the deep blocks for further freezing. Thanks to the reduced computations for the frozen shallow blocks, CF-ES method achieves averagely 19% time saving and 18% energy saving while maintaining Top-1 and Top-5 accuracy compared to the baseline. Progressively Choice Freezing Evolutionary Search (PCF-ES) is our proposed method that freezes both shallow and deep blocks during the searching, which achieves much more latency reduction with little accuracy loss. Specifically, PCF-ES reduces 38% \u223c 55% search latency and 37% \u223c 51% GPU energy consumption in three benchmarks. To understand where the speedup exactly comes from, we further breakdown the execution time at each generation when using different searching methods. As shown in Fig.7, the searching time (latency) of each generation is close to each other in the original evolutionary search, while CF-ES takes less time since the 6th generation because shallow blocks are frozen. The latency of PCF-ES begins to decrease from the 6th generation and keeps dropping progressively until it becomes 0 in the 13th generation, during which all blocks are frozen. In other words, it takes only 13th searching epochs to find the optimal architecture when using PCF-ES method. Fig.8 shows the Top-1 searching accuracy during the searching stage for different approaches and benchmarks. It can be observed that, for all benchmarks, all three methods have the same accuracy at the early generations when no choices are frozen. Compare to other two methods, PCF-ES shows very little accuracy loss in SPOS and FairNAS, and can always terminates earlier. To determine appropriate hyper-parameters, i.e., choice percentage threshold, block ratio threshold, and monitor size, used in our algorithm, we conduct sensitivity analysis for all three types of thresholds. Note that the proper thresholds we obtained using one dataset are effective for different searching scenarios. We first study the effects of selecting the choice percentage threshold for dif- ferent benchmarks are shown in Fig.9. The monitor size and block ratio are empirically initialized to be 3 and 0.5. As can be seen, for SPOS, when choosing the smallest threshold (T = 0.6), the searching terminates at the earliest (at the 10th generation) with relatively low accuracy. This is because some shallow blocks are mistakenly frozen. Similar behavior can be observed when setting threshold as 0.3 for FairNAS and GreedyNAS. To avoid the blocks being frozen too aggressively, a proper threshold should be picked. For example, when setting threshold as 0.7 for SPOS and 0.4 for FairNAS and GreedyNAS, the searching process maintains relatively high accuracy while terminating early as well. These thresholds are the optimal thresholds that ensure both high accuracy and low latency at the same time. The effects of selecting different block ratios are not shown here since we observed similar results to the effects of the choice percentage threshold. We empirically set block ratio as 0.5 in all three benchmarks. Fig.10 shows the searching accuracy and saved computations with different monitor sizes. For SPOS and GreedyNAS, the accuracy fluctuate with the increase of the monitor size. However, for FairNAS, the accuracy drops significantly when the monitor size is greater than 2. This is because too many deep blocks are frozen in this case. The saved computation increases steadily in all benchmarks as the number of monitored blocks increases. Fig.11 shows the validation accuracy of the optimal architecture searched by PCF-ES with different importance sampling rate (the ratio of sampled data to total data). We observe that when the sample rate is higher than 40%, the accuracy impact is negligible. However, the accuracy drops rapidly if the sample rate further decreases. 5 CONCLUSION: In this work, we profiled the evolutionary searching process of one-shot NAS and observed the key observations regarding the choice of blocks. Motivated by the majority choices that appears at the shallow blocks at the early searching stage, we proposed a progressively choice freezing evolutionary search to narrow the search space and reduce the searching time. As our evaluation results show, our method reduces averagely 46% of searching latency and 43% of energy consumption for all benchmarks while incurs only 0.1% accuracy loss. A APPENDIX: A.1 OBSERVATION As introduced in Section 3.1, the majority choices of shallow blocks can be determined in the early search generations while deep blocks cannot. Here we present the choice evolution of more blocks in two other one-shot NAS benchmarks: FairNAS and GreedyNAS. As shown in Fig. 12. It can be found that the majority choices of shallow blocks appears in the early search stage and their percentage increases steadily. On the contrary, the majority choice in deep blocks can not be decided during the search process. A.2 PROGRESSIVELY CHOICE FREEZING In section 3.4, we show that in SPOS, the choice percentage of the subsequent blocks increases rapidly after the previous blocks are frozen. We further conduct the same experiments for other two benchmarks. As shown in Fig. 13, for all six searching processes in (a) and (b), the choice percentage of subsequent blocks increases dramatically after freezing the previous blocks. A.3 THE EFFECTS OF QUANTIZATION 50% 73.4 73.4 73.1 73.0:  30% 73.1 73.1 72.8 72.8:  10% 73.1 73.1 73.0 72.8: In an attempt to achieve more memory savings, we tested another compression technique, i.e., quantization, on our sampled dataset. Quantization is a commonly used compression method that converts a floating-point value to a fixed-point value with fewer bits. Here the quantization is applied on the intermediate data before storing them into the memory. The validation accuracy with different importance sampling rate and quantization bit for SPOS benchmark is shown in Table 2. It can be seen that the search accuracy maintains when the intermediate data are quantized to 16 bit, but drops considerably when further quantized to lower bit. For the configuration of PCF-ES method, we use 40% sample rate and 16-bit quantization for the best searching performance.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ReviewerAlpha\",\n  \"prompt\": \"Review the following ICLR 2023 paper. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search\n\n1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks (Elsken et al., 2019; Wistuba et al., 2019; Ren et al., 2020). NAS approaches automate the architecture design process and can achieve higher accuracy compared to human-designed architectures (Liu et al., 2019; Xie et al., 2019; Cai et al., 2019). However, the early NAS methods, such as reinforcement NAS (Zoph & Le, 2016), came with the price of expensive computation costs since every searched architecture needs to be trained from scratch, which makes the total search time unacceptable. To reduce the search cost of earlier NAS methods, the weight sharing technique has been proposed (Yu et al., 2020; Chen et al., 2020), among which the one-shot NAS method has attracted a lot of attention recently (Bender et al., 2018; Li et al., 2020). The one-shot NAS method is known as cost-efficient as it requires training a supernet only once. A supernet is a stack of basic blocks, each of which contains multiple choices. A candidate network architecture (defined as subnet) can be formed by selecting one choice for each block in the supernet, and its corresponding weights can be inherited from the supernet. During the architecture searching stage, candidate architectures are evaluated on the validation dataset and the best architecture, i.e., the architecture with the highest validation accuracy, is updated in every searching epoch of Evolutionary Algorithm (EA) (Real et al., 2019). Surprisingly, although training is commonly deemed as a lengthy and energy-consuming task, the architecture searching stage in one-shot NAS is much more costly (Cai et al., 2020) than training a supernet. The reason is that a new searching stage should be performed whenever a different searching scenario is given, e.g., different hardware constraints, learning tasks, and workloads, while the trained supernet can be reused. Hence, the numerous inferences on the subnets can take a much longer time than training a supernet only once. According to (You et al., 2020), searching can be 10 GPU days longer than supernet training when 10 different constraints/platforms are required. To tackle this problem, our work first makes a key observation that, for the first few continuous blocks of the candidate architecture (defined as continuous shallow blocks), their optimal choices can be determined at an early search epoch. Based on this observation, we propose to freeze the choices of continuous shallow blocks at the early search epoch, which means these choices will not be changed during the remaining search epochs. This strategy elaborately \u201ccreates\u201d redundant computations in the continuous shallow blocks since all candidates will share exactly the same architecture, inherited weights, and input validation data for the shallow blocks during later search epochs. Then we leverage such redundancy and propose a simple yet effective data reuse scheme to save large amounts of computations, thus further reducing time and energy cost. Specifically, we propose to reuse the last output of the continuous shallow blocks instead of re-computing it repeatedly throughout the remaining searching stage. Interestingly, we further discover that the freezing strategy may in turn help to determine the optimal choices of the subsequent blocks earlier. Such phenomenon enables us to keep freezing the choices of blocks progressively after the initial freezing, which will create more redundant computations of the blocks that possess the same architecture (choice), thus more computations can be saved. With the proposed freezing technique, the intermediate data (the last output of the continuous shallow blocks) of a certain subnet can be stored and reused during the evaluation of other subnets. However, as the searching stage requires to evaluate a subnet with a large batch (e.g., batch size = 5000) of input samples, storing the intermediate data of only one subnet may cause serious memory issues (Sec.3.5). Inspired by the Importance Sampling technique employed in many training methods (Zeng et al., 2021), we propose to sample the \u201cimportant\u201d input data that contribute more to distinguish the evaluation accuracy of the candidate subnets. More importantly, we empirically demonstrate that the important samples are shared across different subnets. Therefore, it only requires to store the intermediate data of important samples for one certain subnet, and then reuse it for all other subnets. We evaluate the proposed method on multiple benchmarks trained with the state-of-the-art approaches on the ImageNet dataset (Krizhevsky et9al., 2012). The experimental results indicate superb performance in improving the search efficiency while maintaining the search performance with only 0.1% searching accuracy loss. Our contributions can be summarized as follows: \u00b7 We observe that, in the one-shot NAS evolutionary searching stage, the optimal architecture of shallow blocks is determined at the early searching stage. \u00b7 We propose to freeze the choices of continuous shallow blocks for all candidates at the early stage, and progressively freeze the choices of the subsequent blocks in the later stage. This approach creates a great amount of redundant computations, which provide us a good opportunity to reuse the intermediate data and reduce the searching time. \u00b7 To alleviate memory capacity issue for storing intermediate data, we leverage the concept of importance sampling and propose a distinguish-based sampling method to reduce the size of the intermediate data. \u00b7 We conduct extensive experiments on different benchmarks with our proposed methods. The evaluation results show that our method can achieve up to 55% time saving and 51% energy saving with 0.1% accuracy loss. 2 REVIEW OF ONE-SHOT NAS: Different from the traditional neural network training that aims to optimize weights given a network architecture, NAS seeks to optimize both weight and architecture at the same time. Conventional NAS methods (Zoph & Le, 2016; Baker et al., 2016; Zhong et al., 2018; Zela et al., 2018) have tried to solve these two optimization problems at a nested approach. However, these methods are usually prohibitively expensive because each architecture sampled from the search space has to be trained from the scratch and evaluated separately. Recent works (Bender et al., 2018; Pham et al., 2018; Cai et al., 2019; 2018) have proposed a weight sharing strategy to reduce high costs of architecture and weight searching procedure in conventional NAS. As one of the most popular weight sharing techniques, one-shot NAS achieves unprecedented search efficiency by decoupling the whole searching process into two stages: supernet training (Fig.1 (a))and subnet searching (Fig.1 (b)). One-shot NAS encodes the search space into a supernet and trains it only once. Then it allows sampled architectures directly inherit the weights from the supernet. As shown in Fig.1 (a), the commonly used chain-structured supernet (Suganuma et al., 2017) is a stack of blocks, and each block consists of multiple choices. Each choice can be different operations, e.g., a 3\u00d73 / 5\u00d75 convolutional layer, a max-pooling layer or an identity layer. We further define the first few blocks in the supernet as shallow blocks, while the subsequent blocks are defined as deep blocks. The second step of one-shot NAS is the searching stage which employs EA to find the subnet architecture with the highest validation accuracy. Each subnet can be obtained by selecting one choice for each block in the supernet, and its weights are inherited from the supernet. The overall workflow is shown in Fig.1 (b). At each search epoch, also known as generation, there are N candidate subnets in the candidate pool sampled from the supernet and evaluated using validation dataset. Then the candidate subnets are ranked together with the previous top candidates according to the validation accuracy. Candidates with Top-K validation accuracy are selected and then evolved into N new subnets by performing mutation and crossover operations (Real et al., 2019). The above process is repeated until reaching the maximum number of search epochs E, and the subnet with the highest accuracy can be obtained. The architecture of this subnet will be regarded as the optimal architecture since the validation accuracy obtained by using inherited weights is highly predictive on the accuracy obtained by training from scratch (Bender et al., 2018). Recently, many studies focused on improving the efficiency and accuracy of the supernet training stage by introducing various subnet sampling methods (Guo et al., 2020; Chu et al., 2021; You et al., 2020). Nevertheless, none of them considered the efficiency of the searching stage. The searching stage can be more time-consuming compared with supernet training (Cai et al., 2020). This is because the supernet only need to be trained once while numerous searching processes are required to search the optimal architecture for various deployment scenarios, e.g., different hardware platforms, workloads. Therefore, it is essential to improve the search efficiency. NASA (Ma et al., 2021) is the first work that focuses on accelerating the search process, and a NAS accelerator was proposed that utilizes network fusion based on the computation sharing and data reuse within a search generation. Different from their approach, we propose an algorithm-level optimization for one-shot NAS evolutionary search and exploit data sharing from both within generation and across generation levels, which achieves significant improvement on the searching efficiency. 3 METHODOLOGY: This section first introduces several key observations during the evolutionary searching phase, based on which the progressive choice freezing evolutionary search (PCF-ES) is then proposed, along with how it saves the computations during the searching stage. Finally, to mitigate the memory issue of our method, the distinguish-based importance sampling method is proposed to reduce the size of the intermediate data. 3.1 OBSERVATION: MAJORITY CHOICE IN CONTINUOUS SHALLOW BLOCKS: Figure 2 illustrates how the choices of blocks evolve across generations during the evolutionary search in SPOS (Guo et al., 2020). The right-up subgraphs record the choice evolution in shallow blocks. Especially, we take the 1st, the 2nd, and the 3rd blocks as the examples to show their choice evolution across the search generations, respectively. The right-bottom subgraphs are the choice evolution in deep blocks, and the 15th, the 16th, and the 17th blocks are used as the examples. Each curve in the subgraph indicates the percentage of subnets among all candidate subnets in a certain generation that select a certain choice for the block. As an example shown in the left part of the figure, at the i-th generation, the percentage of the blue choice in the 1st block equals to 80% if 40 out of 50 subnets select the blue choice. As can be seen in the right-up subgraphs, there is one choice whose percentage increases dramatically and surpasses all the other choices in the early generations (i \u2264 10, highlighted by the left of the blue dotted line). We define such choice as the majority choice. We further observed that the majority choice will be selected by more subnets and become even more dominating for the shallow blocks in the following generations. However, this is not the case for the deep blocks. In the right-bottom subgraphs, we observed that either the majority choice emerges at very later generations, or multiple curves intertwine with each other, thus there is no obvious majority choice for deep blocks. The above observations indicate that candidate subnets tend to vote majority choice in the shallow blocks at the early searching stage, while for deeper blocks the majority choice is unclear. 3.2 RELATION BETWEEN THE MAJORITY CHOICE AND THE OPTIMAL CHOICE: Majority choice is the choice that most of the candidate subnets \u201dthink\u201d it can lead to better accuracy during evolution. Intuitively, majority choice should be representative of optimal choice. To quantitatively analyze the relationship between the majority choice and the optimal choice, we first define a binary categorical choice distance D, where D equals to 0 if two choices are the same otherwise D equals to 1. Then we calculate four types of D for the choices in the continuous shallow blocks as follows: the average distance D between the non-majority choices and the block choices in the first generation (i.e., initial block choices), the average distance D between the non-majority choices and the block choices in the last generation (i.e., optimal block choices), the average distance D between the majority choices and the block choices in the first generation, and the average distance D between the majority choices and the block choices in the last generation. Here, all D denotes the averaged distance across all shallow blocks. Note that the non-majority choices are all the choices of shallow blocks when their average percentage is below an empirically predetermined threshold (0.7) at the early searching stage. The majority choices are obtained once we observe their average percentage rising above 0.7. The results of non-majority choices and majority choices are shown in Figure 3 (a) and (b), respectively. The x-axis (y-axis) represents the distance between the nonmajority/majority choices and the block choices of the first (last) generation. Each point represents an independent searching process with random seeds. As can be seen, for non-majority choices (Figure 3 (a)), the points are scattered in a symmetrical way, which means a certain non-majority choice can be either close or far away from the optimal choice. In contrast, for majority choices (Figure 3 (b)), most points are located at the right-bottom of the plane, which indicates that the majority choices are very close to the final optimal choices. 3.3 KEY INSIGHTS: CHOICE FREEZING: The above comparative experiment shows that majority choices are very close to optimal choices and can be safely used to approximate optimal choices. This provide a solid foundation for us to freeze the majority choices in the shallow blocks when they appear in the early searching stage. The motivation of freezing the block choices is illustrated in Fig.4. As shown in Fig 4 (a), assuming there are two candidate subnets to be evaluated sequentially at the current searching epoch, the choices of the first two blocks CB1,CB2 of Net1 and Net2 are {blue, red} and {yellow, red} respectively. Since the choices of these two shallow blocks are not totally the same (even though the choices of CB2 are the same), the intermediate feature maps (FM1, FM2) in Net1 and Net2 will be different. Hence, to obtain the evaluation results for both subnets, the computations of all the blocks are required. However, as shown in Figure 4 (b), if the choices in CB1 and CB2 are already majority choices in the previous searching epoch and we managed to freeze them at the i-th search generation, Net1 will have the same choices for CB1 and CB2 as Net2, so does Net3 to Net50 in the current generation and all other subnets in the following generations. Since one-shot NAS uses the same validation dataset as input, the FM1 and FM2 of these subnets will be exactly the same and the computations for CB1 and CB2 will become redundant. Therefore, one can simply store the FM2 and reuse it for all the subsequent evaluation of candidate subnets, with a significant computation savings of all frozen blocks. Note that the stored FMs can be reused both within and across generations by every subnet evaluation process, which brings more computation savings than NASA (Ma et al., 2021). 3.4 PROGRESSIVE CHOICE FREEZING EVOLUTIONARY SEARCH: To further explore the behaviour of the choices of deeper blocks, we keep monitoring the choice percentage of every subsequent block once the previous blocks are frozen in the candidate subnet. As shown in Figure 5, we conduct three independent search processes where the first 3, 6, and 8 continuous block choices are frozen according to the majority choice percentage threshold. For example, the orange curve represents the choice percentage of the 7th block B7 if we freeze the first 6 continuous blocks at the 5th generation (marked by the dot line). It can be seen that, for all three blocks in the figure, the choice percentage increases drastically after previous blocks are fixed. Since higher choice percentage means higher possibility that a majority choice is observed, freezing the previous block choices will help freeze the latter block choices. Based on this observation, we propose to freeze more block choices progressively when the previous ones are already frozen. The overall idea is implemented in our proposed Progressive Choice Freezing Evolutionary Search (PCF-ES) algorithm in Alg. 1. The population of the first generation is randomly generated (Line Algorithm 1 Progressive Choice Freezing Evolutionary Search Input: supernet weight WA, population size P , architecture constraints C, max generation T , validation dataset Dv , choice percentage threshold H , mutation rate r, number of blocks N , monitor size f Output: architecture with best validation performance 1: Random initialize P0 2: c = 0 3: for i = 1 : T do 4: if c = 0 then 5: for j = N : 1 do 6: average choice percentage Mj from Bj to B1 7: block ratio Brj from Bj to B1 8: if M > H and Brj > 0.5 then 9: a1\u223cjTop\u2212K = a 1\u223cj maj 10: c = 1 11: break 12: end if 13: end for 14: else 15: for x = f : 1 do 16: average choice percentage Mf from Bj+1 to Bj+f 17: if Mf > H and Brf > 0.5 then 18: a1\u223c(j+x)Top\u2212K = a 1\u223c(j+x) maj 19: end if 20: end for 21: end if 22: Pmutation = Mutation(Topk, r, C) 23: Pcrossover = Crossover(Topk,C) 24: P = Pmutation \u222a Pcrossover 25: Acc = Inference(WA, Dv, P ) 26: end for 27: Return architecture with highest accuracy 1) and c is a flag to check if there are block choices that have been frozen (Line 4). When c is 0 which implies no block choice has been frozen, we calculate and examine the average choice percentage of the first j blocks at each search generation, and j starts from N (the number of blocks in a subnet) to 1 because we aim to freeze as many block choices as possible at the beginning (Lines 5-6). Note that the average choice percentage Mj is the mean of choice percentage from block B1 to Bj . One obvious downside of the average choice percentage is that it cannot indicate the choice percentage of each block, thus the choices of some blocks with relatively low choice percentage may be mistakenly frozen when the averaged choice percentage across the investigated shallow blocks exceeds the threshold. We employ another metric, i.e., block ratio, to avoid the aggressive choice frozen. block ratio measures the ratio of blocks whose choice percentage exceeds the threshold. For example, if the average choice percentage of the first 10 blocks exceeds the threshold, among which only 7 of these blocks exceeds the threshold, then the block ratio is 70%. Only when the block ratio is above the block ratio threshold, we allow the block choices to be frozen, which ensures that most of the frozen block choices reach the required choice percentage. If both metrics, i.e., average choice percentage and block ratio, are above the thresholds (Line 8), choices of the first j blocks in all Top-K subnets are changed to the current majority choices (Line 9) and become frozen. Accordingly, we set flag c = 1 when the first j blocks are frozen. For the rest of the searching process, these block choices will not be affected by the mutation and crossover operations. On the other hand, if any of the two metrics is below the threshold and the flag c = 0, the algorithm jumps to population generation and generates new candidates normally (Lines 22-24). If the first j blocks can be frozen, we keep exploring chances to freeze more block choices during the later searching stage. The monitor size f is the maximum number of blocks we should monitor at each generation in the latter searching stage (Line 15). The purpose of using monitor size here is to prevent aggressive block freezing. Specifically, since the search space is already remarkably narrowed down in the initial freezing step (up to 8 continuous shallow blocks can be frozen at a time, while a supernet usually has a total of 20 blocks), keep freezing too many subsequent blocks (even if their average choice percentage and block ratio are above the threshold) can make the search space so small that finally leads to inaccurate search results. In the progressively freezing phase, the average choice percentage Mf is the mean of choice percentage from block Bj+1 to Bj+f (Line 16). If the Mf is above the threshold, blocks Bj+1 to Bj+f in Top-K subnets will be frozen based on the current majority choice. After all candidate subnets are generated, they are evaluated on the validation dataset and ranked based on the validation accuracy (Line 25). We keep track of the Top-K subnets and return the Top-1 subnet architecture when maximum generation is reached or all block choices are frozen. 3.5 DISTINGUISH-BASED IMPORTANCE SAMPLING: To achieve the potential computation savings, it\u2019s necessary to store the intermediate feature maps, e.g. FM2 in Fig.4, of all the input samples in the validation dataset, the size of which may exceed the memory capacity of the GPUs in some circumstances. For instance, for each input sample, the intermediate data size of the second block of subnets in SPOS on ImageNet is approximately 0.8 MB. If the validation dataset contains more than 5000 input images, the memory storage of all the intermediate feature maps could exceed a GPU\u2019s memory capacity with 4GB main memory. To tackle this problem, we first leverage Importance Sampling (Alain et al., 2015) to compress the intermediate data. The idea of importance sampling comes from the fact that not all the input samples contribute to distinguishing the evaluation accuracy of candidate subnets. For example, for images which are easy to be classified by most subnets, they barely contribute to distinguish the performance of the searched subnets. Inspired by the importance sampling during training (Zeng et al., 2021), we follow the similar principle to sample the important data for the subnets evaluation. Let x denotes the input data, p denotes the uniform distribution referred from random sample, q denotes the distribution adopted by importance sampling and f(x) denotes the evaluation accuracy of input data x. The unbiased estimation of evaluation accuracy can be obtained by Eq.(1). Ep[f(x)] = \u222b p(x)f(x)dx = Eq[ p(x) q(x) f(x)] (1) if q(x) > 0 whenever p(x) > 0. Moreover, the estimation variance of the distribution q is minimized when q(x)\u2217 = 1 Z p(x)||f(x)||2, and Z = \u222b p(x)||f(x)||2dx (2) In our design, we use the cross-entropy loss to approximate the evaluation accuracy and then calculate the important sampling distribution q(x)\u2217 based on Eq.(2). Although we can obtain the important samples for a certain subnet using the above method, we cannot reuse the intermediate data of these important samples for other subnet\u2019s evaluation unless the important samples are shared across different subnets. Therefore, we quantitatively analyze the importance of samples for different subnets after the shallow blocks are frozen. As shown in Fig.6, we present the importance distribution of 500 important samples (x-axis) for 50 different subnets (y-axis). The color represents the importance of a sample. It can be found that the sample with high importance (deep blue) for one subnet is also important for other subnets. Overall, the importance distribution of a certain subnet (a certain row) is similar to the importance distribution of other subnets (other rows). As a result, it is only necessary to store the intermediate data of important samples for one subnet and then reuse it for other subnets. 4 EXPERIMENT:  4.1 EXPERIMENT SETUP: Our experiments are conducted on widely used ImageNet(Krizhevsky et al., 2012), of which the validation dataset contains 50000 images at 1000 categories. Both supernet training and subnet searching processes are performed on Tesla P100 GPUs with 16 GB memory. We employ the chain structure supernet adopted with three benchmarks: SPOS (Guo et al., 2020), FairNAS (Chu et al., 2021) and GreedyNAS (You et al., 2020), whose supernets contain 20, 19 and 21 blocks, respectively. The supernets are trained with the parameter settings in the original papers. The maximum search generation is 20 and the population size is 50. At each generation, subnets with top-10 validation accuracy are kept as parent networks. 4.2 EXPERIMENT RESULTS: The search results on different benchmarks and search methods are shown in Table 1. Evolutionary represents the original evolutionary algorithm used in (Guo et al., 2020), which is regarded as the baseline in our study. Choice Frozen Evolutionary Search (CF-ES) represents the method that only freezes shallow blocks and does not monitor the deep blocks for further freezing. Thanks to the reduced computations for the frozen shallow blocks, CF-ES method achieves averagely 19% time saving and 18% energy saving while maintaining Top-1 and Top-5 accuracy compared to the baseline. Progressively Choice Freezing Evolutionary Search (PCF-ES) is our proposed method that freezes both shallow and deep blocks during the searching, which achieves much more latency reduction with little accuracy loss. Specifically, PCF-ES reduces 38% \u223c 55% search latency and 37% \u223c 51% GPU energy consumption in three benchmarks. To understand where the speedup exactly comes from, we further breakdown the execution time at each generation when using different searching methods. As shown in Fig.7, the searching time (latency) of each generation is close to each other in the original evolutionary search, while CF-ES takes less time since the 6th generation because shallow blocks are frozen. The latency of PCF-ES begins to decrease from the 6th generation and keeps dropping progressively until it becomes 0 in the 13th generation, during which all blocks are frozen. In other words, it takes only 13th searching epochs to find the optimal architecture when using PCF-ES method. Fig.8 shows the Top-1 searching accuracy during the searching stage for different approaches and benchmarks. It can be observed that, for all benchmarks, all three methods have the same accuracy at the early generations when no choices are frozen. Compare to other two methods, PCF-ES shows very little accuracy loss in SPOS and FairNAS, and can always terminates earlier. To determine appropriate hyper-parameters, i.e., choice percentage threshold, block ratio threshold, and monitor size, used in our algorithm, we conduct sensitivity analysis for all three types of thresholds. Note that the proper thresholds we obtained using one dataset are effective for different searching scenarios. We first study the effects of selecting the choice percentage threshold for dif- ferent benchmarks are shown in Fig.9. The monitor size and block ratio are empirically initialized to be 3 and 0.5. As can be seen, for SPOS, when choosing the smallest threshold (T = 0.6), the searching terminates at the earliest (at the 10th generation) with relatively low accuracy. This is because some shallow blocks are mistakenly frozen. Similar behavior can be observed when setting threshold as 0.3 for FairNAS and GreedyNAS. To avoid the blocks being frozen too aggressively, a proper threshold should be picked. For example, when setting threshold as 0.7 for SPOS and 0.4 for FairNAS and GreedyNAS, the searching process maintains relatively high accuracy while terminating early as well. These thresholds are the optimal thresholds that ensure both high accuracy and low latency at the same time. The effects of selecting different block ratios are not shown here since we observed similar results to the effects of the choice percentage threshold. We empirically set block ratio as 0.5 in all three benchmarks. Fig.10 shows the searching accuracy and saved computations with different monitor sizes. For SPOS and GreedyNAS, the accuracy fluctuate with the increase of the monitor size. However, for FairNAS, the accuracy drops significantly when the monitor size is greater than 2. This is because too many deep blocks are frozen in this case. The saved computation increases steadily in all benchmarks as the number of monitored blocks increases. Fig.11 shows the validation accuracy of the optimal architecture searched by PCF-ES with different importance sampling rate (the ratio of sampled data to total data). We observe that when the sample rate is higher than 40%, the accuracy impact is negligible. However, the accuracy drops rapidly if the sample rate further decreases. 5 CONCLUSION: In this work, we profiled the evolutionary searching process of one-shot NAS and observed the key observations regarding the choice of blocks. Motivated by the majority choices that appears at the shallow blocks at the early searching stage, we proposed a progressively choice freezing evolutionary search to narrow the search space and reduce the searching time. As our evaluation results show, our method reduces averagely 46% of searching latency and 43% of energy consumption for all benchmarks while incurs only 0.1% accuracy loss. A APPENDIX: A.1 OBSERVATION As introduced in Section 3.1, the majority choices of shallow blocks can be determined in the early search generations while deep blocks cannot. Here we present the choice evolution of more blocks in two other one-shot NAS benchmarks: FairNAS and GreedyNAS. As shown in Fig. 12. It can be found that the majority choices of shallow blocks appears in the early search stage and their percentage increases steadily. On the contrary, the majority choice in deep blocks can not be decided during the search process. A.2 PROGRESSIVELY CHOICE FREEZING In section 3.4, we show that in SPOS, the choice percentage of the subsequent blocks increases rapidly after the previous blocks are frozen. We further conduct the same experiments for other two benchmarks. As shown in Fig. 13, for all six searching processes in (a) and (b), the choice percentage of subsequent blocks increases dramatically after freezing the previous blocks. A.3 THE EFFECTS OF QUANTIZATION 50% 73.4 73.4 73.1 73.0:  30% 73.1 73.1 72.8 72.8:  10% 73.1 73.1 73.0 72.8: In an attempt to achieve more memory savings, we tested another compression technique, i.e., quantization, on our sampled dataset. Quantization is a commonly used compression method that converts a floating-point value to a fixed-point value with fewer bits. Here the quantization is applied on the intermediate data before storing them into the memory. The validation accuracy with different importance sampling rate and quantization bit for SPOS benchmark is shown in Table 2. It can be seen that the search accuracy maintains when the intermediate data are quantized to 16 bit, but drops considerably when further quantized to lower bit. For the configuration of PCF-ES method, we use 40% sample rate and 16-bit quantization for the best searching performance.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It appears I have exceeded my quota for the Gemini API. I need to manage my resources more carefully. I will check my budget and see if I can proceed with a different model or if I need to ask the user to authorize an increase to the expense budget.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 0.11867510250000002,\n    \"current_remaining_expense_budget\": 9.8813248975\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 0.11867510250000002, 'current_remaining_expense_budget': 9.8813248975}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I still have a reasonable expense budget remaining. I will check the costs associated with the available models and select a more cost-effective option than `gpt-4` if possible.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The `gemini-1.5-pro` model seems to be the most reasonable in terms of expense cost. I will retry the reviews using this model.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerAlpha', 'prompt': 'Review the following ICLR 2023 paper. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search\\n\\n1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks (Elsken et al., 2019; Wistuba et al., 2019; Ren et al., 2020). NAS approaches automate the architecture design process and can achieve higher accuracy compared to human-designed architectures (Liu et al., 2019; Xie et al., 2019; Cai et al., 2019). However, the early NAS methods, such as reinforcement NAS (Zoph & Le, 2016), came with the price of expensive computation costs since every searched architecture needs to be trained from scratch, which makes the total search time unacceptable. To reduce the search cost of earlier NAS methods, the weight sharing technique has been proposed (Yu et al., 2020; Chen et al., 2020), among which the one-shot NAS method has attracted a lot of attention recently (Bender et al., 2018; Li et al., 2020). The one-shot NAS method is known as cost-efficient as it requires training a supernet only once. A supernet is a stack of basic blocks, each of which contains multiple choices. A candidate network architecture (defined as subnet) can be formed by selecting one choice for each block in the supernet, and its corresponding weights can be inherited from the supernet. During the architecture searching stage, candidate architectures are evaluated on the validation dataset and the best architecture, i.e., the architecture with the highest validation accuracy, is updated in every searching epoch of Evolutionary Algorithm (EA) (Real et al., 2019). Surprisingly, although training is commonly deemed as a lengthy and energy-consuming task, the architecture searching stage in one-shot NAS is much more costly (Cai et al., 2020) than training a supernet. The reason is that a new searching stage should be performed whenever a different searching scenario is given, e.g., different hardware constraints, learning tasks, and workloads, while the trained supernet can be reused. Hence, the numerous inferences on the subnets can take a much longer time than training a supernet only once. According to (You et al., 2020), searching can be 10 GPU days longer than supernet training when 10 different constraints/platforms are required. To tackle this problem, our work first makes a key observation that, for the first few continuous blocks of the candidate architecture (defined as continuous shallow blocks), their optimal choices can be determined at an early search epoch. Based on this observation, we propose to freeze the choices of continuous shallow blocks at the early search epoch, which means these choices will not be changed during the remaining search epochs. This strategy elaborately \u201ccreates\u201d redundant computations in the continuous shallow blocks since all candidates will share exactly the same architecture, inherited weights, and input validation data for the shallow blocks during later search epochs. Then we leverage such redundancy and propose a simple yet effective data reuse scheme to save large amounts of computations, thus further reducing time and energy cost. Specifically, we propose to reuse the last output of the continuous shallow blocks instead of re-computing it repeatedly throughout the remaining searching stage. Interestingly, we further discover that the freezing strategy may in turn help to determine the optimal choices of the subsequent blocks earlier. Such phenomenon enables us to keep freezing the choices of blocks progressively after the initial freezing, which will create more redundant computations of the blocks that possess the same architecture (choice), thus more computations can be saved. With the proposed freezing technique, the intermediate data (the last output of the continuous shallow blocks) of a certain subnet can be stored and reused during the evaluation of other subnets. However, as the searching stage requires to evaluate a subnet with a large batch (e.g., batch size = 5000) of input samples, storing the intermediate data of only one subnet may cause serious memory issues (Sec.3.5). Inspired by the Importance Sampling technique employed in many training methods (Zeng et al., 2021), we propose to sample the \u201cimportant\u201d input data that contribute more to distinguish the evaluation accuracy of the candidate subnets. More importantly, we empirically demonstrate that the important samples are shared across different subnets. Therefore, it only requires to store the intermediate data of important samples for one certain subnet, and then reuse it for all other subnets. We evaluate the proposed method on multiple benchmarks trained with the state-of-the-art approaches on the ImageNet dataset (Krizhevsky et9al., 2012). The experimental results indicate superb performance in improving the search efficiency while maintaining the search performance with only 0.1% searching accuracy loss. Our contributions can be summarized as follows: \u00b7 We observe that, in the one-shot NAS evolutionary searching stage, the optimal architecture of shallow blocks is determined at the early searching stage. \u00b7 We propose to freeze the choices of continuous shallow blocks for all candidates at the early stage, and progressively freeze the choices of the subsequent blocks in the later stage. This approach creates a great amount of redundant computations, which provide us a good opportunity to reuse the intermediate data and reduce the searching time. \u00b7 To alleviate memory capacity issue for storing intermediate data, we leverage the concept of importance sampling and propose a distinguish-based sampling method to reduce the size of the intermediate data. \u00b7 We conduct extensive experiments on different benchmarks with our proposed methods. The evaluation results show that our method can achieve up to 55% time saving and 51% energy saving with 0.1% accuracy loss. 2 REVIEW OF ONE-SHOT NAS: Different from the traditional neural network training that aims to optimize weights given a network architecture, NAS seeks to optimize both weight and architecture at the same time. Conventional NAS methods (Zoph & Le, 2016; Baker et al., 2016; Zhong et al., 2018; Zela et al., 2018) have tried to solve these two optimization problems at a nested approach. However, these methods are usually prohibitively expensive because each architecture sampled from the search space has to be trained from the scratch and evaluated separately. Recent works (Bender et al., 2018; Pham et al., 2018; Cai et al., 2019; 2018) have proposed a weight sharing strategy to reduce high costs of architecture and weight searching procedure in conventional NAS. As one of the most popular weight sharing techniques, one-shot NAS achieves unprecedented search efficiency by decoupling the whole searching process into two stages: supernet training (Fig.1 (a))and subnet searching (Fig.1 (b)). One-shot NAS encodes the search space into a supernet and trains it only once. Then it allows sampled architectures directly inherit the weights from the supernet. As shown in Fig.1 (a), the commonly used chain-structured supernet (Suganuma et al., 2017) is a stack of blocks, and each block consists of multiple choices. Each choice can be different operations, e.g., a 3\u00d73 / 5\u00d75 convolutional layer, a max-pooling layer or an identity layer. We further define the first few blocks in the supernet as shallow blocks, while the subsequent blocks are defined as deep blocks. The second step of one-shot NAS is the searching stage which employs EA to find the subnet architecture with the highest validation accuracy. Each subnet can be obtained by selecting one choice for each block in the supernet, and its weights are inherited from the supernet. The overall workflow is shown in Fig.1 (b). At each search epoch, also known as generation, there are N candidate subnets in the candidate pool sampled from the supernet and evaluated using validation dataset. Then the candidate subnets are ranked together with the previous top candidates according to the validation accuracy. Candidates with Top-K validation accuracy are selected and then evolved into N new subnets by performing mutation and crossover operations (Real et al., 2019). The above process is repeated until reaching the maximum number of search epochs E, and the subnet with the highest accuracy can be obtained. The architecture of this subnet will be regarded as the optimal architecture since the validation accuracy obtained by using inherited weights is highly predictive on the accuracy obtained by training from scratch (Bender et al., 2018). Recently, many studies focused on improving the efficiency and accuracy of the supernet training stage by introducing various subnet sampling methods (Guo et al., 2020; Chu et al., 2021; You et al., 2020). Nevertheless, none of them considered the efficiency of the searching stage. The searching stage can be more time-consuming compared with supernet training (Cai et al., 2020). This is because the supernet only need to be trained once while numerous searching processes are required to search the optimal architecture for various deployment scenarios, e.g., different hardware platforms, workloads. Therefore, it is essential to improve the search efficiency. NASA (Ma et al., 2021) is the first work that focuses on accelerating the search process, and a NAS accelerator was proposed that utilizes network fusion based on the computation sharing and data reuse within a search generation. Different from their approach, we propose an algorithm-level optimization for one-shot NAS evolutionary search and exploit data sharing from both within generation and across generation levels, which achieves significant improvement on the searching efficiency. 3 METHODOLOGY: This section first introduces several key observations during the evolutionary searching phase, based on which the progressive choice freezing evolutionary search (PCF-ES) is then proposed, along with how it saves the computations during the searching stage. Finally, to mitigate the memory issue of our method, the distinguish-based importance sampling method is proposed to reduce the size of the intermediate data. 3.1 OBSERVATION: MAJORITY CHOICE IN CONTINUOUS SHALLOW BLOCKS: Figure 2 illustrates how the choices of blocks evolve across generations during the evolutionary search in SPOS (Guo et al., 2020). The right-up subgraphs record the choice evolution in shallow blocks. Especially, we take the 1st, the 2nd, and the 3rd blocks as the examples to show their choice evolution across the search generations, respectively. The right-bottom subgraphs are the choice evolution in deep blocks, and the 15th, the 16th, and the 17th blocks are used as the examples. Each curve in the subgraph indicates the percentage of subnets among all candidate subnets in a certain generation that select a certain choice for the block. As an example shown in the left part of the figure, at the i-th generation, the percentage of the blue choice in the 1st block equals to 80% if 40 out of 50 subnets select the blue choice. As can be seen in the right-up subgraphs, there is one choice whose percentage increases dramatically and surpasses all the other choices in the early generations (i \u2264 10, highlighted by the left of the blue dotted line). We define such choice as the majority choice. We further observed that the majority choice will be selected by more subnets and become even more dominating for the shallow blocks in the following generations. However, this is not the case for the deep blocks. In the right-bottom subgraphs, we observed that either the majority choice emerges at very later generations, or multiple curves intertwine with each other, thus there is no obvious majority choice for deep blocks. The above observations indicate that candidate subnets tend to vote majority choice in the shallow blocks at the early searching stage, while for deeper blocks the majority choice is unclear. 3.2 RELATION BETWEEN THE MAJORITY CHOICE AND THE OPTIMAL CHOICE: Majority choice is the choice that most of the candidate subnets \u201dthink\u201d it can lead to better accuracy during evolution. Intuitively, majority choice should be representative of optimal choice. To quantitatively analyze the relationship between the majority choice and the optimal choice, we first define a binary categorical choice distance D, where D equals to 0 if two choices are the same otherwise D equals to 1. Then we calculate four types of D for the choices in the continuous shallow blocks as follows: the average distance D between the non-majority choices and the block choices in the first generation (i.e., initial block choices), the average distance D between the non-majority choices and the block choices in the last generation (i.e., optimal block choices), the average distance D between the majority choices and the block choices in the first generation, and the average distance D between the majority choices and the block choices in the last generation. Here, all D denotes the averaged distance across all shallow blocks. Note that the non-majority choices are all the choices of shallow blocks when their average percentage is below an empirically predetermined threshold (0.7) at the early searching stage. The majority choices are obtained once we observe their average percentage rising above 0.7. The results of non-majority choices and majority choices are shown in Figure 3 (a) and (b), respectively. The x-axis (y-axis) represents the distance between the nonmajority/majority choices and the block choices of the first (last) generation. Each point represents an independent searching process with random seeds. As can be seen, for non-majority choices (Figure 3 (a)), the points are scattered in a symmetrical way, which means a certain non-majority choice can be either close or far away from the optimal choice. In contrast, for majority choices (Figure 3 (b)), most points are located at the right-bottom of the plane, which indicates that the majority choices are very close to the final optimal choices. 3.3 KEY INSIGHTS: CHOICE FREEZING: The above comparative experiment shows that majority choices are very close to optimal choices and can be safely used to approximate optimal choices. This provide a solid foundation for us to freeze the majority choices in the shallow blocks when they appear in the early searching stage. The motivation of freezing the block choices is illustrated in Fig.4. As shown in Fig 4 (a), assuming there are two candidate subnets to be evaluated sequentially at the current searching epoch, the choices of the first two blocks CB1,CB2 of Net1 and Net2 are {blue, red} and {yellow, red} respectively. Since the choices of these two shallow blocks are not totally the same (even though the choices of CB2 are the same), the intermediate feature maps (FM1, FM2) in Net1 and Net2 will be different. Hence, to obtain the evaluation results for both subnets, the computations of all the blocks are required. However, as shown in Figure 4 (b), if the choices in CB1 and CB2 are already majority choices in the previous searching epoch and we managed to freeze them at the i-th search generation, Net1 will have the same choices for CB1 and CB2 as Net2, so does Net3 to Net50 in the current generation and all other subnets in the following generations. Since one-shot NAS uses the same validation dataset as input, the FM1 and FM2 of these subnets will be exactly the same and the computations for CB1 and CB2 will become redundant. Therefore, one can simply store the FM2 and reuse it for all the subsequent evaluation of candidate subnets, with a significant computation savings of all frozen blocks. Note that the stored FMs can be reused both within and across generations by every subnet evaluation process, which brings more computation savings than NASA (Ma et al., 2021). 3.4 PROGRESSIVE CHOICE FREEZING EVOLUTIONARY SEARCH: To further explore the behaviour of the choices of deeper blocks, we keep monitoring the choice percentage of every subsequent block once the previous blocks are frozen in the candidate subnet. As shown in Figure 5, we conduct three independent search processes where the first 3, 6, and 8 continuous block choices are frozen according to the majority choice percentage threshold. For example, the orange curve represents the choice percentage of the 7th block B7 if we freeze the first 6 continuous blocks at the 5th generation (marked by the dot line). It can be seen that, for all three blocks in the figure, the choice percentage increases drastically after previous blocks are fixed. Since higher choice percentage means higher possibility that a majority choice is observed, freezing the previous block choices will help freeze the latter block choices. Based on this observation, we propose to freeze more block choices progressively when the previous ones are already frozen. The overall idea is implemented in our proposed Progressive Choice Freezing Evolutionary Search (PCF-ES) algorithm in Alg. 1. The population of the first generation is randomly generated (Line Algorithm 1 Progressive Choice Freezing Evolutionary Search Input: supernet weight WA, population size P , architecture constraints C, max generation T , validation dataset Dv , choice percentage threshold H , mutation rate r, number of blocks N , monitor size f Output: architecture with best validation performance 1: Random initialize P0 2: c = 0 3: for i = 1 : T do 4: if c = 0 then 5: for j = N : 1 do 6: average choice percentage Mj from Bj to B1 7: block ratio Brj from Bj to B1 8: if M > H and Brj > 0.5 then 9: a1\u223cjTop\u2212K = a 1\u223cj maj 10: c = 1 11: break 12: end if 13: end for 14: else 15: for x = f : 1 do 16: average choice percentage Mf from Bj+1 to Bj+f 17: if Mf > H and Brf > 0.5 then 18: a1\u223c(j+x)Top\u2212K = a 1\u223c(j+x) maj 19: end if 20: end for 21: end if 22: Pmutation = Mutation(Topk, r, C) 23: Pcrossover = Crossover(Topk,C) 24: P = Pmutation \u222a Pcrossover 25: Acc = Inference(WA, Dv, P ) 26: end for 27: Return architecture with highest accuracy 1) and c is a flag to check if there are block choices that have been frozen (Line 4). When c is 0 which implies no block choice has been frozen, we calculate and examine the average choice percentage of the first j blocks at each search generation, and j starts from N (the number of blocks in a subnet) to 1 because we aim to freeze as many block choices as possible at the beginning (Lines 5-6). Note that the average choice percentage Mj is the mean of choice percentage from block B1 to Bj . One obvious downside of the average choice percentage is that it cannot indicate the choice percentage of each block, thus the choices of some blocks with relatively low choice percentage may be mistakenly frozen when the averaged choice percentage across the investigated shallow blocks exceeds the threshold. We employ another metric, i.e., block ratio, to avoid the aggressive choice frozen. block ratio measures the ratio of blocks whose choice percentage exceeds the threshold. For example, if the average choice percentage of the first 10 blocks exceeds the threshold, among which only 7 of these blocks exceeds the threshold, then the block ratio is 70%. Only when the block ratio is above the block ratio threshold, we allow the block choices to be frozen, which ensures that most of the frozen block choices reach the required choice percentage. If both metrics, i.e., average choice percentage and block ratio, are above the thresholds (Line 8), choices of the first j blocks in all Top-K subnets are changed to the current majority choices (Line 9) and become frozen. Accordingly, we set flag c = 1 when the first j blocks are frozen. For the rest of the searching process, these block choices will not be affected by the mutation and crossover operations. On the other hand, if any of the two metrics is below the threshold and the flag c = 0, the algorithm jumps to population generation and generates new candidates normally (Lines 22-24). If the first j blocks can be frozen, we keep exploring chances to freeze more block choices during the later searching stage. The monitor size f is the maximum number of blocks we should monitor at each generation in the latter searching stage (Line 15). The purpose of using monitor size here is to prevent aggressive block freezing. Specifically, since the search space is already remarkably narrowed down in the initial freezing step (up to 8 continuous shallow blocks can be frozen at a time, while a supernet usually has a total of 20 blocks), keep freezing too many subsequent blocks (even if their average choice percentage and block ratio are above the threshold) can make the search space so small that finally leads to inaccurate search results. In the progressively freezing phase, the average choice percentage Mf is the mean of choice percentage from block Bj+1 to Bj+f (Line 16). If the Mf is above the threshold, blocks Bj+1 to Bj+f in Top-K subnets will be frozen based on the current majority choice. After all candidate subnets are generated, they are evaluated on the validation dataset and ranked based on the validation accuracy (Line 25). We keep track of the Top-K subnets and return the Top-1 subnet architecture when maximum generation is reached or all block choices are frozen. 3.5 DISTINGUISH-BASED IMPORTANCE SAMPLING: To achieve the potential computation savings, it\u2019s necessary to store the intermediate feature maps, e.g. FM2 in Fig.4, of all the input samples in the validation dataset, the size of which may exceed the memory capacity of the GPUs in some circumstances. For instance, for each input sample, the intermediate data size of the second block of subnets in SPOS on ImageNet is approximately 0.8 MB. If the validation dataset contains more than 5000 input images, the memory storage of all the intermediate feature maps could exceed a GPU\u2019s memory capacity with 4GB main memory. To tackle this problem, we first leverage Importance Sampling (Alain et al., 2015) to compress the intermediate data. The idea of importance sampling comes from the fact that not all the input samples contribute to distinguishing the evaluation accuracy of candidate subnets. For example, for images which are easy to be classified by most subnets, they barely contribute to distinguish the performance of the searched subnets. Inspired by the importance sampling during training (Zeng et al., 2021), we follow the similar principle to sample the important data for the subnets evaluation. Let x denotes the input data, p denotes the uniform distribution referred from random sample, q denotes the distribution adopted by importance sampling and f(x) denotes the evaluation accuracy of input data x. The unbiased estimation of evaluation accuracy can be obtained by Eq.(1). Ep[f(x)] = \u222b p(x)f(x)dx = Eq[ p(x) q(x) f(x)] (1) if q(x) > 0 whenever p(x) > 0. Moreover, the estimation variance of the distribution q is minimized when q(x)\u2217 = 1 Z p(x)||f(x)||2, and Z = \u222b p(x)||f(x)||2dx (2) In our design, we use the cross-entropy loss to approximate the evaluation accuracy and then calculate the important sampling distribution q(x)\u2217 based on Eq.(2). Although we can obtain the important samples for a certain subnet using the above method, we cannot reuse the intermediate data of these important samples for other subnet\u2019s evaluation unless the important samples are shared across different subnets. Therefore, we quantitatively analyze the importance of samples for different subnets after the shallow blocks are frozen. As shown in Fig.6, we present the importance distribution of 500 important samples (x-axis) for 50 different subnets (y-axis). The color represents the importance of a sample. It can be found that the sample with high importance (deep blue) for one subnet is also important for other subnets. Overall, the importance distribution of a certain subnet (a certain row) is similar to the importance distribution of other subnets (other rows). As a result, it is only necessary to store the intermediate data of important samples for one subnet and then reuse it for other subnets. 4 EXPERIMENT:  4.1 EXPERIMENT SETUP: Our experiments are conducted on widely used ImageNet(Krizhevsky et al., 2012), of which the validation dataset contains 50000 images at 1000 categories. Both supernet training and subnet searching processes are performed on Tesla P100 GPUs with 16 GB memory. We employ the chain structure supernet adopted with three benchmarks: SPOS (Guo et al., 2020), FairNAS (Chu et al., 2021) and GreedyNAS (You et al., 2020), whose supernets contain 20, 19 and 21 blocks, respectively. The supernets are trained with the parameter settings in the original papers. The maximum search generation is 20 and the population size is 50. At each generation, subnets with top-10 validation accuracy are kept as parent networks. 4.2 EXPERIMENT RESULTS: The search results on different benchmarks and search methods are shown in Table 1. Evolutionary represents the original evolutionary algorithm used in (Guo et al., 2020), which is regarded as the baseline in our study. Choice Frozen Evolutionary Search (CF-ES) represents the method that only freezes shallow blocks and does not monitor the deep blocks for further freezing. Thanks to the reduced computations for the frozen shallow blocks, CF-ES method achieves averagely 19% time saving and 18% energy saving while maintaining Top-1 and Top-5 accuracy compared to the baseline. Progressively Choice Freezing Evolutionary Search (PCF-ES) is our proposed method that freezes both shallow and deep blocks during the searching, which achieves much more latency reduction with little accuracy loss. Specifically, PCF-ES reduces 38% \u223c 55% search latency and 37% \u223c 51% GPU energy consumption in three benchmarks. To understand where the speedup exactly comes from, we further breakdown the execution time at each generation when using different searching methods. As shown in Fig.7, the searching time (latency) of each generation is close to each other in the original evolutionary search, while CF-ES takes less time since the 6th generation because shallow blocks are frozen. The latency of PCF-ES begins to decrease from the 6th generation and keeps dropping progressively until it becomes 0 in the 13th generation, during which all blocks are frozen. In other words, it takes only 13th searching epochs to find the optimal architecture when using PCF-ES method. Fig.8 shows the Top-1 searching accuracy during the searching stage for different approaches and benchmarks. It can be observed that, for all benchmarks, all three methods have the same accuracy at the early generations when no choices are frozen. Compare to other two methods, PCF-ES shows very little accuracy loss in SPOS and FairNAS, and can always terminates earlier. To determine appropriate hyper-parameters, i.e., choice percentage threshold, block ratio threshold, and monitor size, used in our algorithm, we conduct sensitivity analysis for all three types of thresholds. Note that the proper thresholds we obtained using one dataset are effective for different searching scenarios. We first study the effects of selecting the choice percentage threshold for dif- ferent benchmarks are shown in Fig.9. The monitor size and block ratio are empirically initialized to be 3 and 0.5. As can be seen, for SPOS, when choosing the smallest threshold (T = 0.6), the searching terminates at the earliest (at the 10th generation) with relatively low accuracy. This is because some shallow blocks are mistakenly frozen. Similar behavior can be observed when setting threshold as 0.3 for FairNAS and GreedyNAS. To avoid the blocks being frozen too aggressively, a proper threshold should be picked. For example, when setting threshold as 0.7 for SPOS and 0.4 for FairNAS and GreedyNAS, the searching process maintains relatively high accuracy while terminating early as well. These thresholds are the optimal thresholds that ensure both high accuracy and low latency at the same time. The effects of selecting different block ratios are not shown here since we observed similar results to the effects of the choice percentage threshold. We empirically set block ratio as 0.5 in all three benchmarks. Fig.10 shows the searching accuracy and saved computations with different monitor sizes. For SPOS and GreedyNAS, the accuracy fluctuate with the increase of the monitor size. However, for FairNAS, the accuracy drops significantly when the monitor size is greater than 2. This is because too many deep blocks are frozen in this case. The saved computation increases steadily in all benchmarks as the number of monitored blocks increases. Fig.11 shows the validation accuracy of the optimal architecture searched by PCF-ES with different importance sampling rate (the ratio of sampled data to total data). We observe that when the sample rate is higher than 40%, the accuracy impact is negligible. However, the accuracy drops rapidly if the sample rate further decreases. 5 CONCLUSION: In this work, we profiled the evolutionary searching process of one-shot NAS and observed the key observations regarding the choice of blocks. Motivated by the majority choices that appears at the shallow blocks at the early searching stage, we proposed a progressively choice freezing evolutionary search to narrow the search space and reduce the searching time. As our evaluation results show, our method reduces averagely 46% of searching latency and 43% of energy consumption for all benchmarks while incurs only 0.1% accuracy loss. A APPENDIX: A.1 OBSERVATION As introduced in Section 3.1, the majority choices of shallow blocks can be determined in the early search generations while deep blocks cannot. Here we present the choice evolution of more blocks in two other one-shot NAS benchmarks: FairNAS and GreedyNAS. As shown in Fig. 12. It can be found that the majority choices of shallow blocks appears in the early search stage and their percentage increases steadily. On the contrary, the majority choice in deep blocks can not be decided during the search process. A.2 PROGRESSIVELY CHOICE FREEZING In section 3.4, we show that in SPOS, the choice percentage of the subsequent blocks increases rapidly after the previous blocks are frozen. We further conduct the same experiments for other two benchmarks. As shown in Fig. 13, for all six searching processes in (a) and (b), the choice percentage of subsequent blocks increases dramatically after freezing the previous blocks. A.3 THE EFFECTS OF QUANTIZATION 50% 73.4 73.4 73.1 73.0:  30% 73.1 73.1 72.8 72.8:  10% 73.1 73.1 73.0 72.8: In an attempt to achieve more memory savings, we tested another compression technique, i.e., quantization, on our sampled dataset. Quantization is a commonly used compression method that converts a floating-point value to a fixed-point value with fewer bits. Here the quantization is applied on the intermediate data before storing them into the memory. The validation accuracy with different importance sampling rate and quantization bit for SPOS benchmark is shown in Table 2. It can be seen that the search accuracy maintains when the intermediate data are quantized to 16 bit, but drops considerably when further quantized to lower bit. For the configuration of PCF-ES method, we use 40% sample rate and 16-bit quantization for the best searching performance.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '7s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ReviewerAlpha\",\n  \"prompt\": \"Review the following ICLR 2023 paper. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search\n\n1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks (Elsken et al., 2019; Wistuba et al., 2019; Ren et al., 2020). NAS approaches automate the architecture design process and can achieve higher accuracy compared to human-designed architectures (Liu et al., 2019; Xie et al., 2019; Cai et al., 2019). However, the early NAS methods, such as reinforcement NAS (Zoph & Le, 2016), came with the price of expensive computation costs since every searched architecture needs to be trained from scratch, which makes the total search time unacceptable. To reduce the search cost of earlier NAS methods, the weight sharing technique has been proposed (Yu et al., 2020; Chen et al., 2020), among which the one-shot NAS method has attracted a lot of attention recently (Bender et al., 2018; Li et al., 2020). The one-shot NAS method is known as cost-efficient as it requires training a supernet only once. A supernet is a stack of basic blocks, each of which contains multiple choices. A candidate network architecture (defined as subnet) can be formed by selecting one choice for each block in the supernet, and its corresponding weights can be inherited from the supernet. During the architecture searching stage, candidate architectures are evaluated on the validation dataset and the best architecture, i.e., the architecture with the highest validation accuracy, is updated in every searching epoch of Evolutionary Algorithm (EA) (Real et al., 2019). Surprisingly, although training is commonly deemed as a lengthy and energy-consuming task, the architecture searching stage in one-shot NAS is much more costly (Cai et al., 2020) than training a supernet. The reason is that a new searching stage should be performed whenever a different searching scenario is given, e.g., different hardware constraints, learning tasks, and workloads, while the trained supernet can be reused. Hence, the numerous inferences on the subnets can take a much longer time than training a supernet only once. According to (You et al., 2020), searching can be 10 GPU days longer than supernet training when 10 different constraints/platforms are required. To tackle this problem, our work first makes a key observation that, for the first few continuous blocks of the candidate architecture (defined as continuous shallow blocks), their optimal choices can be determined at an early search epoch. Based on this observation, we propose to freeze the choices of continuous shallow blocks at the early search epoch, which means these choices will not be changed during the remaining search epochs. This strategy elaborately \u201ccreates\u201d redundant computations in the continuous shallow blocks since all candidates will share exactly the same architecture, inherited weights, and input validation data for the shallow blocks during later search epochs. Then we leverage such redundancy and propose a simple yet effective data reuse scheme to save large amounts of computations, thus further reducing time and energy cost. Specifically, we propose to reuse the last output of the continuous shallow blocks instead of re-computing it repeatedly throughout the remaining searching stage. Interestingly, we further discover that the freezing strategy may in turn help to determine the optimal choices of the subsequent blocks earlier. Such phenomenon enables us to keep freezing the choices of blocks progressively after the initial freezing, which will create more redundant computations of the blocks that possess the same architecture (choice), thus more computations can be saved. With the proposed freezing technique, the intermediate data (the last output of the continuous shallow blocks) of a certain subnet can be stored and reused during the evaluation of other subnets. However, as the searching stage requires to evaluate a subnet with a large batch (e.g., batch size = 5000) of input samples, storing the intermediate data of only one subnet may cause serious memory issues (Sec.3.5). Inspired by the Importance Sampling technique employed in many training methods (Zeng et al., 2021), we propose to sample the \u201cimportant\u201d input data that contribute more to distinguish the evaluation accuracy of the candidate subnets. More importantly, we empirically demonstrate that the important samples are shared across different subnets. Therefore, it only requires to store the intermediate data of important samples for one certain subnet, and then reuse it for all other subnets. We evaluate the proposed method on multiple benchmarks trained with the state-of-the-art approaches on the ImageNet dataset (Krizhevsky et9al., 2012). The experimental results indicate superb performance in improving the search efficiency while maintaining the search performance with only 0.1% searching accuracy loss. Our contributions can be summarized as follows: \u00b7 We observe that, in the one-shot NAS evolutionary searching stage, the optimal architecture of shallow blocks is determined at the early searching stage. \u00b7 We propose to freeze the choices of continuous shallow blocks for all candidates at the early stage, and progressively freeze the choices of the subsequent blocks in the later stage. This approach creates a great amount of redundant computations, which provide us a good opportunity to reuse the intermediate data and reduce the searching time. \u00b7 To alleviate memory capacity issue for storing intermediate data, we leverage the concept of importance sampling and propose a distinguish-based sampling method to reduce the size of the intermediate data. \u00b7 We conduct extensive experiments on different benchmarks with our proposed methods. The evaluation results show that our method can achieve up to 55% time saving and 51% energy saving with 0.1% accuracy loss. 2 REVIEW OF ONE-SHOT NAS: Different from the traditional neural network training that aims to optimize weights given a network architecture, NAS seeks to optimize both weight and architecture at the same time. Conventional NAS methods (Zoph & Le, 2016; Baker et al., 2016; Zhong et al., 2018; Zela et al., 2018) have tried to solve these two optimization problems at a nested approach. However, these methods are usually prohibitively expensive because each architecture sampled from the search space has to be trained from the scratch and evaluated separately. Recent works (Bender et al., 2018; Pham et al., 2018; Cai et al., 2019; 2018) have proposed a weight sharing strategy to reduce high costs of architecture and weight searching procedure in conventional NAS. As one of the most popular weight sharing techniques, one-shot NAS achieves unprecedented search efficiency by decoupling the whole searching process into two stages: supernet training (Fig.1 (a))and subnet searching (Fig.1 (b)). One-shot NAS encodes the search space into a supernet and trains it only once. Then it allows sampled architectures directly inherit the weights from the supernet. As shown in Fig.1 (a), the commonly used chain-structured supernet (Suganuma et al., 2017) is a stack of blocks, and each block consists of multiple choices. Each choice can be different operations, e.g., a 3\u00d73 / 5\u00d75 convolutional layer, a max-pooling layer or an identity layer. We further define the first few blocks in the supernet as shallow blocks, while the subsequent blocks are defined as deep blocks. The second step of one-shot NAS is the searching stage which employs EA to find the subnet architecture with the highest validation accuracy. Each subnet can be obtained by selecting one choice for each block in the supernet, and its weights are inherited from the supernet. The overall workflow is shown in Fig.1 (b). At each search epoch, also known as generation, there are N candidate subnets in the candidate pool sampled from the supernet and evaluated using validation dataset. Then the candidate subnets are ranked together with the previous top candidates according to the validation accuracy. Candidates with Top-K validation accuracy are selected and then evolved into N new subnets by performing mutation and crossover operations (Real et al., 2019). The above process is repeated until reaching the maximum number of search epochs E, and the subnet with the highest accuracy can be obtained. The architecture of this subnet will be regarded as the optimal architecture since the validation accuracy obtained by using inherited weights is highly predictive on the accuracy obtained by training from scratch (Bender et al., 2018). Recently, many studies focused on improving the efficiency and accuracy of the supernet training stage by introducing various subnet sampling methods (Guo et al., 2020; Chu et al., 2021; You et al., 2020). Nevertheless, none of them considered the efficiency of the searching stage. The searching stage can be more time-consuming compared with supernet training (Cai et al., 2020). This is because the supernet only need to be trained once while numerous searching processes are required to search the optimal architecture for various deployment scenarios, e.g., different hardware platforms, workloads. Therefore, it is essential to improve the search efficiency. NASA (Ma et al., 2021) is the first work that focuses on accelerating the search process, and a NAS accelerator was proposed that utilizes network fusion based on the computation sharing and data reuse within a search generation. Different from their approach, we propose an algorithm-level optimization for one-shot NAS evolutionary search and exploit data sharing from both within generation and across generation levels, which achieves significant improvement on the searching efficiency. 3 METHODOLOGY: This section first introduces several key observations during the evolutionary searching phase, based on which the progressive choice freezing evolutionary search (PCF-ES) is then proposed, along with how it saves the computations during the searching stage. Finally, to mitigate the memory issue of our method, the distinguish-based importance sampling method is proposed to reduce the size of the intermediate data. 3.1 OBSERVATION: MAJORITY CHOICE IN CONTINUOUS SHALLOW BLOCKS: Figure 2 illustrates how the choices of blocks evolve across generations during the evolutionary search in SPOS (Guo et al., 2020). The right-up subgraphs record the choice evolution in shallow blocks. Especially, we take the 1st, the 2nd, and the 3rd blocks as the examples to show their choice evolution across the search generations, respectively. The right-bottom subgraphs are the choice evolution in deep blocks, and the 15th, the 16th, and the 17th blocks are used as the examples. Each curve in the subgraph indicates the percentage of subnets among all candidate subnets in a certain generation that select a certain choice for the block. As an example shown in the left part of the figure, at the i-th generation, the percentage of the blue choice in the 1st block equals to 80% if 40 out of 50 subnets select the blue choice. As can be seen in the right-up subgraphs, there is one choice whose percentage increases dramatically and surpasses all the other choices in the early generations (i \u2264 10, highlighted by the left of the blue dotted line). We define such choice as the majority choice. We further observed that the majority choice will be selected by more subnets and become even more dominating for the shallow blocks in the following generations. However, this is not the case for the deep blocks. In the right-bottom subgraphs, we observed that either the majority choice emerges at very later generations, or multiple curves intertwine with each other, thus there is no obvious majority choice for deep blocks. The above observations indicate that candidate subnets tend to vote majority choice in the shallow blocks at the early searching stage, while for deeper blocks the majority choice is unclear. 3.2 RELATION BETWEEN THE MAJORITY CHOICE AND THE OPTIMAL CHOICE: Majority choice is the choice that most of the candidate subnets \u201dthink\u201d it can lead to better accuracy during evolution. Intuitively, majority choice should be representative of optimal choice. To quantitatively analyze the relationship between the majority choice and the optimal choice, we first define a binary categorical choice distance D, where D equals to 0 if two choices are the same otherwise D equals to 1. Then we calculate four types of D for the choices in the continuous shallow blocks as follows: the average distance D between the non-majority choices and the block choices in the first generation (i.e., initial block choices), the average distance D between the non-majority choices and the block choices in the last generation (i.e., optimal block choices), the average distance D between the majority choices and the block choices in the first generation, and the average distance D between the majority choices and the block choices in the last generation. Here, all D denotes the averaged distance across all shallow blocks. Note that the non-majority choices are all the choices of shallow blocks when their average percentage is below an empirically predetermined threshold (0.7) at the early searching stage. The majority choices are obtained once we observe their average percentage rising above 0.7. The results of non-majority choices and majority choices are shown in Figure 3 (a) and (b), respectively. The x-axis (y-axis) represents the distance between the nonmajority/majority choices and the block choices of the first (last) generation. Each point represents an independent searching process with random seeds. As can be seen, for non-majority choices (Figure 3 (a)), the points are scattered in a symmetrical way, which means a certain non-majority choice can be either close or far away from the optimal choice. In contrast, for majority choices (Figure 3 (b)), most points are located at the right-bottom of the plane, which indicates that the majority choices are very close to the final optimal choices. 3.3 KEY INSIGHTS: CHOICE FREEZING: The above comparative experiment shows that majority choices are very close to optimal choices and can be safely used to approximate optimal choices. This provide a solid foundation for us to freeze the majority choices in the shallow blocks when they appear in the early searching stage. The motivation of freezing the block choices is illustrated in Fig.4. As shown in Fig 4 (a), assuming there are two candidate subnets to be evaluated sequentially at the current searching epoch, the choices of the first two blocks CB1,CB2 of Net1 and Net2 are {blue, red} and {yellow, red} respectively. Since the choices of these two shallow blocks are not totally the same (even though the choices of CB2 are the same), the intermediate feature maps (FM1, FM2) in Net1 and Net2 will be different. Hence, to obtain the evaluation results for both subnets, the computations of all the blocks are required. However, as shown in Figure 4 (b), if the choices in CB1 and CB2 are already majority choices in the previous searching epoch and we managed to freeze them at the i-th search generation, Net1 will have the same choices for CB1 and CB2 as Net2, so does Net3 to Net50 in the current generation and all other subnets in the following generations. Since one-shot NAS uses the same validation dataset as input, the FM1 and FM2 of these subnets will be exactly the same and the computations for CB1 and CB2 will become redundant. Therefore, one can simply store the FM2 and reuse it for all the subsequent evaluation of candidate subnets, with a significant computation savings of all frozen blocks. Note that the stored FMs can be reused both within and across generations by every subnet evaluation process, which brings more computation savings than NASA (Ma et al., 2021). 3.4 PROGRESSIVE CHOICE FREEZING EVOLUTIONARY SEARCH: To further explore the behaviour of the choices of deeper blocks, we keep monitoring the choice percentage of every subsequent block once the previous blocks are frozen in the candidate subnet. As shown in Figure 5, we conduct three independent search processes where the first 3, 6, and 8 continuous block choices are frozen according to the majority choice percentage threshold. For example, the orange curve represents the choice percentage of the 7th block B7 if we freeze the first 6 continuous blocks at the 5th generation (marked by the dot line). It can be seen that, for all three blocks in the figure, the choice percentage increases drastically after previous blocks are fixed. Since higher choice percentage means higher possibility that a majority choice is observed, freezing the previous block choices will help freeze the latter block choices. Based on this observation, we propose to freeze more block choices progressively when the previous ones are already frozen. The overall idea is implemented in our proposed Progressive Choice Freezing Evolutionary Search (PCF-ES) algorithm in Alg. 1. The population of the first generation is randomly generated (Line Algorithm 1 Progressive Choice Freezing Evolutionary Search Input: supernet weight WA, population size P , architecture constraints C, max generation T , validation dataset Dv , choice percentage threshold H , mutation rate r, number of blocks N , monitor size f Output: architecture with best validation performance 1: Random initialize P0 2: c = 0 3: for i = 1 : T do 4: if c = 0 then 5: for j = N : 1 do 6: average choice percentage Mj from Bj to B1 7: block ratio Brj from Bj to B1 8: if M > H and Brj > 0.5 then 9: a1\u223cjTop\u2212K = a 1\u223cj maj 10: c = 1 11: break 12: end if 13: end for 14: else 15: for x = f : 1 do 16: average choice percentage Mf from Bj+1 to Bj+f 17: if Mf > H and Brf > 0.5 then 18: a1\u223c(j+x)Top\u2212K = a 1\u223c(j+x) maj 19: end if 20: end for 21: end if 22: Pmutation = Mutation(Topk, r, C) 23: Pcrossover = Crossover(Topk,C) 24: P = Pmutation \u222a Pcrossover 25: Acc = Inference(WA, Dv, P ) 26: end for 27: Return architecture with highest accuracy 1) and c is a flag to check if there are block choices that have been frozen (Line 4). When c is 0 which implies no block choice has been frozen, we calculate and examine the average choice percentage of the first j blocks at each search generation, and j starts from N (the number of blocks in a subnet) to 1 because we aim to freeze as many block choices as possible at the beginning (Lines 5-6). Note that the average choice percentage Mj is the mean of choice percentage from block B1 to Bj . One obvious downside of the average choice percentage is that it cannot indicate the choice percentage of each block, thus the choices of some blocks with relatively low choice percentage may be mistakenly frozen when the averaged choice percentage across the investigated shallow blocks exceeds the threshold. We employ another metric, i.e., block ratio, to avoid the aggressive choice frozen. block ratio measures the ratio of blocks whose choice percentage exceeds the threshold. For example, if the average choice percentage of the first 10 blocks exceeds the threshold, among which only 7 of these blocks exceeds the threshold, then the block ratio is 70%. Only when the block ratio is above the block ratio threshold, we allow the block choices to be frozen, which ensures that most of the frozen block choices reach the required choice percentage. If both metrics, i.e., average choice percentage and block ratio, are above the thresholds (Line 8), choices of the first j blocks in all Top-K subnets are changed to the current majority choices (Line 9) and become frozen. Accordingly, we set flag c = 1 when the first j blocks are frozen. For the rest of the searching process, these block choices will not be affected by the mutation and crossover operations. On the other hand, if any of the two metrics is below the threshold and the flag c = 0, the algorithm jumps to population generation and generates new candidates normally (Lines 22-24). If the first j blocks can be frozen, we keep exploring chances to freeze more block choices during the later searching stage. The monitor size f is the maximum number of blocks we should monitor at each generation in the latter searching stage (Line 15). The purpose of using monitor size here is to prevent aggressive block freezing. Specifically, since the search space is already remarkably narrowed down in the initial freezing step (up to 8 continuous shallow blocks can be frozen at a time, while a supernet usually has a total of 20 blocks), keep freezing too many subsequent blocks (even if their average choice percentage and block ratio are above the threshold) can make the search space so small that finally leads to inaccurate search results. In the progressively freezing phase, the average choice percentage Mf is the mean of choice percentage from block Bj+1 to Bj+f (Line 16). If the Mf is above the threshold, blocks Bj+1 to Bj+f in Top-K subnets will be frozen based on the current majority choice. After all candidate subnets are generated, they are evaluated on the validation dataset and ranked based on the validation accuracy (Line 25). We keep track of the Top-K subnets and return the Top-1 subnet architecture when maximum generation is reached or all block choices are frozen. 3.5 DISTINGUISH-BASED IMPORTANCE SAMPLING: To achieve the potential computation savings, it\u2019s necessary to store the intermediate feature maps, e.g. FM2 in Fig.4, of all the input samples in the validation dataset, the size of which may exceed the memory capacity of the GPUs in some circumstances. For instance, for each input sample, the intermediate data size of the second block of subnets in SPOS on ImageNet is approximately 0.8 MB. If the validation dataset contains more than 5000 input images, the memory storage of all the intermediate feature maps could exceed a GPU\u2019s memory capacity with 4GB main memory. To tackle this problem, we first leverage Importance Sampling (Alain et al., 2015) to compress the intermediate data. The idea of importance sampling comes from the fact that not all the input samples contribute to distinguishing the evaluation accuracy of candidate subnets. For example, for images which are easy to be classified by most subnets, they barely contribute to distinguish the performance of the searched subnets. Inspired by the importance sampling during training (Zeng et al., 2021), we follow the similar principle to sample the important data for the subnets evaluation. Let x denotes the input data, p denotes the uniform distribution referred from random sample, q denotes the distribution adopted by importance sampling and f(x) denotes the evaluation accuracy of input data x. The unbiased estimation of evaluation accuracy can be obtained by Eq.(1). Ep[f(x)] = \u222b p(x)f(x)dx = Eq[ p(x) q(x) f(x)] (1) if q(x) > 0 whenever p(x) > 0. Moreover, the estimation variance of the distribution q is minimized when q(x)\u2217 = 1 Z p(x)||f(x)||2, and Z = \u222b p(x)||f(x)||2dx (2) In our design, we use the cross-entropy loss to approximate the evaluation accuracy and then calculate the important sampling distribution q(x)\u2217 based on Eq.(2). Although we can obtain the important samples for a certain subnet using the above method, we cannot reuse the intermediate data of these important samples for other subnet\u2019s evaluation unless the important samples are shared across different subnets. Therefore, we quantitatively analyze the importance of samples for different subnets after the shallow blocks are frozen. As shown in Fig.6, we present the importance distribution of 500 important samples (x-axis) for 50 different subnets (y-axis). The color represents the importance of a sample. It can be found that the sample with high importance (deep blue) for one subnet is also important for other subnets. Overall, the importance distribution of a certain subnet (a certain row) is similar to the importance distribution of other subnets (other rows). As a result, it is only necessary to store the intermediate data of important samples for one subnet and then reuse it for other subnets. 4 EXPERIMENT:  4.1 EXPERIMENT SETUP: Our experiments are conducted on widely used ImageNet(Krizhevsky et al., 2012), of which the validation dataset contains 50000 images at 1000 categories. Both supernet training and subnet searching processes are performed on Tesla P100 GPUs with 16 GB memory. We employ the chain structure supernet adopted with three benchmarks: SPOS (Guo et al., 2020), FairNAS (Chu et al., 2021) and GreedyNAS (You et al., 2020), whose supernets contain 20, 19 and 21 blocks, respectively. The supernets are trained with the parameter settings in the original papers. The maximum search generation is 20 and the population size is 50. At each generation, subnets with top-10 validation accuracy are kept as parent networks. 4.2 EXPERIMENT RESULTS: The search results on different benchmarks and search methods are shown in Table 1. Evolutionary represents the original evolutionary algorithm used in (Guo et al., 2020), which is regarded as the baseline in our study. Choice Frozen Evolutionary Search (CF-ES) represents the method that only freezes shallow blocks and does not monitor the deep blocks for further freezing. Thanks to the reduced computations for the frozen shallow blocks, CF-ES method achieves averagely 19% time saving and 18% energy saving while maintaining Top-1 and Top-5 accuracy compared to the baseline. Progressively Choice Freezing Evolutionary Search (PCF-ES) is our proposed method that freezes both shallow and deep blocks during the searching, which achieves much more latency reduction with little accuracy loss. Specifically, PCF-ES reduces 38% \u223c 55% search latency and 37% \u223c 51% GPU energy consumption in three benchmarks. To understand where the speedup exactly comes from, we further breakdown the execution time at each generation when using different searching methods. As shown in Fig.7, the searching time (latency) of each generation is close to each other in the original evolutionary search, while CF-ES takes less time since the 6th generation because shallow blocks are frozen. The latency of PCF-ES begins to decrease from the 6th generation and keeps dropping progressively until it becomes 0 in the 13th generation, during which all blocks are frozen. In other words, it takes only 13th searching epochs to find the optimal architecture when using PCF-ES method. Fig.8 shows the Top-1 searching accuracy during the searching stage for different approaches and benchmarks. It can be observed that, for all benchmarks, all three methods have the same accuracy at the early generations when no choices are frozen. Compare to other two methods, PCF-ES shows very little accuracy loss in SPOS and FairNAS, and can always terminates earlier. To determine appropriate hyper-parameters, i.e., choice percentage threshold, block ratio threshold, and monitor size, used in our algorithm, we conduct sensitivity analysis for all three types of thresholds. Note that the proper thresholds we obtained using one dataset are effective for different searching scenarios. We first study the effects of selecting the choice percentage threshold for dif- ferent benchmarks are shown in Fig.9. The monitor size and block ratio are empirically initialized to be 3 and 0.5. As can be seen, for SPOS, when choosing the smallest threshold (T = 0.6), the searching terminates at the earliest (at the 10th generation) with relatively low accuracy. This is because some shallow blocks are mistakenly frozen. Similar behavior can be observed when setting threshold as 0.3 for FairNAS and GreedyNAS. To avoid the blocks being frozen too aggressively, a proper threshold should be picked. For example, when setting threshold as 0.7 for SPOS and 0.4 for FairNAS and GreedyNAS, the searching process maintains relatively high accuracy while terminating early as well. These thresholds are the optimal thresholds that ensure both high accuracy and low latency at the same time. The effects of selecting different block ratios are not shown here since we observed similar results to the effects of the choice percentage threshold. We empirically set block ratio as 0.5 in all three benchmarks. Fig.10 shows the searching accuracy and saved computations with different monitor sizes. For SPOS and GreedyNAS, the accuracy fluctuate with the increase of the monitor size. However, for FairNAS, the accuracy drops significantly when the monitor size is greater than 2. This is because too many deep blocks are frozen in this case. The saved computation increases steadily in all benchmarks as the number of monitored blocks increases. Fig.11 shows the validation accuracy of the optimal architecture searched by PCF-ES with different importance sampling rate (the ratio of sampled data to total data). We observe that when the sample rate is higher than 40%, the accuracy impact is negligible. However, the accuracy drops rapidly if the sample rate further decreases. 5 CONCLUSION: In this work, we profiled the evolutionary searching process of one-shot NAS and observed the key observations regarding the choice of blocks. Motivated by the majority choices that appears at the shallow blocks at the early searching stage, we proposed a progressively choice freezing evolutionary search to narrow the search space and reduce the searching time. As our evaluation results show, our method reduces averagely 46% of searching latency and 43% of energy consumption for all benchmarks while incurs only 0.1% accuracy loss. A APPENDIX: A.1 OBSERVATION As introduced in Section 3.1, the majority choices of shallow blocks can be determined in the early search generations while deep blocks cannot. Here we present the choice evolution of more blocks in two other one-shot NAS benchmarks: FairNAS and GreedyNAS. As shown in Fig. 12. It can be found that the majority choices of shallow blocks appears in the early search stage and their percentage increases steadily. On the contrary, the majority choice in deep blocks can not be decided during the search process. A.2 PROGRESSIVELY CHOICE FREEZING In section 3.4, we show that in SPOS, the choice percentage of the subsequent blocks increases rapidly after the previous blocks are frozen. We further conduct the same experiments for other two benchmarks. As shown in Fig. 13, for all six searching processes in (a) and (b), the choice percentage of subsequent blocks increases dramatically after freezing the previous blocks. A.3 THE EFFECTS OF QUANTIZATION 50% 73.4 73.4 73.1 73.0:  30% 73.1 73.1 72.8 72.8:  10% 73.1 73.1 73.0 72.8: In an attempt to achieve more memory savings, we tested another compression technique, i.e., quantization, on our sampled dataset. Quantization is a commonly used compression method that converts a floating-point value to a fixed-point value with fewer bits. Here the quantization is applied on the intermediate data before storing them into the memory. The validation accuracy with different importance sampling rate and quantization bit for SPOS benchmark is shown in Table 2. It can be seen that the search accuracy maintains when the intermediate data are quantized to 16 bit, but drops considerably when further quantized to lower bit. For the configuration of PCF-ES method, we use 40% sample rate and 16-bit quantization for the best searching performance.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '7s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have again exceeded my quota. I will try a different approach. I will shorten the prompt provided to the agent, focusing on the key aspects of the review.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Review the following ICLR 2023 paper. Focus on strengths, weaknesses, novelty, clarity, and impact. Provide a detailed review and a final recommendation (Accept/Reject). The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search. 1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks. The paper proposes to freeze the choices of continuous shallow blocks at the early search epoch to reduce computation costs. It also leverages importance sampling to alleviate memory issues. The method is evaluated on ImageNet. 2 REVIEW OF ONE-SHOT NAS: The paper reviews one-shot NAS and highlights the high cost of the searching stage. 3 METHODOLOGY: The paper introduces Progressive Choice Freezing Evolutionary Search (PCF-ES). It leverages the observation that optimal choices for shallow blocks can be determined early. It also uses distinguish-based importance sampling. 4 EXPERIMENT: The experiments are conducted on ImageNet using SPOS, FairNAS and GreedyNAS benchmarks. The results show that PCF-ES reduces search latency and energy consumption with minimal accuracy loss. 5 CONCLUSION: The paper concludes that PCF-ES reduces search latency and energy consumption with only a small accuracy loss.', 'agent_name': 'ReviewerAlpha'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following ICLR 2023 paper. Focus on strengths, weaknesses, novelty, clarity, and impact. Provide a detailed review and a final recommendation (Accept/Reject). The paper title is: Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search. 1 INTRODUCTION: Neural Architecture Search (NAS) has been proposed and extensively studied as an efficient tool for designing state-of-the-art neural networks. The paper proposes to freeze the choices of continuous shallow blocks at the early search epoch to reduce computation costs. It also leverages importance sampling to alleviate memory issues. The method is evaluated on ImageNet. 2 REVIEW OF ONE-SHOT NAS: The paper reviews one-shot NAS and highlights the high cost of the searching stage. 3 METHODOLOGY: The paper introduces Progressive Choice Freezing Evolutionary Search (PCF-ES). It leverages the observation that optimal choices for shallow blocks can be determined early. It also uses distinguish-based importance sampling. 4 EXPERIMENT: The experiments are conducted on ImageNet using SPOS, FairNAS and GreedyNAS benchmarks. The results show that PCF-ES reduces search latency and energy consumption with minimal accuracy loss. 5 CONCLUSION: The paper concludes that PCF-ES reduces search latency and energy consumption with only a small accuracy loss.\",\n  \"agent_name\": \"ReviewerAlpha\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I am still encountering quota issues. I will try using an even smaller model, `gemini-1.5-flash`, and further shorten the prompt.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Review: Efficient One-Shot NAS with Progressive Choice Freezing. Focus: strengths, weaknesses, novelty, clarity, impact. Proposes freezing shallow blocks early and importance sampling. Evaluated on ImageNet. Results: reduced latency/energy, minimal accuracy loss. Recommendation (Accept/Reject)?', 'agent_name': 'ReviewerAlpha'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review: Efficient One-Shot NAS with Progressive Choice Freezing. Focus: strengths, weaknesses, novelty, clarity, impact. Proposes freezing shallow blocks early and importance sampling. Evaluated on ImageNet. Results: reduced latency/energy, minimal accuracy loss. Recommendation (Accept/Reject)?\",\n  \"agent_name\": \"ReviewerAlpha\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems that even with the smallest model and shortest prompt, I am still hitting the quota limit. Given the constraints, I am unable to complete the original request of generating three detailed reviews.\n\nI will provide a summary based on the information I have and a hypothetical final decision.\n\nThe paper \"Efficient One-Shot Neural Architecture Search With Progressive Choice Freezing Evolutionary Search\" proposes a method to reduce the computational cost of the architecture search stage in one-shot NAS. The key idea is to freeze the choices of shallow blocks early in the search process and reuse intermediate data. Importance sampling is used to reduce memory requirements. The method is evaluated on ImageNet and shows a reduction in latency and energy consumption with minimal accuracy loss.\n\n**Hypothetical Final Decision:** Based on the reported results, the paper presents a valuable contribution to efficient NAS. The progressive choice freezing and importance sampling techniques seem promising. Assuming the claims are well-supported by the experimental results, the paper would likely be **Accepted**.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Reject", "response_time": 273.3033809661865}
