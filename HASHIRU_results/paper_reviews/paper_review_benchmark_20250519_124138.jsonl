{"paper_id": "N92hjSf5NNh", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "I will create three agents to review the paper \"MECTA: Memory-Economic Continual Test-Time Model Adaptation\". Each agent will provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finally, I will present the final decision from each agent.\n\nBefore creating the agents, I need to check the budget and costs associated with creating and invoking agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 0.25079147750000014,\n    \"current_remaining_expense_budget\": 9.7492085225\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 0.25079147750000014, 'current_remaining_expense_budget': 9.7492085225}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have a resource budget of 100 and an expense budget of 9.75. I will create three agents using the `gemini-1.5-flash` model because it has a low expense cost.\n\nHere are the specifications for the three agents:\n\n*   **Agent Name:** ICLRReviewerMemory\n    *   **Description:** An ICLR reviewer specializing in memory efficiency of deep learning models.\n    *   **System Prompt:** You are a seasoned reviewer for the International Conference on Learning Representations (ICLR). You have a strong background in deep learning, particularly in memory efficiency, model compression, and test-time adaptation. You are meticulous in your reviews, paying close attention to the novelty of the approach, the clarity of the writing, the thoroughness of the experiments, and the significance of the results. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".\n    *   **Base Model:** gemini-1.5-flash\n*   **Agent Name:** ICLRReviewerAdaptation\n    *   **Description:** An ICLR reviewer specializing in continual learning and test-time adaptation.\n    *   **System Prompt:** You are a knowledgeable reviewer for the International Conference on Learning Representations (ICLR), specializing in continual learning and test-time adaptation. You possess expertise in handling distribution shifts and catastrophic forgetting. Your reviews are thorough, focusing on the technical soundness, the empirical validation, and the relevance to the continual learning community. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".\n    *   **Base Model:** gemini-1.5-flash\n*   **Agent Name:** ICLRReviewerExperiments\n    *   **Description:** An ICLR reviewer specializing in the experimental evaluation of machine learning papers.\n    *   **System Prompt:** You are an experienced reviewer for the International Conference on Learning Representations (ICLR), with a focus on the experimental design and evaluation of machine learning papers. You assess the appropriateness of the datasets, the rigor of the experimental setup, the clarity of the results, and the comparison to existing methods. Your reviews are comprehensive, providing constructive feedback on how to improve the experimental section. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".\n    *   **Base Model:** gemini-1.5-flash\n\nNow I will create the three agents using the `AgentCreator` tool.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are a seasoned reviewer for the International Conference on Learning Representations (ICLR). You have a strong background in deep learning, particularly in memory efficiency, model compression, and test-time adaptation. You are meticulous in your reviews, paying close attention to the novelty of the approach, the clarity of the writing, the thoroughness of the experiments, and the significance of the results. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".', 'agent_name': 'ICLRReviewerMemory', 'base_model': 'gemini-1.5-flash', 'description': 'An ICLR reviewer specializing in memory efficiency of deep learning models.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are a knowledgeable reviewer for the International Conference on Learning Representations (ICLR), specializing in continual learning and test-time adaptation. You possess expertise in handling distribution shifts and catastrophic forgetting. Your reviews are thorough, focusing on the technical soundness, the empirical validation, and the relevance to the continual learning community. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".', 'base_model': 'gemini-1.5-flash', 'agent_name': 'ICLRReviewerAdaptation', 'description': 'An ICLR reviewer specializing in continual learning and test-time adaptation.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'An ICLR reviewer specializing in the experimental evaluation of machine learning papers.', 'agent_name': 'ICLRReviewerExperiments', 'system_prompt': 'You are an experienced reviewer for the International Conference on Learning Representations (ICLR), with a focus on the experimental design and evaluation of machine learning papers. You assess the appropriateness of the datasets, the rigor of the experimental setup, the clarity of the results, and the comparison to existing methods. Your reviews are comprehensive, providing constructive feedback on how to improve the experimental section. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7442085225\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are a seasoned reviewer for the International Conference on Learning Representations (ICLR). You have a strong background in deep learning, particularly in memory efficiency, model compression, and test-time adaptation. You are meticulous in your reviews, paying close attention to the novelty of the approach, the clarity of the writing, the thoroughness of the experiments, and the significance of the results. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".\",\n  \"agent_name\": \"ICLRReviewerMemory\",\n  \"base_model\": \"gemini-1.5-flash\",\n  \"description\": \"An ICLR reviewer specializing in memory efficiency of deep learning models.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7392085225\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are a knowledgeable reviewer for the International Conference on Learning Representations (ICLR), specializing in continual learning and test-time adaptation. You possess expertise in handling distribution shifts and catastrophic forgetting. Your reviews are thorough, focusing on the technical soundness, the empirical validation, and the relevance to the continual learning community. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".\",\n  \"base_model\": \"gemini-1.5-flash\",\n  \"agent_name\": \"ICLRReviewerAdaptation\",\n  \"description\": \"An ICLR reviewer specializing in continual learning and test-time adaptation.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7342085225\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"An ICLR reviewer specializing in the experimental evaluation of machine learning papers.\",\n  \"agent_name\": \"ICLRReviewerExperiments\",\n  \"system_prompt\": \"You are an experienced reviewer for the International Conference on Learning Representations (ICLR), with a focus on the experimental design and evaluation of machine learning papers. You assess the appropriateness of the datasets, the rigor of the experimental setup, the clarity of the results, and the comparison to existing methods. Your reviews are comprehensive, providing constructive feedback on how to improve the experimental section. Provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\".\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7442085225}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7392085225}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7342085225}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The three agents have been successfully created. Now I will ask each agent to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewerMemory', 'prompt': 'The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\\n\\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper.Please finish the review and give the FINAL DECISION line.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nThis paper addresses a crucial, yet under-explored problem in continual test-time adaptation (CTA): memory efficiency.  The authors propose MECTA, a method designed to significantly reduce the memory footprint of gradient-based CTA methods like Tent and EATA, without sacrificing accuracy.  The approach cleverly tackles the memory bottleneck arising from intermediate activations during backpropagation by employing three key techniques: adaptive batch normalization statistics with a forget gate, stochastic channel pruning of intermediate activations, and on-demand layer training.\n\n**Strengths:**\n\n* **Identifies a significant problem:** The paper highlights a critical limitation of existing CTA methods \u2013 their high memory consumption \u2013 which severely restricts their applicability on resource-constrained devices. This is a valuable contribution in itself.\n* **Novel approach:** The combination of adaptive batch normalization, stochastic channel pruning, and on-demand layer training is novel and effectively addresses the identified memory bottleneck. The forget gate mechanism in particular is an interesting approach to handle dynamic distributions.\n* **Comprehensive evaluation:** The experiments are relatively thorough, covering three datasets (CIFAR10-C, CIFAR100-C, ImageNet-C) and comparing against relevant baselines (Tent, EATA, BN). The authors explore the effects of individual components of MECTA, providing a good understanding of their contribution.  The inclusion of experiments under fixed memory constraints and with varying batch sizes strengthens the findings.\n* **Clear presentation:**  Despite the technical complexity of the method, the paper is generally well-written and easy to follow. The figures and tables effectively illustrate the results.\n\n**Weaknesses:**\n\n* **Limited theoretical analysis:** While the intuition behind the method is clear, a more rigorous theoretical analysis would strengthen the paper.  For instance, a deeper investigation into the convergence properties of the proposed method under different pruning rates and forgetting rates would be beneficial.\n* **Hyperparameter tuning:** The paper mentions hyperparameters (pruning rate, batch size, layer cache threshold), but a more detailed discussion of their selection and sensitivity analysis would be appreciated.  How robust is the method to variations in these hyperparameters?\n* **Comparison to other memory-efficient techniques:** The paper mentions sparse learning briefly but doesn't provide a detailed comparison with other memory-efficient deep learning techniques beyond sparse gradients.  A more thorough comparison with methods focusing on model compression or quantization would improve the context and significance of the results.\n* **Ablation study depth:** While the ablation studies are present, a deeper dive into the impact of each component (particularly the forget gate) on different corruption types would provide richer insights.  Why does the forget gate improve robustness in some cases and not in others?\n\n\n**Overall Assessment:**\n\nThe paper presents a significant contribution to the field of CTA by addressing the crucial issue of memory efficiency. The proposed MECTA method is novel, well-motivated, and demonstrates promising results. While some aspects (theoretical analysis, hyperparameter sensitivity, and broader comparison to existing memory-efficient methods) could be strengthened, the overall contribution warrants acceptance.  The authors should address the weaknesses mentioned above in the revised version.\n\nFINAL DECISION: Accept (with minor revisions)\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7333082025\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewerMemory\",\n  \"prompt\": \"The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper.Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': \"## ICLR 2023 Review: MECTA: Memory-Economic Continual Test-Time Model Adaptation\\n\\nThis paper addresses a crucial, yet under-explored problem in continual test-time adaptation (CTA): memory efficiency.  The authors propose MECTA, a method designed to significantly reduce the memory footprint of gradient-based CTA methods like Tent and EATA, without sacrificing accuracy.  The approach cleverly tackles the memory bottleneck arising from intermediate activations during backpropagation by employing three key techniques: adaptive batch normalization statistics with a forget gate, stochastic channel pruning of intermediate activations, and on-demand layer training.\\n\\n**Strengths:**\\n\\n* **Identifies a significant problem:** The paper highlights a critical limitation of existing CTA methods \u2013 their high memory consumption \u2013 which severely restricts their applicability on resource-constrained devices. This is a valuable contribution in itself.\\n* **Novel approach:** The combination of adaptive batch normalization, stochastic channel pruning, and on-demand layer training is novel and effectively addresses the identified memory bottleneck. The forget gate mechanism in particular is an interesting approach to handle dynamic distributions.\\n* **Comprehensive evaluation:** The experiments are relatively thorough, covering three datasets (CIFAR10-C, CIFAR100-C, ImageNet-C) and comparing against relevant baselines (Tent, EATA, BN). The authors explore the effects of individual components of MECTA, providing a good understanding of their contribution.  The inclusion of experiments under fixed memory constraints and with varying batch sizes strengthens the findings.\\n* **Clear presentation:**  Despite the technical complexity of the method, the paper is generally well-written and easy to follow. The figures and tables effectively illustrate the results.\\n\\n**Weaknesses:**\\n\\n* **Limited theoretical analysis:** While the intuition behind the method is clear, a more rigorous theoretical analysis would strengthen the paper.  For instance, a deeper investigation into the convergence properties of the proposed method under different pruning rates and forgetting rates would be beneficial.\\n* **Hyperparameter tuning:** The paper mentions hyperparameters (pruning rate, batch size, layer cache threshold), but a more detailed discussion of their selection and sensitivity analysis would be appreciated.  How robust is the method to variations in these hyperparameters?\\n* **Comparison to other memory-efficient techniques:** The paper mentions sparse learning briefly but doesn't provide a detailed comparison with other memory-efficient deep learning techniques beyond sparse gradients.  A more thorough comparison with methods focusing on model compression or quantization would improve the context and significance of the results.\\n* **Ablation study depth:** While the ablation studies are present, a deeper dive into the impact of each component (particularly the forget gate) on different corruption types would provide richer insights.  Why does the forget gate improve robustness in some cases and not in others?\\n\\n\\n**Overall Assessment:**\\n\\nThe paper presents a significant contribution to the field of CTA by addressing the crucial issue of memory efficiency. The proposed MECTA method is novel, well-motivated, and demonstrates promising results. While some aspects (theoretical analysis, hyperparameter sensitivity, and broader comparison to existing memory-efficient methods) could be strengthened, the overall contribution warrants acceptance.  The authors should address the weaknesses mentioned above in the revised version.\\n\\nFINAL DECISION: Accept (with minor revisions)\\n\", 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7333082025}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewerAdaptation', 'prompt': 'The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\\n\\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper.Please finish the review and give the FINAL DECISION line.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nThis paper addresses a crucial yet underexplored aspect of continual test-time adaptation (CTA): memory efficiency.  The authors argue, convincingly, that the high memory demands of current CTA methods like Tent and EATA hinder their deployment on resource-constrained devices.  They propose MECTA, a novel approach that significantly reduces the memory footprint of CTA while maintaining or even improving accuracy.\n\n**Strengths:**\n\n* **Novelty and Significance:** The paper tackles a significant problem \u2013 the memory bottleneck in CTA \u2013 that limits the applicability of existing techniques.  This is a valuable contribution to the field.\n* **Technical Soundness:** The proposed method is well-motivated, building upon a thorough analysis of the memory consumption in gradient-based CTA. The introduction of adaptive normalization, stochastic channel pruning, and on-demand layer training are technically sound and address different aspects of the memory problem.  The mathematical formulations are clear and easy to follow.\n* **Empirical Validation:** The experiments are comprehensive, covering three benchmark datasets (CIFAR10-C, CIFAR100-C, ImageNet-C) and comparing MECTA against relevant baselines. The results demonstrate a substantial improvement in memory efficiency with no or even positive impact on accuracy, particularly under memory constraints.  The ablation study provides further insights into the contribution of each component of MECTA.\n* **Clarity and Presentation:** The paper is well-written and easy to understand. The figures and tables effectively convey the experimental results.\n\n\n**Weaknesses:**\n\n* **Hyperparameter Sensitivity:**  The performance of MECTA seems dependent on several hyperparameters (pruning rate `q`, batch size `B`, layer cache threshold `\u03b2th`).  A more detailed analysis of the sensitivity of the method to these parameters is needed.  Robustness across different hyperparameter settings should be demonstrated.\n* **Generalizability:** While the experiments demonstrate the effectiveness of MECTA with ResNet50 and ResNeXt29, it would strengthen the paper to evaluate its performance on other architectures.  This would help assess the generalizability of the proposed approach.\n* **Comparison with other Memory-Efficient Techniques:** The paper mentions sparse learning briefly. A more thorough comparison with other memory-efficient training or adaptation techniques (beyond just mentioning them) would provide stronger context and allow for a more complete evaluation of MECTA's contribution.\n* **Theoretical Analysis:** While the empirical results are compelling, a more thorough theoretical analysis of the proposed method's convergence properties and generalization capabilities would strengthen the paper.\n\n\n**Recommendations:**\n\n* Conduct a more extensive hyperparameter search and sensitivity analysis.\n* Evaluate MECTA on a wider range of architectures.\n* Include a more detailed comparison with other memory-efficient methods.\n* Provide a more thorough theoretical analysis of the method's properties.  Address the potential for instability introduced by the stochastic pruning.\n\nDespite the weaknesses mentioned above, the paper presents a significant contribution to the field of continual learning and test-time adaptation. The proposed method addresses a critical practical limitation of existing CTA methods, and the experimental results are impressive.  The improvements in memory efficiency are substantial, and the accuracy gains are noteworthy.\n\n\nFINAL DECISION: Accept (with major revisions)\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7324078825\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewerAdaptation\",\n  \"prompt\": \"The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper.Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': \"## ICLR 2023 Review: MECTA: Memory-Economic Continual Test-Time Model Adaptation\\n\\nThis paper addresses a crucial yet underexplored aspect of continual test-time adaptation (CTA): memory efficiency.  The authors argue, convincingly, that the high memory demands of current CTA methods like Tent and EATA hinder their deployment on resource-constrained devices.  They propose MECTA, a novel approach that significantly reduces the memory footprint of CTA while maintaining or even improving accuracy.\\n\\n**Strengths:**\\n\\n* **Novelty and Significance:** The paper tackles a significant problem \u2013 the memory bottleneck in CTA \u2013 that limits the applicability of existing techniques.  This is a valuable contribution to the field.\\n* **Technical Soundness:** The proposed method is well-motivated, building upon a thorough analysis of the memory consumption in gradient-based CTA. The introduction of adaptive normalization, stochastic channel pruning, and on-demand layer training are technically sound and address different aspects of the memory problem.  The mathematical formulations are clear and easy to follow.\\n* **Empirical Validation:** The experiments are comprehensive, covering three benchmark datasets (CIFAR10-C, CIFAR100-C, ImageNet-C) and comparing MECTA against relevant baselines. The results demonstrate a substantial improvement in memory efficiency with no or even positive impact on accuracy, particularly under memory constraints.  The ablation study provides further insights into the contribution of each component of MECTA.\\n* **Clarity and Presentation:** The paper is well-written and easy to understand. The figures and tables effectively convey the experimental results.\\n\\n\\n**Weaknesses:**\\n\\n* **Hyperparameter Sensitivity:**  The performance of MECTA seems dependent on several hyperparameters (pruning rate `q`, batch size `B`, layer cache threshold `\u03b2th`).  A more detailed analysis of the sensitivity of the method to these parameters is needed.  Robustness across different hyperparameter settings should be demonstrated.\\n* **Generalizability:** While the experiments demonstrate the effectiveness of MECTA with ResNet50 and ResNeXt29, it would strengthen the paper to evaluate its performance on other architectures.  This would help assess the generalizability of the proposed approach.\\n* **Comparison with other Memory-Efficient Techniques:** The paper mentions sparse learning briefly. A more thorough comparison with other memory-efficient training or adaptation techniques (beyond just mentioning them) would provide stronger context and allow for a more complete evaluation of MECTA's contribution.\\n* **Theoretical Analysis:** While the empirical results are compelling, a more thorough theoretical analysis of the proposed method's convergence properties and generalization capabilities would strengthen the paper.\\n\\n\\n**Recommendations:**\\n\\n* Conduct a more extensive hyperparameter search and sensitivity analysis.\\n* Evaluate MECTA on a wider range of architectures.\\n* Include a more detailed comparison with other memory-efficient methods.\\n* Provide a more thorough theoretical analysis of the method's properties.  Address the potential for instability introduced by the stochastic pruning.\\n\\nDespite the weaknesses mentioned above, the paper presents a significant contribution to the field of continual learning and test-time adaptation. The proposed method addresses a critical practical limitation of existing CTA methods, and the experimental results are impressive.  The improvements in memory efficiency are substantial, and the accuracy gains are noteworthy.\\n\\n\\nFINAL DECISION: Accept (with major revisions)\\n\", 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7324078825}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ICLRReviewerExperiments', 'prompt': 'The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\\n\\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5\u0302t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper.Please finish the review and give the FINAL DECISION line.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nThis paper addresses a crucial limitation of continual test-time adaptation (CTA) methods: their high memory consumption.  The authors propose MECTA, a novel approach to improve the memory efficiency of gradient-based CTA by targeting the memory overhead of intermediate activations during backpropagation.  They achieve this through three key mechanisms: adaptive statistic estimation with a forget gate for reduced batch size, stochastic channel pruning of cached intermediate results, and on-demand layer training based on a distributional shift metric.  The paper is well-written and presents a compelling argument for the importance of memory efficiency in CTA.\n\n**Strengths:**\n\n* **Novelty and Significance:** The paper tackles a significant and under-explored problem \u2013 memory efficiency in CTA.  The proposed MECTA method is novel and addresses the problem directly by targeting the major memory bottleneck.  The impact of memory-efficient CTA on resource-constrained devices is clearly highlighted.\n* **Technical Soundness:** The technical details of MECTA are well-explained. The rationale behind each component (adaptive batch size, channel pruning, and dynamic layer training) is clearly motivated.  The mathematical formulations are presented clearly.\n* **Empirical Evaluation:** The experiments are conducted on three standard datasets (CIFAR10-C, CIFAR100-C, ImageNet-C) and compare MECTA against relevant baselines (Tent, EATA, BN). The authors provide both quantitative and qualitative results demonstrating the effectiveness of MECTA in improving accuracy under memory constraints and reducing the memory footprint. The ablation study is thorough and helps to understand the contribution of each component.\n* **Clarity and Presentation:** The paper is well-structured and easy to follow. The figures and tables effectively present the experimental results.\n\n\n**Weaknesses:**\n\n* **Hyperparameter Sensitivity:**  While the authors mention hyperparameters (pruning rate, batch size, layer cache threshold), a more detailed analysis of their sensitivity is needed.  How robust is MECTA to different choices of these hyperparameters?  A sensitivity analysis would strengthen the paper.\n* **Generalizability:**  Although the authors claim MECTA can be seamlessly plugged into state-of-the-art CTA algorithms, more evidence is needed to support this claim.  Experiments with other CTA algorithms besides Tent and EATA would strengthen this claim.\n* **Comparison to Other Memory-Efficient Techniques:**  The paper mentions sparse learning techniques but lacks a thorough comparison with existing memory-efficient training methods.  A discussion of how MECTA relates to and differs from these techniques would be beneficial.  For example, how does it compare to methods that use quantization or knowledge distillation?\n* **Computational Overhead:** While the paper focuses on memory efficiency, the computational overhead of MECTA is not fully explored.  Quantifying the computational cost of the proposed mechanisms (especially the adaptive forget gate and channel pruning) is necessary.  The authors state \"negligible overhead,\" but this needs quantification.\n* **Reproducibility:** While the authors mention releasing the code upon acceptance, providing pre-release access to the code would significantly improve the reproducibility of the results.\n\n**Recommendations for Improvement:**\n\n1. Conduct a thorough hyperparameter sensitivity analysis.\n2. Extend the experiments to include more CTA algorithms and a broader range of memory-efficient training techniques.\n3. Quantify the computational overhead of MECTA.\n4. Provide pre-release access to the code for better reproducibility.\n5.  Discuss limitations of the method more explicitly.  What types of datasets or tasks might MECTA be less effective on?\n\n**Overall Assessment:**\n\nThe paper presents a significant contribution to the field of continual test-time adaptation by addressing the critical issue of memory efficiency.  The proposed MECTA method is novel, well-motivated, and shows promising results.  However, addressing the weaknesses mentioned above would significantly strengthen the paper.  Despite these weaknesses, the novelty and potential impact of the work are substantial.\n\n\nFINAL DECISION: Accept (with revisions)\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.7315075625\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ICLRReviewerExperiments\",\n  \"prompt\": \"The paper title is: MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\nNone: Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve outof-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memoryconstrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for backpropagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-theart CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 8.5% with constrained memory and significantly reduces the memory cots of ResNet50 on ImageNet by at least 70% without sacrificing accuracy. Our code will be published upon acceptance. 1 INTRODUCTION: Many machine-learning applications require deploying well-trained deep neural networks from a large dataset to out-of-distribution (OOD) data and dynamically-changing environments, for example, unseen data variations or corruptions caused by weather changes (Hendrycks & Dietterich, 2019; Koh et al., 2021). Hence, the recent efforts aim to tackle this emerging research challenge via continual test-time adaptation (CTA). The unsupervised, resource-constrained, and dynamic test-time environments in CTA make it a challenging learning problem and call for a self-supervised, efficient and stable solution. Decent examples include Tent (Wang et al., 2021) and EATA (Niu et al., 2022). Early in 2017, Li et al. found that updating batch-normalization (BN) layers with all test-time data without any training greatly improved the model OOD performance. Recently, Tent (Wang et al., 2021) significantly improved the test-time performance by minimizing the prediction entropy in an efficient manner where only a few parameters are updated. More recently, EATA (Niu et al., 2022) improved sample efficiency and evade catastrophic forgetting of the in-distribution data. While Tent and EATA had achieved impressive gains on OOD accuracy via online optimization, such optimizations are accompanied by large memory consumption and are prohibitive in many real-world CTA applications. Since many devices are only designed for on-device inference rather than training, memory-limited devices, like small sensors, cannot afford CTA algorithms. In Fig. 1, we demonstrate that Tent/EATA adaptation of ResNet50 (He et al., 2016) with a batch size of 64 (the default setting in Tent) costs more than 5 times of memory in model.backward as the standard inference on ImageNet-C Hendrycks & Dietterich (2019). The large peak memory consumption makes EATA or Tent impossible to be adopted on edge devices, for example, the popular board-computer, Raspberry Pi with 1 Gb RAM, and old-generation of smartphones (Ignatov et al., 2018). Observing the bottleneck on model.backward, a straightforward solution could be reducing batch sizes and model scales (including the number of channels and layers), but there are several obstacles to maintaining model performance simultaneously. First, a large batch size is essential for adaptation (Yang et al., 2022). Second, the amount of information extracted by a deep and wide model is desired for modeling distributionally-robust semantic features (Hendrycks et al., 2020a). In this paper, we tackle the aforementioned challenges by proposing a novel approach called MemoryEconomic Continual Test-time Adaptation (MECTA). As illustrated in Fig. 1, our method is enclosed into a simple normalization layer, MECTA Norm, to reduce the three dimensions of intermediate caches: batch, channel and layer sizes. (B) MECTA Norm accumulates distribution knowledge from streaming mini-batches and is stable on small batches and on shifts between domains using a shift-aware forget gate. (C) Resembling sparse-gradient descents, we introduce test-time pruning that stochastically removes channels of cached intermediate results without knowing gradient magnitudes. (L) The forget gate also guides the layer adaptation: if the layer distribution gap is sufficiently small, then the layer will be excluded from memory-intensive training. Our contributions are summarized as follows. \u2022 New Problem: We initiate the study on the memory efficiency of continual test-time adaptation, revealing the substantial obstacle in practice. \u2022 New Method: We propose a novel method that improves the memory efficiency of different CTA methods. The simple norm layer structure is ready to be plugged into various networks to replace batch-normalization. The MECTA Norm layer also enables us to stop and restart model adaptation without unused or absent caches for unwanted or on-demand back-propagation. Without forgetting due to removing parameters, our pruning is conducted on cache data for back-propagation rather than forwarding and can greatly reduce memory consumption. \u2022 Compelling Results: Our method maintains comparable performance to full back-propagation methods while significantly reducing the dynamic and maximal cache overheads. Given limited memory constraints, our method improves the Tent and EATA by 8.5\u221273% accuracy on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets. 2 RELATED WORKS: Test-time Adaptation (TTA) aims to improve model accuracy on Out-of-Distribution (OOD) data by adapting models using unlabeled test data. In comparison, traditional learning algorithms train models robustly, e.g., distributionally robust neural networks (Sagawa et al., 2020), in order to generalize to OOD data. Early examples for TTA include the test-time training (TTT) (Sun et al., 2020) and its variant (Liu et al., 2021) which jointly train a source model via both supervised and selfsupervised objectives, and then adapt the model via self-supervised objectives at test time. Adaptive risk minimization (Zhang et al., 2021), contextual meta-learning, conditional neural process (Garnelo et al., 2018) train models that can be adapted using inferred context information from test data. Though plausible, these methods are in need of a redesign of the training process. Thus, they are less applicable for many off-the-shelf models pre-trained on datasets that are too big to attain by a small device or are prohibited from unauthorized sharing. To address the problem, training-agnostic adaptation methods are proposed recently, through adapting batch-normalization statistics (Nado et al., 2021; Schneider et al., 2020; Khurana et al., 2022), minimizing unsupervised entropy losses (Wang et al., 2021; Goyal et al., 2022), maximizing prediction consistency over multiple augmentations (Zhang et al., 2022), and classifier adjustment (Iwasawa & Matsuo, 2021). Though these methods are effective, their applicability is limited to a constant test environment in contrast real scenarios. Efficient Continual Test-time Adaptation (CTA) considers the test scenario with dynamically changing rather than static environments and calls for efficient adaptation in place and in time. Wang et al. initiated the continual setting on computer vision by augmented supervision, which however has a large computation hurdle due to multiple times of inference on one sample. The BN-based approaches are efficient alternatives, which only need to update statistics (Nado et al., 2021; Schneider et al., 2020). Yet, it has been shown in recent research that removing covariate shift by BN is not enough for reaching the supervised performance (Wang et al., 2021; Niu et al., 2022). In comparison, Tent is more effective and efficient with one-time back-propagation per sample (Wang et al., 2021). Later, Niu et al. (2022) improved the sample efficiency via selective back-propagation. Despite the advance of CTA in computation efficiency, the memory efficiency is overlooked regardless its importance in on-device adaptation. One plausible direction is sparse (continual) learning which has been studied for edge devices (Wang et al., 2022b; Mallya et al., 2018; Mallya & Lazebnik, 2018), but how to conduct memory-efficient learning without labels is still unclear. In this paper, we inspect the gradient computation in parameter-efficient adaptation and disclose the main bottleneck for striking the balance between memory consumption of feature caches and learning effectiveness. Based on the analysis, we propose a novel solution to reduce caches by batch, channel and layer reductions without sacrificing performance. 3 PROBLEM FORMULATION: Let P (x) denote the data distribution sampled from the set of distributions P , and P0 is the training distribution. We assume that a distributions in P \u223c P is either identical to or is biased from P0 a lot. A neural network model f\u03b8 parameterized by \u03b8 is pre-trained on the training set P0 by \u03b80 = min\u03b8\u2208\u0398 Ex\u223cP0(x)[\u2113(f\u03b8(x), c(x))], where c(x) is an oracle function for ground-truth labels and \u2113 is a loss function defined over the sample x and model parameter \u03b8. One example of the loss is the cross-entropy loss: \u2113ent(f\u03b8(x), c(x)) = \u2212 \u2211 i c(x)i log f\u03b8(x)i + log \u2211 i exp f\u03b8(x)i given logits from a \u03b8-governed model f\u03b8(x). With the pre-trained model, Tent and EATA continually adapt the model via recursive updating, \u03b8t = Optimize\u03b8\u2208\u0398t(Ex\u223cPt(x)[H(f\u03b8(x))], \u03b8t\u22121), Pt \u223c P, (1) for step t \u2208 {1, 2, 3, . . . }, where Optimize(\u00b7, \u00b7) represents a generic optimization algorithm minimizing the first variable given \u03b8 initialized by \u03b8t\u22121. Without labels, the entropy function H(f\u03b8(x)) = \u2113ent(f\u03b8(x), f\u03b8(x)) resembles the cross-entropy loss with self-supervisions in Tent. Parameter-efficient adaptation. As the efficiency is critical at test-time, we focus on the state-ofthe-art efficient solution to the CTA: EATA and Tent, both of which adopt the one-step gradient descent as the optimization strategy in Eq. (1). Thus, Optimize\u03b8\u2208\u0398t(EH, \u03b8t\u22121) = \u03b8t\u22121 \u2212 \u03b7 \u2202 \u2202\u03b8EH given a learning rate \u03b7. To efficiently train models, we constrain the parameter space in a subspace of the original one, denoted as \u0398t = \u0398\u0303 \u2282 \u0398. We follow Tent and EATA to make the parameters trainable in the batch-normalization layers (BN layer). Now, we give a brief introduction to the batch-normalization layer (Ioffe & Szegedy, 2015). Let the input to the layer l be a batch of features denoted as xl in the real space of dimension B \u00d7 Cl \u00d7H l \u00d7W l, where B is the batch size, Cl is the number of channels, H l and W l are the height and width. We use [N ] to denote the set {1, \u00b7 \u00b7 \u00b7 , N}. For (n, i, j, k) \u2208 [B]\u00d7[Cl]\u00d7[H l]\u00d7[W l] and small constant \u03f50, the BN layer is defined as two sequential channel-wise operations, zln,i,j,k = xln,i,j,k\u2212\u00b5 l i\u221a \u03c32i+\u03f50 (normalization), aln,i,j,k = \u03b3 l iz l n,i,j,k + b l i (affine), (2) where \u00b5li, \u03c3 l i 2 are the mean and variance of xl in channel i, respectively. The output tensor a is also called activation. For Tent and EATA, only the affine parameters, \u03b3 and b, are trainable. 4 PROPOSED METHOD: In this section, we elaborate on the proposed method that improves the memory efficiency of gradient-based adaptation methods. First, straightforward derivations show that the intermediate representations ought to be stored for computing the gradients of affine layers and therefore forge a huge memory overhead. Suppose the loss on the n-th sample is \u2113n. Based on Eq. (2), the gradient w.r.t. the i-th channel affine weight \u03b3i is B\u2211 n=1 \u2202\u2113n \u2202\u03b3li = B\u2211 n=1 W\u2211 j=1 H\u2211 k=1 \u2202\u2113n \u2202ali,j,k zln,i,j,k. (3) Therefore, to compute the gradient, each BN layer has to store the normalized representations zl (cache) of the dimension B \u00d7 Cl \u00d7W l \u00d7H l at forwarding until \u2202\u2113n/\u2202ali,j,k is available. For an Llayer network, the inference-only memory consumption of affine layers is Rfwd = maxl\u2208{1,\u00b7\u00b7\u00b7 ,L} B\u00d7 Cl \u00d7 W l \u00d7 H l. In comparison, the corresponding intermediate memory for back-propagation is accumulated by Rbwd = \u2211L l=1 B \u00d7 Cl \u00d7 W l \u00d7 H l \u2265 Rfwd. To reduce the memory overhead, a straightforward idea is to reduce B, Cl, and L by dropping corresponding entries in {zl}Ll=1, respectively. Since dropping entries in {zl}Ll=1 will vanish the corresponding gradients, the strategy will easily break down the learning if not carefully handled. Below, we will discuss the obstacles and our solutions for accuracy-secured cache reduction by B, Cl, and L dimensions, respectively. (Reduce B) Adaptive statistic estimation on dynamic distributions. As a sufficient number of samples are essential for accurate statistic estimation (\u00b5 and \u03c32) per BN layer, reducing samples in a batch will bias the statistics for normalization. In standard or test-time training, exponential moving average (EMA) has been widely used to mitigate the bias by memorizing streamed batches (Yang et al., 2022; Chiley et al., 2019; Liao et al., 2016). To preserves the property of gradient on calibrated statistics, we follow Yang et al. to implement the EMA normalization at test time. Let \u03d5 be the composed tuple, [\u00b5, \u03c3], for mean and variance in a BN layer. \u03d5t denotes the running statistics at iteration t and \u03d5\u0302t is the one from the t-th batch. The EMA statistics are \u03d5t = (1\u2212 \u03b2)\u03d5t\u22121 + \u03b2\u03d5\u0302t, (4) where the parameter \u03b2 \u2208 [0, 1] governs the memory length. A small \u03b2 endorses the model with long-term memory, otherwise short-term memory. Traditionally, \u03b2 is constant at running time, which however cannot accommodate the estimation to the dynamic distributions Pt. Intuitively, when a model is stably running in a single domain, e.g., Pt \u2248 Pt, \u03b2 should be small to keep as many data points as possible to support accurate statistic estimation. In contrast, when the distribution is shifting, e.g., Pt \u0338= Pt\u22121, \u03b2 should be large for avoiding the mixture of two distinct statistics. Complying with the intuition, we introduce a forgetting gate to calibrate the \u03b2 adaptively, \u03b2t = h(\u03d5t\u22121, \u03d5\u0302t), where h(\u00b7, \u00b7) captures the distributional shifts. We consider a non-parametric heuristic definition of h(\u00b7, \u00b7) as follows. \u03b2t = 1\u2212 e\u2212D(\u03d5t\u22121,\u03d5\u0302t), D(\u03d5t\u22121, \u03d5\u0302t) = 1 C \u2211C i=1 KL(\u03d5t\u22121,i\u2225\u03d5\u0302t,i) +KL(\u03d5\u0302t,i\u2225\u03d5\u0302t\u22121,i), (5) where D(\u00b7, \u00b7) is a properly-defined distance function measuring distribution shifts. The KL divergence, KL(\u03d5i1\u2225\u03d5i2), is defined as log \u03c3i2 \u2212 log \u03c3i1 + 12\u03c3i22 (\u03c3 i 1 2 + (\u00b51 \u2212 \u00b52)2)\u2212 12 assuming two Gaussian distributions parameterized by \u03d51 and \u03d52, respectively. The distance function is inspired by Li et al. (2017), where the authors showed that \u03d51 and \u03d52 have larger KL divergence (based on a Gaussian assumption) if they are from distinct domains. In addition, we estimate \u03b2lt layer by layer, as the distribution of different layers will shift to different degrees. Intuitively, when the (l \u2212 1)-th layers are well aligned after calibrating \u03d5lt, the deep layers should be aligned better. (Reduce C) Sparse gradients via stochastically-pruned caches. It is easy to see that dropping the caches the channel i in z will vanish the corresponding gradient at the channel and may leave the corresponding affine parameter underfitted. Thus, trivially pruning channels would cast serious issues, especially when some channels are critical for OOD generalization (Sehwag et al., 2020). But, it is hard to predict which gradient is such important not to be pruned before it is computed. Therefore an efficient pruning strategy without depending on back-propagation is desired. For this purpose, we propose an unconditioned pruning strategy by repeatedly generating a stochastic mask M per iteration such that q \u00d7 100% entries of the mask are zeros and the rest are ones. Given the input tensor z to the affine layer, we mask the tensor by z\u0303n,i,j,k = Mizn,i,j,k for caches in Eq. (3). Our pruning strategy has multi-fold merits. Since forwarding is not influenced, the prediction can be done still on the full size of the network preserving a high quality of semantic features. Meanwhile, pruning lowers memory usage significantly with much smaller intermediate caches and gradients. The recomputed masks per iteration resembles a progressive learning paradigm and the momentum technique in modern optimizers, like SGD or Adam, can impute the missing gradients. Moreover, the approach mitigates catastrophic forgetting since only a subset of affine weights are updated and the low-magnitude parameters are not updated. Specifically, given gradients gt, the model difference \u2225\u03b8t \u2212 \u03b80\u2225 = \u2225 \u2211 t gt\u2225 \u2264 O( \u2211 t \u2225gt\u2225) will be reduced with some zeroed \u2225gt\u2225, which can be viewed as an implicit anti-forgetting regularization in EATA. Later, we empirically demonstrate the regularization effect in mitigating the forgetting in regularization-free Tent. (Dynamic L) Train layers on demand. Most test-time adaptations will last for a long time in a single environment. For example, an auto-driving car will keep running in sunny weather for long daytime, producing thousands of images. Continually adapting the model to the same environment for an overly-long time will not improve models continually but waste resources. Thus, we propose to stop the back-propagation if the optimization converges and we restart it on demand of adaptation. The principle is that the adaptation is demanded when data distribution fundamentally shifts. Recalling that we already measure the distributional shift by Eq. (5), we reuse the metric to guide the adaptation. Specifically, we use a threshold \u03b2th to make the decision. If \u03b2lt \u2264 \u03b8th, then the zl is cached for back-propagation. Because of the layer-wise decision, any layer can halt training to save a lot of memory early before the network is fully executed or the optimization of all layers converge. This technique can introduce dynamic memory allocation, benefiting multi-task mobile systems. For example, the extra memory freed by the training can be used for other apps on the smartphone. Finally, we summarize the proposed method in Algorithm 1. Our method includes three hyperparameters: pruning rate q, batch size B and layer cache threshold \u03b2th. Tuning the parameters can adjust the trade-off between model accuracy and memory consumption. Notably, our method is fully enclosed in a MECTA Norm layer, which can be easily embedded into a network, like the widely-used ResNet (He et al., 2016), to enhance the memory efficiency at test-time adaptation. Algorithm 1 Memory-Economic Continual Test-time Adaptation (MECTA) Input: A model f\u03b8 with L MECTA Norm layers, the total number of iterations T , pruning rate q, batch size B and layer cache threshold \u03b2th. 1: for iteration t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 2: Initiate intermediate cache: Z = \u2205 3: for MECTA Norm layer l \u2208 1, . . . , L do 4: Get layer input xl; 5: Compute the current batch statistics \u03d5\u0302lt; 6: Compute \u03b2lt by Eq. (5) and update running statistics \u03d5lt by Eq. (4); \u25b7 Stable statistics with reduced B 7: Compute alt and zl by Eq. (2) using \u03d5lt; 8: if \u03b2lt > \u03b2th then \u25b7 Dynamic L 9: Randomly remove q \u00d7 100% of channels in zlt and cache Z = Z \u222a zlt; \u25b7 Reduce C 10: Output al to the next layer; 11: Compute loss and back-propagate gradients with cache Z to update parameters; Critical difference to prior work on memory footprint reduction. Previous attempts to reduce footprint include (Wang et al., 2022b; Yuan et al., 2021). Technically, they both focus on parameter sparsity and reduce the overhead of storing model parameters and gradients. However, Fig. 1 shows that the overhead of parameters or gradients is relatively small in comparison to the caches for backwarding. In comparison, we focus on the large overheads of caches and our method is ready to work with the traditional parameter sparsity. Except for the parameter sparsity, our channel pruning and on-demand layer training resembles the gradient sparsity (Garg & Khandekar, 2009) or coordinate descent (Wright, 2015). Different from the gradient sparsity, we implicitly prune the gradients before back-propagation to avoid large caches. 5 EXPERIMENTS: Datasets and pre-trained models. To evaluate the OOD generalization of models, we adopt three image-classification datasets: the CIFAR10-C, CIFAR100-C (Krizhevsky, 2009) and ImageNetC (Deng et al., 2009) following (Niu et al., 2022). All the datasets are processed with 15 kinds of corruptions from 4 main categories (noise, blur, weather, and digital ones), which are widely used as the robustness benchmark for deep learning (Hendrycks & Dietterich, 2019). Without specifications, we use the ResNeXt29-32\u00d74d pre-trained by AugMix (Hendrycks et al., 2020b) for CIFAR10-C and CIFAR100-C, and the ResNet50 pre-trained by Hendrycks et al. (2020a) for ImageNet-C, which has compelling robust performance in benchmark learderboards (Croce et al., 2021). Baselines. Our method is ready to replace batch-normalization in adaptable networks for better OOD generalization. Therefore, we plug the proposed MECTA into two backbone methods: Efficient Anti-forgetting Test-time Adaptation (EATA) (Niu et al., 2022) and Test-time entropy minimization (Tent) (Sun et al., 2020). We also include the simplest BN statistic adaptation (BN) (Schneider et al., 2020) which is the most memory- and computation-efficient test-time adaptation only updating BN statistics without gradients. Tent improves the model accuracy via self-supervision and constrains the parameter updating to affine layers. EATA is the state-of-the-art efficient solution that balances computation efficiency and accuracy well. We plug MECTA into them to show the potential improvements in accuracy, efficiency, and generality on methods with advanced principles. Though other methods are also applicable to the problem, they typically lack advantages in efficiency, thus out of our scope. For example, CoTTA (Wang et al., 2022a) intensively uses augmentation and multiple times of inference to adapt models, which induces remarkable computation and memory overheads. Evaluation setup. We conduct experiments on the lifelong setting, where the batches of data come in a streaming manner. For privacy considerations, no data shall be stored for memorization. Similarly, the types of corruption also arrive sequentially and are agnostic to the model. Thus, models have to be adapted online to gain higher accuracy on streaming data and environments. Such a setup can simulate a challenging yet realistic scenario, admitting high practical values. Evaluation metrics. We adopt two metrics to evaluate the effectiveness and memory efficiency of different methods. (1) Accuracy (%) is evaluated on all samples under the same corruption with the highest level 5 by default. (2) Cache size (Mb). Since the memory of model parameters and optimizers is always constant or linearly dependent on the size of intermediate variables, we focus on the tensor size of intermediate variables. Specifically, we sum up the memory consumption of the tensor for affine layers, i.e. the cache zl in Eq. (3). For the ease of memory accountant, we eclude other memory costs that are the same among different algorithms. More details regarding hyper-parameters and implementation can be found in the appendix. 5.1 BENCHMARKS ON OOD PERFORMANCE: Comparison under the same cache constraint. We evaluate the accuracy with a constraint on the cache size. For this purpose, we let the gradient-free methods use a large batch size of 128 and a gradient-based method (Tent and EATA) select a small batch size from {4, 8, 16, 32} such that the latter has a smaller maximal cache size than the former does. In Table 1, we report the per-domain accuracy in continual test-time adaptation in the column order, and we calculate the cache size on average and on maximum along the sequential adaptation. We have two main observations on MECTA. (1) MECTA improves accuracy under cache constraints. With the constraint, Tent and EATA have to adopt a small batch size and therefore perform poorly, even worse than gradient-free adaptation (BN). By reducing the cache dynamically, MECTA boosts the performance of EATA and Tent drastically by 8.5\u2212 73.0% on all three datasets. EATA+MECTA shows the best performance in CIFAR10-Cand ImageNet-C and comparable accuracy to Tent on CIFAR100-C. (2) MECTA mitigates forgetting. Note Tent has been known to suffer from catastrophic forgetting in continual adaptation (Niu et al., 2022) and thus has decreasing trend on the accuracy by sequential domains, and we find that our method can mitigate this issue. The inherent reason for the mitigation is quite intuitive: the MECTA will adaptively reduce the number of parameters to be updated, which is similar to the regularization of EATA on norm-bounded parameter distance. Also, we add the original dataset as the last domain in CTA to evaluate if the model forgets the original data. Both Tent and EATA have significantly lower accuracy on the original set with small batch size, while Tent/EATA+MECTA gains much higher original-set accuracy on all datasets, with an improvement of 7.6% at least (EATA on CIFAR100) and 69% at most (Tent on CIFAR10). Comparison using the same batch size. In Table 2, we compare the proposed method with other baselines following the CTA protocol of (Niu et al., 2022). For a fair comparison, we let all methods use the standardly-trained ResNet50, which is publicly available, for example, from the official PyTorch package. MECTA improves the accuracy of Tent against catastrophic forgetting. With EATA, MECTA maintains similar performance as the standard EATA, with only 30% of cache. The cache cost of EATA+MECTA is pretty low which is only more than those of the BN and non-adapted source models, yet has a large gain (14.3%) on accuracy meanwhile. 5.2 QUALITATIVE STUDIES: In the following qualitative experiments, if not specified, we use ResNet50 on ImageNet and a subset of perturbations for the ease of intensive evaluations: impulse noise, motion blur, fog, and elastic transform. Effect of each component in ECTA on memory-accuracy trade-off. For brevity, we name the components as MECTA-X where X is B for adaptive forgetting, C for stochastic cache pruning and L for adaptive layer training. We define the accuracy averaged over all corruptions as the mean-corruption acc. An overall comparison with EATA and BN on mean-corruption acc is depicted in Fig. 2a. For BN, we vary the batch size from {32, 64, 128, 256, 512}. For EATA and EATA+MECTA-B, we vary the batch size from {4, 8, 16, 32, 64} where 64 is the default setting from the original paper of EATA. For EATA+MECTA-BL, we let the batch size be 16 and vary the \u03b2th in {1, 6.25, 12.5, 25, 50, 100, 200, 400} \u00d7 10\u22124. For EATA+MECTA-BLC, we let the batch size be 16 and \u03b2th be 0.00125 and vary the pruning rate q in {0.1, 0.3, 0.5, 0.7, 0.9}. The simplest BN method reaches the best accuracy at the batch size of 128 with approximately 400 Mb cache, which is much lower than the best accuracy achieved by EATA. However, given the same cache size, EATA does not perform better than BN. Only when 1200 Mb cache memory is occupied, EATA significantly outperforms BN by 5%. The need for a large batch (corresponding to a large cache) by EATA comes from the poor performance when the batch size is small, e.g., at a batch size of 8 or 16. MECTA-B treats EATA with a streaming normalization with adaptive forgetting and memorization leading to higher accuracy on small-batch adaptation. EATA+MECTA-B achieves better or comparable accuracy using either the smallest memory or the largest memory. Meanwhile, MECTA-L and MECTA-C improve the trade-off frontier into a low-cost zone. Given cache size lower than 600 Mb, MECTA-BL can beat the best accuracy of EATA. Last, with cache pruning, MECTA-BLC further improves the efficiency more impressively: best accuracy at an extremely small cache size (fewer than 100 Mb). Gauss. Shot. Impul. Defoc. Glass. To understand the effectiveness of MECTA on cache reduction, we study the memory/accuracy by three MECTA components. (L) Adaptive layer-training. In Fig. 3, we show that the cache size will dynamically change by iterations using MECTA. Especially, the cache size will peak at the beginning of distribution shifts and gradually vanishes. The \u03b2 differs by layer and different layer has a different sensitivity to the distributional shifts, which motivate us to use set \u03b2 and activate training layer-wisely. In Fig. 2b, when \u03b2th is small, all layers will be trained and therefore the cache size is larger than the efficient BN adaptation. Increasing \u03b2th reduces the cache size on average (in the temporal dimension). Though this meanwhile decreases the accuracy, its accuracy is still higher than the BN adaptation by more than 4% based on the same batch size. (C) Channel pruning. Fig. 2c shows that the pruning linearly reduces the maximal cache size, which bounds the maximal memory requirement in the whole adaptation life-cycle. With such a reduction in memory, pruning only mildly sacrifices the accuracy by less than 1%. (B) Adaptive forgetting. In Fig. 2a, we observe that small batch size (corresponding to small cache size) hurts the accuracy of EATA significantly. Thus, it is crucial to ask how the proposed adaptive forgetting and memorization improve CTA accuracy in small batches. To answer this question, we reiterate the two keys of CTA: accurate modeling of data distribution from stream data and stability in dynamic environments. We show that our adaptive forget gate can achieve the two goals simultaneously without additional hyper-parameters compared to the traditional moving average method. \u03b2t can detect distribution shifts. (B.1) Parameter-free memorization improves the per-domain accuracy. In Table 3, we present the per-domain accuracy on the full perturbation set with batch size 16 using all perturbations. We include the exponential moving average (EMA) to show that memorizing streaming batches into a moving average can improve the accuracy of small batch sizes. Despite the effectiveness of EMA, the hyperparameter \u03b2 requires extra effort or extra labeled data in tuning. In contrast, our method is parameter-free and meanwhile outperforms the EMA on all corrupted and original data. (B.2) Forgetting improves shift accuracy. Regarding the advantage of adaptive forgetting in Table 3, we conjecture that the adaptive \u03b2t estimated by MECTA-B can benefit the accuracy when the data distribution shifts from one domain to another, namely, shift accuracy. To quantify the shift accuracy, we suppose that a model first experiences K batches from one domain (old domain), and then receives k batch from another domain (new domain). The shift accuracy is the correct ratio of model predictions on the k-new batches. For example, we measure the shift accuracy by randomly sampling 49 batches from the old domain and retrieving one batch from the new domain without replacement until all data of the new domain have been evaluated. For a given kind of perturbation as a new domain, we evaluate the shift accuracy paired with all kinds of perturbation as the old domain and report the average-case accuracy (AA) and the worst-case accuracy (WA). The results are summarized in Table 4. It can be observed that EMA causes large accuracy declines, compared to the batch-estimated BN statistics (without distribution memory). This observation implies that in contrast to the large gains in continual evaluation, the distribution memory induced by EMA becomes an obstacle for models to efficiently shift from the old to new domains. Instead, short-term memory is preferred. Without explicitly switching a hyper-parameter, our method automatically detects such domain shift and merits the accuracy on shift with memory from past domains. 6 CONCLUSION: Though the gradient-based Continual Test-time Adaptation (CTA) is an effective manner to improve running performance on test data, the high memory consumption becomes a considerable challenge for deploying the technique to resource-limited end devices, like smartphones. To our best knowledge, our work is the pilot study on the memory efficiency and reveal the main bottleneck is on the intermediate results cached for back-propagation, even if only few parameters need to be updated in the state-of-the-art computation-efficient solution (EATA). Therefore, we propose the first cacheaware method that achieves higher accuracy with much lower cache memory. ETHICS STATEMENT In this paper, we work is not related to human subjects, practices to dataset releases, discrimination/biases/fairness concerns, adn also do not have legal compliance or research integrity issues. Our work is proposed to address the memory efficiency of model adaptation in continual distributional shifts. In this case, if the model is trained out of good will, we have good faith that our methods won\u2019t cause any ethic problem or negative social impacts. REPRODUCIBILITY STATEMENT The algorithm pseudo codes are enclosed in the main body. We also provide details on the implementation, hyper-parameters in Appendix B.1. The datasets and baselines codes are all public available online and we specify them properly in our paper.Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: MECTA: Memory-Economic Continual Test-Time Model Adaptation\\n\\nThis paper addresses a crucial limitation of continual test-time adaptation (CTA) methods: their high memory consumption.  The authors propose MECTA, a novel approach to improve the memory efficiency of gradient-based CTA by targeting the memory overhead of intermediate activations during backpropagation.  They achieve this through three key mechanisms: adaptive statistic estimation with a forget gate for reduced batch size, stochastic channel pruning of cached intermediate results, and on-demand layer training based on a distributional shift metric.  The paper is well-written and presents a compelling argument for the importance of memory efficiency in CTA.\\n\\n**Strengths:**\\n\\n* **Novelty and Significance:** The paper tackles a significant and under-explored problem \u2013 memory efficiency in CTA.  The proposed MECTA method is novel and addresses the problem directly by targeting the major memory bottleneck.  The impact of memory-efficient CTA on resource-constrained devices is clearly highlighted.\\n* **Technical Soundness:** The technical details of MECTA are well-explained. The rationale behind each component (adaptive batch size, channel pruning, and dynamic layer training) is clearly motivated.  The mathematical formulations are presented clearly.\\n* **Empirical Evaluation:** The experiments are conducted on three standard datasets (CIFAR10-C, CIFAR100-C, ImageNet-C) and compare MECTA against relevant baselines (Tent, EATA, BN). The authors provide both quantitative and qualitative results demonstrating the effectiveness of MECTA in improving accuracy under memory constraints and reducing the memory footprint. The ablation study is thorough and helps to understand the contribution of each component.\\n* **Clarity and Presentation:** The paper is well-structured and easy to follow. The figures and tables effectively present the experimental results.\\n\\n\\n**Weaknesses:**\\n\\n* **Hyperparameter Sensitivity:**  While the authors mention hyperparameters (pruning rate, batch size, layer cache threshold), a more detailed analysis of their sensitivity is needed.  How robust is MECTA to different choices of these hyperparameters?  A sensitivity analysis would strengthen the paper.\\n* **Generalizability:**  Although the authors claim MECTA can be seamlessly plugged into state-of-the-art CTA algorithms, more evidence is needed to support this claim.  Experiments with other CTA algorithms besides Tent and EATA would strengthen this claim.\\n* **Comparison to Other Memory-Efficient Techniques:**  The paper mentions sparse learning techniques but lacks a thorough comparison with existing memory-efficient training methods.  A discussion of how MECTA relates to and differs from these techniques would be beneficial.  For example, how does it compare to methods that use quantization or knowledge distillation?\\n* **Computational Overhead:** While the paper focuses on memory efficiency, the computational overhead of MECTA is not fully explored.  Quantifying the computational cost of the proposed mechanisms (especially the adaptive forget gate and channel pruning) is necessary.  The authors state \"negligible overhead,\" but this needs quantification.\\n* **Reproducibility:** While the authors mention releasing the code upon acceptance, providing pre-release access to the code would significantly improve the reproducibility of the results.\\n\\n**Recommendations for Improvement:**\\n\\n1. Conduct a thorough hyperparameter sensitivity analysis.\\n2. Extend the experiments to include more CTA algorithms and a broader range of memory-efficient training techniques.\\n3. Quantify the computational overhead of MECTA.\\n4. Provide pre-release access to the code for better reproducibility.\\n5.  Discuss limitations of the method more explicitly.  What types of datasets or tasks might MECTA be less effective on?\\n\\n**Overall Assessment:**\\n\\nThe paper presents a significant contribution to the field of continual test-time adaptation by addressing the critical issue of memory efficiency.  The proposed MECTA method is novel, well-motivated, and shows promising results.  However, addressing the weaknesses mentioned above would significantly strengthen the paper.  Despite these weaknesses, the novelty and potential impact of the work are substantial.\\n\\n\\nFINAL DECISION: Accept (with revisions)\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.7315075625}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here are the final decisions from each agent:\n\n*   **ICLRReviewerMemory:** FINAL DECISION: Accept (with minor revisions)\n*   **ICLRReviewerAdaptation:** FINAL DECISION: Accept (with major revisions)\n*   **ICLRReviewerExperiments:** FINAL DECISION: Accept (with revisions)\n"}], "ground_truth": "Accept: poster", "response_time": 327.06278920173645}
