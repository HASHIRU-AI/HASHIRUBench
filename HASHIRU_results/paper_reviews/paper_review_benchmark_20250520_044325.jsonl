{"paper_id": "4cOfD2qL6T", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Exploring perceptual straightness in learned visual representations\n\n1 INTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al., 2020). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper and provide a final decision. First, I need to check the available agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {\n    \"CriticalReviewer2\": {\n      \"description\": \"A highly critical reviewer focusing on theoretical soundness and methodological rigor.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"PracticalReviewer2\": {\n      \"description\": \"A practical reviewer focusing on the applicability and potential impact of the work.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"VisionaryReviewer\": {\n      \"description\": \"A reviewer interested in the big picture and potential impact of the work, looking for novel ideas and connections.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"CriticalReviewerFlash\": {\n      \"description\": \"A highly critical reviewer focusing on theoretical soundness and methodological rigor, using gemini-1.5-flash.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    },\n    \"PracticalReviewerFlash\": {\n      \"description\": \"A practical reviewer focusing on the applicability and potential impact of the work, using gemini-1.5-flash.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {'CriticalReviewer2': {'description': 'A highly critical reviewer focusing on theoretical soundness and methodological rigor.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'PracticalReviewer2': {'description': 'A practical reviewer focusing on the applicability and potential impact of the work.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'VisionaryReviewer': {'description': 'A reviewer interested in the big picture and potential impact of the work, looking for novel ideas and connections.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'CriticalReviewerFlash': {'description': 'A highly critical reviewer focusing on theoretical soundness and methodological rigor, using gemini-1.5-flash.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}, 'PracticalReviewerFlash': {'description': 'A practical reviewer focusing on the applicability and potential impact of the work, using gemini-1.5-flash.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I will use the following agents:\n\n*   **CriticalReviewer2:** A highly critical reviewer focusing on theoretical soundness and methodological rigor.\n*   **PracticalReviewer2:** A practical reviewer focusing on the applicability and potential impact of the work.\n*   **VisionaryReviewer:** A reviewer interested in the big picture and potential impact of the work, looking for novel ideas and connections.\n\nNow, I need to check the budget before invoking the agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 0.9895163825000003,\n    \"current_remaining_expense_budget\": 9.0104836175\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 0.9895163825000003, 'current_remaining_expense_budget': 9.0104836175}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have enough budget to invoke the three agents. Now I will invoke the agents to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\\n\\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\", 'agent_name': 'CriticalReviewer2'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '56s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\n\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\",\n  \"agent_name\": \"CriticalReviewer2\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '56s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems I have exceeded the quota for the `gemini-1.5-pro` model. I will try using the `gemini-1.5-flash` model instead, which should be faster and cheaper. I will create new agents using the flash model.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'A highly critical reviewer focusing on theoretical soundness and methodological rigor, using gemini-1.5-flash.', 'agent_name': 'CriticalReviewerFlash', 'system_prompt': 'You are a highly critical reviewer focusing on theoretical soundness and methodological rigor.', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-1.5-flash', 'system_prompt': 'You are a practical reviewer focusing on the applicability and potential impact of the work.', 'description': 'A practical reviewer focusing on the applicability and potential impact of the work, using gemini-1.5-flash.', 'agent_name': 'PracticalReviewerFlash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent CriticalReviewerFlash already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"A highly critical reviewer focusing on theoretical soundness and methodological rigor, using gemini-1.5-flash.\",\n  \"agent_name\": \"CriticalReviewerFlash\",\n  \"system_prompt\": \"You are a highly critical reviewer focusing on theoretical soundness and methodological rigor.\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent PracticalReviewerFlash already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-1.5-flash\",\n  \"system_prompt\": \"You are a practical reviewer focusing on the applicability and potential impact of the work.\",\n  \"description\": \"A practical reviewer focusing on the applicability and potential impact of the work, using gemini-1.5-flash.\",\n  \"agent_name\": \"PracticalReviewerFlash\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent CriticalReviewerFlash already exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent PracticalReviewerFlash already exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems I already created the agents with the flash model. I will try to invoke them directly.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'CriticalReviewerFlash', 'prompt': \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\\n\\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Exploring perceptual straightness in learned visual representations\n\nThis paper investigates the relationship between representational \"straightness\" (measured as low curvature in a trajectory through representation space) and various properties of learned visual representations in deep neural networks.  The authors analyze several architectures and training methods, including adversarial training and biologically-inspired models, across different vision tasks.\n\n**Strengths:**\n\n* **Novel Research Question:** The paper tackles a relatively unexplored area: the connection between representational straightness, adversarial robustness, and biological plausibility in deep learning models.  The concept of straightness as a desirable property for temporal robustness is intriguing.\n* **Comprehensive Evaluation:** The authors evaluate a wide range of models and training methods, including CNNs, Transformers, and biologically-inspired architectures, on multiple datasets and tasks.  This broad scope provides valuable insights into the generalizability of their findings.\n* **Clear Methodology:** The curvature calculation is clearly defined, although its reliance on frame differencing might merit further discussion (see Weaknesses). The experimental setup is well-described, allowing for potential reproducibility.\n\n**Weaknesses:**\n\n* **Theoretical Underpinnings:** The paper lacks a strong theoretical justification for why \"straightness\" should be a desirable property. The argument for its connection to temporal robustness and extrapolation is intuitive but needs more rigorous theoretical backing.  Why is a straight trajectory in representation space inherently better than a curved one?  What are the specific benefits, beyond mere intuition?  Are there potential downsides to enforcing straightness?\n* **Curvature Metric:** The choice of curvature as a measure of straightness is crucial.  The method relies heavily on frame differencing (v_t). This is sensitive to noise and might not capture the essence of \"perceptual straightness\" in a robust way.  Alternative metrics or a more in-depth justification for this specific choice are needed.  Furthermore, the authors should discuss the sensitivity of their results to different choices of parameters, such as the number of frames or the dimensionality reduction technique (PCA).  The limited discussion of PCA application in Appendix A.2.2 is insufficient.\n* **Lack of Causal Inference:**  The paper mainly demonstrates correlations between straightness and other properties (adversarial robustness, biological inspiration). However, it doesn't establish causality.  Does adversarial training *cause* straighter representations, or is there a confounding factor?  Further analysis, such as ablation studies or controlled experiments, is needed to strengthen the causal claims.\n* **Inconclusive Results:** While the paper presents many results, the conclusions are often tentative.  The relationship between straightness and performance on specific tasks is not consistently clear.  For instance, the improved performance of the \"straightened\" PredNet on a different task is not compelling evidence for the general utility of straightness.\n* **Missing Baseline Comparisons:** Several experiments lack appropriate baselines.  For example, the comparisons between different adversarial training techniques could be strengthened by including a baseline without adversarial training, and a baseline that uses other types of regularization or training techniques.\n\n\n**Overall Assessment:**\n\nThe paper raises an interesting question and performs a reasonably comprehensive evaluation. However, the theoretical foundation is weak, the chosen metric requires more justification, and the causal claims are not sufficiently supported. The inconclusive results and lack of strong baselines further weaken the paper.  While the broad scope is admirable, the lack of depth in both theoretical understanding and methodological rigor prevents it from reaching the standards of ICLR.\n\n\n**Recommendation:** Reject.  The authors should significantly strengthen the theoretical grounding, refine the curvature metric and its justification, perform more rigorous statistical analysis, and address the other weaknesses mentioned above before resubmission to a suitable venue.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.009179164999999\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"CriticalReviewerFlash\",\n  \"prompt\": \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\n\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: Exploring perceptual straightness in learned visual representations\\n\\nThis paper investigates the relationship between representational \"straightness\" (measured as low curvature in a trajectory through representation space) and various properties of learned visual representations in deep neural networks.  The authors analyze several architectures and training methods, including adversarial training and biologically-inspired models, across different vision tasks.\\n\\n**Strengths:**\\n\\n* **Novel Research Question:** The paper tackles a relatively unexplored area: the connection between representational straightness, adversarial robustness, and biological plausibility in deep learning models.  The concept of straightness as a desirable property for temporal robustness is intriguing.\\n* **Comprehensive Evaluation:** The authors evaluate a wide range of models and training methods, including CNNs, Transformers, and biologically-inspired architectures, on multiple datasets and tasks.  This broad scope provides valuable insights into the generalizability of their findings.\\n* **Clear Methodology:** The curvature calculation is clearly defined, although its reliance on frame differencing might merit further discussion (see Weaknesses). The experimental setup is well-described, allowing for potential reproducibility.\\n\\n**Weaknesses:**\\n\\n* **Theoretical Underpinnings:** The paper lacks a strong theoretical justification for why \"straightness\" should be a desirable property. The argument for its connection to temporal robustness and extrapolation is intuitive but needs more rigorous theoretical backing.  Why is a straight trajectory in representation space inherently better than a curved one?  What are the specific benefits, beyond mere intuition?  Are there potential downsides to enforcing straightness?\\n* **Curvature Metric:** The choice of curvature as a measure of straightness is crucial.  The method relies heavily on frame differencing (v_t). This is sensitive to noise and might not capture the essence of \"perceptual straightness\" in a robust way.  Alternative metrics or a more in-depth justification for this specific choice are needed.  Furthermore, the authors should discuss the sensitivity of their results to different choices of parameters, such as the number of frames or the dimensionality reduction technique (PCA).  The limited discussion of PCA application in Appendix A.2.2 is insufficient.\\n* **Lack of Causal Inference:**  The paper mainly demonstrates correlations between straightness and other properties (adversarial robustness, biological inspiration). However, it doesn\\'t establish causality.  Does adversarial training *cause* straighter representations, or is there a confounding factor?  Further analysis, such as ablation studies or controlled experiments, is needed to strengthen the causal claims.\\n* **Inconclusive Results:** While the paper presents many results, the conclusions are often tentative.  The relationship between straightness and performance on specific tasks is not consistently clear.  For instance, the improved performance of the \"straightened\" PredNet on a different task is not compelling evidence for the general utility of straightness.\\n* **Missing Baseline Comparisons:** Several experiments lack appropriate baselines.  For example, the comparisons between different adversarial training techniques could be strengthened by including a baseline without adversarial training, and a baseline that uses other types of regularization or training techniques.\\n\\n\\n**Overall Assessment:**\\n\\nThe paper raises an interesting question and performs a reasonably comprehensive evaluation. However, the theoretical foundation is weak, the chosen metric requires more justification, and the causal claims are not sufficiently supported. The inconclusive results and lack of strong baselines further weaken the paper.  While the broad scope is admirable, the lack of depth in both theoretical understanding and methodological rigor prevents it from reaching the standards of ICLR.\\n\\n\\n**Recommendation:** Reject.  The authors should significantly strengthen the theoretical grounding, refine the curvature metric and its justification, perform more rigorous statistical analysis, and address the other weaknesses mentioned above before resubmission to a suitable venue.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.009179164999999}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\\n\\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\", 'agent_name': 'PracticalReviewerFlash'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"Exploring perceptual straightness in learned visual representations\" for ICLR 2023\n\nThis paper investigates the relationship between representational straightness in learned visual representations and various factors like model architecture, training methods (adversarial training, self-supervised learning), and task.  The authors propose curvature as a metric to quantify straightness and apply it to a wide range of models across different vision tasks (image classification, segmentation, video prediction).\n\n**Strengths:**\n\n* **Novelty and Scope:** The paper tackles a relatively unexplored aspect of representation learning \u2013 the relationship between representational straightness and robustness/alignment with human perception. The empirical evaluation is extensive, covering various architectures (CNNs, Transformers), training methods, and tasks. This broad scope provides valuable insights.\n* **Clear Methodology:** The curvature metric is well-defined and the experimental setup is clearly explained. The authors provide sufficient detail on model selection and data used, facilitating reproducibility.\n* **Interesting Findings:** The results highlight that adversarial training, particularly with l2 norm perturbations, tends to lead to straighter representations in image classification models.  This contrasts with the findings for segmentation and video prediction, where straightness is not necessarily beneficial. The investigation of biologically-inspired models adds another interesting dimension, showing that while biologically-plausible mechanisms can reduce curvature, they don't always maintain it throughout the network.  The interplay between task and representational straightness is a valuable contribution.\n* **Potential Impact:**  The concept of representational straightness, if further explored, could offer a novel perspective on model robustness and generalization. A simple metric like curvature could become a valuable diagnostic tool for assessing the quality of learned representations beyond accuracy on benchmark datasets. This could lead to the development of more robust and human-like AI systems.\n\n**Weaknesses:**\n\n* **Lack of Theoretical Justification:** The paper lacks a strong theoretical foundation explaining *why* straighter representations might be advantageous.  While the authors suggest links to robustness and human perception, this connection needs deeper exploration.  A more rigorous theoretical framework would significantly strengthen the paper.\n* **Limited Causal Interpretation:** The correlations observed between adversarial training and straighter representations do not necessarily imply causation.  The authors acknowledge this, but further investigation to disentangle the effects of different training factors would be beneficial.\n* **Unclear Practical Implications:** While the potential for improved robustness is suggested, the paper doesn't clearly demonstrate any significant practical advantages of straighter representations in terms of downstream task performance.  The improvement in accuracy or robustness wasn't consistently observed across all tasks and models.  More direct evidence of improved performance would enhance the paper's impact.\n* **Reproducibility Concerns (Minor):** Although the authors mention the availability of models and code,  the lack of publicly available code and a detailed description of the experimental setup might hinder full reproducibility for some readers.\n\n**Overall Recommendation:**\n\nThe paper presents a valuable investigation into a novel aspect of visual representation learning. The extensive empirical study provides interesting insights, particularly the connection between adversarial training and representational straightness for certain tasks. However, the lack of theoretical grounding and clear demonstration of practical benefits limit the impact.  The paper would benefit from a more thorough investigation of the causal relationships and a more compelling demonstration of practical advantages.  Addressing these weaknesses will strengthen the work significantly.\n\n**Final Decision: Accept (with major revisions)**  The paper's novelty and broad scope warrant acceptance, but significant revisions are needed to address the weaknesses mentioned above before publication.  Specifically, the authors should strengthen the theoretical foundation, explore causal relationships more rigorously, and demonstrate clearer practical implications of their findings. The availability of fully reproducible code is also strongly recommended.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.008434395\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\n\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to a trained network with more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\",\n  \"agent_name\": \"PracticalReviewerFlash\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"Exploring perceptual straightness in learned visual representations\" for ICLR 2023\\n\\nThis paper investigates the relationship between representational straightness in learned visual representations and various factors like model architecture, training methods (adversarial training, self-supervised learning), and task.  The authors propose curvature as a metric to quantify straightness and apply it to a wide range of models across different vision tasks (image classification, segmentation, video prediction).\\n\\n**Strengths:**\\n\\n* **Novelty and Scope:** The paper tackles a relatively unexplored aspect of representation learning \u2013 the relationship between representational straightness and robustness/alignment with human perception. The empirical evaluation is extensive, covering various architectures (CNNs, Transformers), training methods, and tasks. This broad scope provides valuable insights.\\n* **Clear Methodology:** The curvature metric is well-defined and the experimental setup is clearly explained. The authors provide sufficient detail on model selection and data used, facilitating reproducibility.\\n* **Interesting Findings:** The results highlight that adversarial training, particularly with l2 norm perturbations, tends to lead to straighter representations in image classification models.  This contrasts with the findings for segmentation and video prediction, where straightness is not necessarily beneficial. The investigation of biologically-inspired models adds another interesting dimension, showing that while biologically-plausible mechanisms can reduce curvature, they don\\'t always maintain it throughout the network.  The interplay between task and representational straightness is a valuable contribution.\\n* **Potential Impact:**  The concept of representational straightness, if further explored, could offer a novel perspective on model robustness and generalization. A simple metric like curvature could become a valuable diagnostic tool for assessing the quality of learned representations beyond accuracy on benchmark datasets. This could lead to the development of more robust and human-like AI systems.\\n\\n**Weaknesses:**\\n\\n* **Lack of Theoretical Justification:** The paper lacks a strong theoretical foundation explaining *why* straighter representations might be advantageous.  While the authors suggest links to robustness and human perception, this connection needs deeper exploration.  A more rigorous theoretical framework would significantly strengthen the paper.\\n* **Limited Causal Interpretation:** The correlations observed between adversarial training and straighter representations do not necessarily imply causation.  The authors acknowledge this, but further investigation to disentangle the effects of different training factors would be beneficial.\\n* **Unclear Practical Implications:** While the potential for improved robustness is suggested, the paper doesn\\'t clearly demonstrate any significant practical advantages of straighter representations in terms of downstream task performance.  The improvement in accuracy or robustness wasn\\'t consistently observed across all tasks and models.  More direct evidence of improved performance would enhance the paper\\'s impact.\\n* **Reproducibility Concerns (Minor):** Although the authors mention the availability of models and code,  the lack of publicly available code and a detailed description of the experimental setup might hinder full reproducibility for some readers.\\n\\n**Overall Recommendation:**\\n\\nThe paper presents a valuable investigation into a novel aspect of visual representation learning. The extensive empirical study provides interesting insights, particularly the connection between adversarial training and representational straightness for certain tasks. However, the lack of theoretical grounding and clear demonstration of practical benefits limit the impact.  The paper would benefit from a more thorough investigation of the causal relationships and a more compelling demonstration of practical advantages.  Addressing these weaknesses will strengthen the work significantly.\\n\\n**Final Decision: Accept (with major revisions)**  The paper\\'s novelty and broad scope warrant acceptance, but significant revisions are needed to address the weaknesses mentioned above before publication.  Specifically, the authors should strengthen the theoretical foundation, explore causal relationships more rigorously, and demonstrate clearer practical implications of their findings. The availability of fully reproducible code is also strongly recommended.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.008434395}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'VisionaryReviewer', 'prompt': \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\\n\\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"VisionaryReviewer\",\n  \"prompt\": \"Please provide a detailed review of the paper 'Exploring perceptual straightness in learned visual representations' for ICLR 2023, including strengths and weaknesses, and recommend Accept/Reject. The paper is as follows:\n\nINTRODUCTION: Visual input from the natural world evolves over time, and one can think of that change over time as a trajectory in some representation space. For humans, this trajectory changes at different levels of processing, from input at the retina to brain regions such as V1 and finally to perception, as Figure 1 illustrates. We can ask about the shape of that trajectory and consider that there might be advantages to a straighter, less curved, trajectory. He\u0301naff et al. (2019) demonstrated that trajectories are straighter in human perceptual space than in pixel space. They suggested that a straighter representation may be useful for visual tasks that require extrapolation, such as predicting the future. Learning a useful visual representation is one of the major goals of computer vision. Properties like temporal stability, robustness to transformations, and task flexibility \u2013 all of which characterize human vision \u2013 are often desirable in computer vision representations. Yet, many existing computer vision models still fail to capture aspects of human vision, despite achieving high accuracy on visual tasks like recognition (Feather et al., 2019; He\u0301naff et al., 2019). In He\u0301naff et al. (2019) it was found that, while biologically-inspired V1-like transformations yield straighter representations compared to the input domain, popular computer vision models such as the original ImageNet-trained AlexNet (Krizhevsky et al., 2017) do not. In an effort to achieve these favorable human-vision properties for computer vision models, there has been much work dedicated to incorporating various aspects of human vision into deep neural networks. These include modifying network architectures to mimic that seen in the human visual system Huang & Rao (2011), incorporating filter banks matching the receptive field properties of visual neurons Dapello et al. (2020), as well as enforcing activation properties similar to that seen in visual cortex with properties such as sparsity Wen et al. (2016). Another promising avenue in this area has been in modifying training with adversarial examples. In directly targeting areas of vulnerability, adversarially robust networks show increased representational robustness, more closely aligning them with their human counterparts (Engstrom et al., 2019b). In this paper, we explore what makes some learned visual representations straighter than others. We evaluate a variety of network architectures, both biologically and non-biologically inspired, for representational straightness across layers; we then ask whether training for adversarial robustness in both CNN and transformer-based architectures may lead to the straighter representations generated by human vision. Because DNNs learn an early representation that differs from what is known about human vision, we also ask if hard-coding that early representation might lead to more straightening downstream. Overall, we find that most deep learning models do not employ straightened representations; however, with modified training schemes (notably adversarial training), many models are capable of relying on a straightened representations. This capability to utilize a straightened representation may be useful for learning more robust and stable image and video processing systems. 2 PREVIOUS WORK: Deep neural networks have been proposed as models of human visual processing, owing to their ability to predict neural response patterns (Yamins & DiCarlo, 2016; Rajalingham et al., 2015; Kell & McDermott, 2019). As such, there has been much effort to improve the alignment of deep networks to human vision by incorporating known aspects of the human visual system. Some of these include: simulating the multi-scale V1 receptive fields of early vision (Dapello et al., 2020), adding foveation using a texture-like representation in the periphery at a CNN\u2019s input stage (Deza & Konkle, 2020), and incorporating activation properties of visual neurons such as sparsity Olshausen & Field (1997); Wen et al. (2016). Predictive coding, often attributed to biological networks Huang & Rao (2011), has been incorporated into deep networks trained to perform tasks such as video frame prediction Lotter et al. (2016), using layers that propagate error-signals. The desire to evaluate the effectiveness of these techniques at creating models of the human visual system, motivated the creation of measures like BrainScore Schrimpf et al. (2020) that compare models to humans. One important area in understanding how humans and DNNs differ lies in their response to adversarial examples (Elsayed et al., 2018; Ilyas et al., 2019; Feather et al., 2022; Dapello et al., 2021). Adversarial examples, which modify images with changes that are imperceptible to humans, can cause a network to misclassify an image (Goodfellow et al., 2014; Szegedy et al., 2013). Adversarial training (Madry et al., 2017) improves the misclassfication problem and has been suggested to help networks learn visual representations that are more perceptually aligned with humans Engstrom et al. (2019b); Ilyas et al. (2019). If adversarial training leads to models that are more aligned with human perception, perhaps they also learn straight representations like humans do. Adversarial training schemes, however, are not biologically plausible, and recent work has identified mechanisms that are better supported by vision science (Dapello et al., 2020; Guo et al., 2022; Dapello et al., 2021). In the context of representation learning in computer vision, adversarially robust models have also been shown to do better at transfer learning than their non-adversarially robust counterparts (Davchev et al., 2019), and adversarially robust features can be used directly for tasks like image generation and in-painting (Santurkar et al., 2019). Adversarial training schemes have also been developed for tasks other than classification like semantic segmentation (Xu et al., 2021). In this paper, we build on work around adversarial robustness by evaluating if increasing this robustness leads to straightened representations like those found in human spatiotemporal processing. There have been various efforts to incorporate constraints that reduce curvature and increase straightness in temporal representation learning. Slow feature analysis (Berkes & Wiskott, 2005) encourages a learned representation to change slowly over time, but does not encourage straightness explicitly. Another early attempt to add a straightness constraint to representation learning (Goroshin et al., 2015) focused on video frame prediction for unsupervised learning. We follow in the same vein of this work, evaluating straightness as a metric for encouraging a robust, stable, representation when learning to solve tasks. 3 METHODS:  3.1 MEASURING STRAIGHTNESS AS LOW CURVATURE: Representational straightness can be evaluated as a reduction in curvature. For a temporal sequence, such as a video, curvature is defined as the angle between the vectors representing the difference between consecutive frames. Let x refer to a representation of a video of length T , with xt being a representation of one frame of a video at time step t. The representation may be at any stage of the processing pipeline, from a vector of raw input pixels from the video frame, to the activations of a network\u2019s hidden layer. Then, vt represents the difference between successive frames: vt = xt \u2212 xt\u22121 (1) v\u0302t = vt \u2225vt\u2225 (2) We can find the curvature at time t by finding the angle between successive v\u0302t, which we call ct: ct = arccos (v\u0302t \u00b7 v\u0302t+1) (3) The global curvature of a video sequence is then simply the mean angle over all time steps. Note there are T \u2212 1 time steps due to curvature being evaluated on frame differences: Global curvature := 1 T \u2212 1 T\u22121\u2211 t=0 ct (4) This is the formulation proposed by He\u0301naff et al. (2019). One can compute this global curvature for any representation of a video sequence over time, either on the vector of pixels (likely not very straight), or one can apply it to a representation of that video, e.g. at any layer of a neural network model. In this paper, we often report change in curvature with respect to the pixel-value input to show if a model\u2019s representation is more straight at later stages of processing. Change in curvature is the representational curvature minus the pixel curvature. Note that curvature is distinct from simple cosine similarity in that curvature is calculated on frame differences (vt), whereas cosine similarity depends on the angle between the frame vectors themselves (xt). Curvature can be thought of as a first-order variant of cosine similarity (see Figure 2 and Section A.1). For a complete description of how curvature was determined for each model, see Section A.2.2. 4 CURVATURE AND ROBUSTNESS IN CONVOLUTIONAL OBJECT RECOGNITION MODELS: We tested a variety of models for output curvature, to investigate the relationship between model type and curvature of the output layer. As shown in Figure 3, we found non-adversarially trained image recognition models to have the highest output curvature. All adversarially trained models have lower curvature than their non-adversarially trained counterparts, as well as overall, with the majority reducing output curvature below that of the input pixels. Self-supervised DINO (Caron et al., 2021) models have similar output curvature values to their supervised counterparts \u2013 despite DINO models having been shown to have more semantically meaningful feature correspondences. 4.1 ADVERSARIAL ATTACK TYPE, STRENGTH, AND CURVATURE: Given the increased straightness seen in adversarially robust image recognition / object detection models, we investigated the relationship between the type and strength of adversarial attack and the resulting curvature of the model\u2019s output. To evaluate the effect of these attacks on curvature, we compare a set of ResNet50 networks (He et al., 2016), trained on CIFAR-10 (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), and Restricted ImageNet (a subset of ImageNet (Engstrom et al., 2019b; Ilyas et al., 2019)), with and without adversarial training and measure the output curvature (Figure -5). The adversarially trained models are trained using projected gradient descent with l2 or l\u221e norms at different perturbation levels (Madry et al., 2017; Engstrom et al., 2019a). We use ResNet50 adversarially trained with projected gradient descent because that model type has been shown to be more aligned with human perception (Engstrom et al., 2019b; Feather et al., 2022; Harrington & Deza, 2022). During projected gradient descent, adversarial examples are created to train the model to correctly classify them. The adversarial example is created by adding noise to an image. The amount of noise is bounded by a norm (usually l2 or l\u221e set to a level \u03f5). We include some models trained on CIFAR-10 to understand how perturbation level interacts with straightness (CIFAR-10 models are faster to train, so more models are available). We find that, as predicted, output curvature is highest for non-adversarially trained networks. l\u221e attacks decrease output curvature, with larger values of \u03f5 leading to decreased curvature. l2-attacked networks have the straightest output curvatures, however strength of attack does not greatly affect the output curvature.The l2 norm models may achieve straighter representations than the l\u221e models because they allow for greater \u03f5 in training. Among l2 models, higher \u03f5 values may need to be tested to see a difference in curvature among l2 norm trained models. Overall, we show that robustness to larger perturbations leads to straighter representations. Note that while we show that curvature is predictive of adversarial robustness, this does not translate to improved accuracy on the test set (See Section A.4). We argue however, that while curvature is not predictive of test accuracy directly, adversarial robustness is associated with many other model benefits, including representational stability and alignment with human perception. 5 CURVATURE IN VISUAL TRANSFORMER MODELS FOR OBJECT RECOGNITION: Vision transformers (ViT) have very different architectures than CNNs. They replace convolutional elements with self-attention on visual tokens and achieve state of the art performance on a variety of visual tasks. In addition, these networks are more adversarially robust than standard-trained CNNs, which is attributed to their higher-level and more generalizable features (Shao et al., 2021). Despite this, we find that standard-trained ViT (ViT Base, patch 16) does not reduce curvature on the natural video sequences tested (Fig 3). Self-supervised training of ViT (base, patch 16) using the DINO method (Caron et al., 2021) also results in a more curved representation than the input space. However, this DINO trained ViT has a more straight representation than the baseline ViT, despite the two models sharing the same architecture. This suggests that self-supervised training may be better than supervised training for obtaining straight representations. No transformer model except for CrossViT daggar (Chen et al., 2021) reduced curvature in any layer compared to the pixel input (Fig 6). The structure of CrossViT models is unique in that they split into multiple paths, and the reduction in curvature we found interestingly only happens in one pathway of the model and does not persist after the two paths recombine. A possible explanation for this is that the CrossViT is able to straighten in the part of the model that splits because that pathway utilizes a multi-scale representation, a known aspect of human vision representations such as V1 that which we show in 8 to induce straightening. 6 CURVATURE AND ROBUSTNESS IN IMAGE SEGMENTATION MODELS: Output straightness over time should be a favorable, stabilizing property for object detection models, where objects often stay continuously in-frame over many frames of a video. For image segmentation models on the other hand, predictions lie close to low-level pixel space and change on a frame-by-frame basis. Therefore, for the task of predicting the dynamically changing segmentation map of an input movie, straightness of the output representation would not be desirable (assuming the input movie contains motion). To test this prediction, we measured output curvature for a family of 12 image segmentation models from Xu et al. (2021), consisting of two different architectures, trained on two different datasets, with a non-adversarial as well as two different adversarial training schemes. Unlike object detection models, where adversarial training greatly reduces output curvature, adversarial training in segmentation models only slightly reduces curvature compared to non-robust segmentation models (Figure 3), with all adversarially trained segmentation models displaying output curvature higher than the input pixel curvature. Investigating curvature change over model layers (Figure 7), reveals that this trend is consistent throughout all layers of the model. This highlights the importance of task on straightness, and supports the idea that output curvature reduction is not simply the result of any adversarial training scheme \u2013 only when temporal stability is beneficial does adversarial training promote straightness. 7 CURVATURE IN VIDEO FRAME PREDICTION MODELS: Another DNN task for which output predictions are low level, is next-frame prediction for video sequences. As in image segmentation models, good predictions are expected to vary dynamically frame-to-frame, and representational straightness at and near the output layer is not a favorable quality in response to videos containing motion. Rather, for a frame-prediction network, a favorable quality would be equal curvature at the input and output. This is because for a well-performing model, the output prediction over many frames is the same as the input video; two identical videos have identical curvature. To evaluate this, we test the layer-evolution of curvature for three variants of PredNet (Lotter et al., 2016), a network inspired by predictive coding, which is trained to predict the next frame in a video sequence (Figure 8). Indeed, for all pretrained variants of PredNet tested, the output layer maintains a very similar curvature value to that of its input frame, with final output curvature closer than all other models to that of the input (Fig 3). For all pretrained variants, the model curvatures strongly increase in the representation of the first model block, then re-straighten the representation throughout the rest of the network before returning to the original pixel curvature. We also investigated a variant of PredNet which had been fine-tuned to predict multiple frames in the future (PredNet Extrap KITTI), which we find only slightly reduces model curvature, with a similar trajectory over layers. For the variant of PredNet in which predictive error for all layers is incorporated into the loss during training (PredNet Lall KITTI), this increase is much less pronounced, pointing to a more temporally-stable representation for this network globally. This is in agreement with previous findings, showing that while this globally-trained PredNet slightly under-performs at the original frameprediction task, when using its network backbone to perform a classification task, this least-curved PredNet model is the best performer. In other words, the network with the most straight representation performs best in the context of a different task, where temporal stability is favorable. We also tested the utility of straightness as a training signal, adding a curvature constraint to the first (R3) and second (R2) layers of PredNet during training to create \u201dstraight\u201d PredNet models. We evaluate how the constraint affects the model\u2019s internal representation and the final frame prediction. For both models, curvature is greatly reduced in R2 and R3 compared to the input curvature (Fig 8). Because of PredNet\u2019s top-down and bottom-up feedback structure, the constraint on layer R3 does not affect any other layers of the model. Although we find that it is possible to train PredNet to straighten and still get next-frame predictions that has lower mean square error (MSE) than the copying the previous frame, we are not able to achieve equivalent or better prediction MSE with the straight models compared to the original on KITTI (Geiger et al., 2012) (original L0 PredNet:0.00687 MSE, straight R2 PredNet: 0.00976 MSE, compared to previous frame: 0.02122 MSE). See Fig 13 for example predictions. Again, while these straightened PredNet variants do not achieve state of the art, we argue that these models may display other advantages such as robustness and similarity to human representations. 8 CURVATURE IN BIOLOGICALLY INSPIRED MODELS: We investigated straightness for a variety of both parametric (learned) and non-parametric biologically-inspired models. Given straightness is thought to increase over progressively deeper layers of visual processing, we align these networks along the visual processing areas they are most closely matched to (Figure 9). Some models match to visual areas explicitly in their architecture like the non-parametric Henaffbio (He\u0301naff et al., 2021), which is a two-stage model based on centersurround filters follows by oriented Gabor filters, and Brucenet (Brown et al., 2021), which is based on summary statistics of a steerable pyramid. This explicit match to biological vision also holds for the early layers VOneNetCornets (Dapello et al., 2020), which combines a V1 filter front-end and Cornets as a backbone (Cornets being a shallow neural network that has layers matched to regions of the human brain (Kubilius et al., 2019)). For the adversarially trained Visual Transformer network CrossViTRotAdv (Berrios & Deza, 2022), these layers are those best matched by BrainScore (a benchmark for how similar a model is to the human brain at object recognition Schrimpf et al. (2020)), for V4 and IT layers (Schrimpf et al. (2020)). For all biologically-inspired models except for VOneNetCornets, curvature progressively decreases through deeper network layers. For VOneNetCornets, curvature decreases up until the V1 layer in which a noise term capturing neural stochasticity is added; curvature then strongly increases, far above the pixel-curvature baseline. To determine if this increase in curvature was due to the added noise, we tested the same model with the noise term set to zero. While this reduced the downstream curvature after the V1 layer, this change did not eliminate the curvature increase present right after the V1 layer. The increase in curvature for VOneNet at later layers suggest that making the front-end of a deep network more like biologically-inspired models does not suffice to get a straightened representation downstream in a deep network. This is interesting because VOneNet is is reported to be more adversarially robust to white-box attacks than a standard trained CNN. This suggests that adversarial training, not the property of adversarial robustness itself, leads to straightened representations in deep neural networks. Our finding supports Dapello et al. (2021) who found that the neural population geometry of adversarial trained models was noticeably different from VOneNet and other models trained with neural stochasticity mechanisms. However, it is puzzling that adversarial training, which is biologically implausible, would lead to straighter representations than biologically inspired mechanisms. More constraints or modifications may be needed to achieve straight representations with biologically plausible methods. 9 DISCUSSION: We show that one can use the change in model representational curvature as a simple and computationally cheap metric for evaluating both image and video models across a variety of tasks. For a variety of image task models, we show that training with strong white-box adversarial attacks reduces output curvature. However, this is only true for tasks like image classification for which temporal stability is favorable, and not for semantic segmentation and video frame prediction. Moreover, we show that self-supervised training does not produce straightened representations. In evaluating both CNN and transformer-base architectures we show that curvature reduction is mainly a product of task and training procedure, not deep neural network architecture alone. In evaluating curvature over layers in biologically inspired models, we show that biologically inspired mechanisms work to reduce curvature in a model\u2019s representation, even more so than adversarial training. However, the simple addition of non-parametric biologically inspired filtering mechanisms at the input of a model are insufficient to maintain output curvature. In summary, we find that models with more known aspects of human vision like robustness have a greater ability to straighten natural videos. Our results suggests that the ability to straighten might be a useful and easily computed measure of how similar visual representations are to humans. These results identify representational curvature as a common thread between biologically inspired and adversarially robust models, and highlight the benefits and limitations of these techniques in creating temporally-stable representations. 10 REPRODUCIBILITY: To reproduce our results, links to all the models analyzed can be found in the supplemental material in the Network Comparison Spreadsheet. Sources for the stimuli are in Sec A.2.2 and in the code base linked for the HenaffBio model in Network Comparison Spreadsheet. Straightness can be determined by calculating curvature using Sec 3 and A.1. The models, stimuli, and curvature function together should be sufficient to reproduce our results. A APPENDIX:  A.1 CURVATURE VS COSINE SIMILARITY: cosine similarity (vectors) := cos (\u03b8) = A \u00b7B \u2225A\u2225\u2225B\u2225 (5) cosine similarity (video frames) := cos (\u03b8) = xt \u00b7 xt+1 \u2225xt\u2225\u2225xt+1\u2225 (6) curvature := ct = arccos (v\u0302t \u00b7 v\u0302t+1) (7) cosine curvature := v\u0302t \u00b7 v\u0302t+1 = vt \u00b7 vt+1 \u2225vt\u2225\u2225vt+1\u2225 = cos (ct) (8) A.2 MODELS:  A.2.1 MODEL SOURCES: All deep neural networks we analyzed were pretrained. Exact links to code and weights can be found in the supplemental material in the Network Comparison Spreadsheet. The standard ImageNettrained ResNet (He et al., 2016) model was downloaded from PyTorch\u2019s model zoo (Paszke et al., 2019). Adversarially robust ResNet models were all downloaded from (Engstrom et al., 2019a). The adversarially robust ResNets were trained using projected gradient descent. All ViT (Dosovitskiy et al., 2020) and standard trained CrossViT daggar (Chen et al., 2021) models were downloaded from the timm library (Wightman, 2019). All CrossViT daggar adversarially robust and rotationally invariant models were downloaded from the repository of Berrios & Deza (2022). The adversarially robust CrossViTs were trained with fast gradient sign method as stated in Berrios & Deza (2022). DINO models were downloaded from the DINO repository (Caron et al., 2021), while PredNet models were downloaded from Lotter et al. (2016). A.2.2 MODEL ANALYSIS PROCEDURE: We showed each model the same 12 natural videos that were used in the psychophysics experiments of (He\u0301naff et al., 2019). The videos were taken from the Chicago Motion Database (at University of Chicago, 2022), the film \u2018Dogville\u2019, Lions Gate Entertainment (2003), and LIVE Video Quality Database (Seshadrinathan et al., 2010b;a). The videos were grayscale, consisting of 11 frames each of 512 \u00d7 512 pixels, capturing natural motion such as rippling ocean water or a person walking through a crowded street. We resized the video frames to be 224 \u00d7 224 for all deep networks and 256\u00d7 256 for bio-models that use steerable pyramids. One limitation of this work is that we did not evaluate models on larger video datasets, but we wanted to use psychophysically validated stimuli for our analyses. For each model, we recorded its activations at intermediate and final layers for each video. We then found the global curvature for each stage of the model using equation 4 where we used the flattened model activations as the input xt to the curvature procedure. We compared the global curvature at each layer of the model to the curvature of the video in pixel space. Models that straighten are defined as models that have a lower global curvature at deeper layers. When comparing the curvature of different model layers, we chose not to reduce the dimensionality of each layer activation to be the same across stages. Although principle components analysis (PCA) was sometimes used in He\u0301naff et al. (2019) when expressing curvature, they did not use it in their analysis of deep networks. Furthermore, while an architecture\u2019s inherent dimensionality is likely relevant to a representation\u2019s curvature, we preferred not to introduce any additional transformations that would influence the measured curvature. We also found that performing PCA did not greatly affect trends like adversarial training leading to more straight representations See Figure 10 for examples of curvature for different numbers of principle components on two ResNet50 models. A.2.3 COMPUTE: Our methods do not require large compute. All individual model analyses can be run on CPU. We used a single GPU to speed up getting the features activations at each layer to the order of minutes per model. Standard-Trained ResNet50 Adversarially-Trained ResNet50:  A.2.4 EFFECT OF PCA ON MODEL CURVATURE:  A.3 FINAL STAGE CURVATURES: The final stage of a network for image classification models is the last fully connected layer. We see similar trends in curvature across model families as in Fig 3. We chose to report the the curvature on the layer just before classification because this is the layer that would be most commonly used as the representational backbone for other tasks. A.4 ADVERSARIAL ACCURACY AND CURVATURE:  A.5 PREDNET PREDICTIONS:  A.6 NEGATIVE SOCIETAL IMPACTS: We believe there are few negative societal impacts of this paper. Our work was exploratory and did not introduce any new models. However, we note that the development of machine vision systems, which can operate at or above the level of performance of humans at certain tasks, may lead to negative societal impacts such as the loss of jobs and industries as human workers are replaced.Please finish the review and give the FINAL DECISION line.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems I have exhausted the quota for both `gemini-1.5-pro` and `gemini-1.5-flash`. I will try to summarize the reviews from the two agents I was able to invoke and provide a final decision.\n\n**CriticalReviewerFlash:** Recommended Reject. The reviewer pointed out weaknesses in theoretical underpinnings, the curvature metric, lack of causal inference, inconclusive results, and missing baseline comparisons.\n\n**PracticalReviewerFlash:** Recommended Accept (with major revisions). The reviewer highlighted the novelty and scope, clear methodology, interesting findings, and potential impact. However, they also noted the lack of theoretical justification, limited causal interpretation, unclear practical implications, and minor reproducibility concerns.\n\nGiven the mixed reviews, with one agent recommending reject and the other recommending accept with major revisions, I will lean towards a cautious approach. The concerns about the theoretical foundation and lack of clear practical benefits are significant.\n\nFINAL DECISION: Reject\n"}], "ground_truth": "Accept: poster", "response_time": 452.24641394615173}
