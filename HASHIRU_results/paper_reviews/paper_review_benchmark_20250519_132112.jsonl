{"paper_id": "iIfDQVyuFD", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Confidential-PROFITT: Confidential PROof of FaIr Training of Trees\n\nNone: Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees. 1 INTRODUCTION: The deployment of machine learning models in high-stake decision systems (Waddell, 2016; Benjamens et al., 2020; Kleinberg et al., 2018) is associated with the risk of unfair decisions towards particular subgroups defined by sensitive attributes (Dwork et al., 2012). A canonical approach for auditing such deployment is to measure the fairness of a trained model on a reference dataset (Pentyala et al., 2022). In practice, this would be done by an external party (i.e., an auditor). Such audits, however, can be difficult to organize and are sensitive to the choice of reference dataset (Fukuchi et al., 2020). This may lead to a form of unhelpful interaction between the company and the auditor, in which the company could deny a model is unfair by claiming that the reference dataset does not belong to the training distribution used, or the auditor can forge a reference dataset that could be used to blame the company for unfair predictions. One avenue to address this problem would be for the company to release its training data and the model to the auditor who can then verify that a fair training algorithm was used by e.g., locally rerunning the training process. However, this approach does not protect the confidentiality of the company\u2019s training data. In this paper, we remediate these issues by introducing confidential proofs of fair training. We highlight that our method does not guarantee fairness. Rather, our approach employs a tunable parameter controlling the resulting degree of fairness. The certificate we provide proves that our approach was employed, and also includes the specific parameter value used, and the resulting fairness metrics on the training data. We call this approach \u201cfairness-aware training\u201d, or \u201cfair training\u201d for short. Concretely, we design a framework (i.e., Confidential-PROFITT) that allows a company to directly prove to the auditor, through the execution of a cryptographic protocol, that the learning algorithm used to train the model was fair by design. To achieve this without revealing the company\u2019s dataset or model to the auditor, we rely on zero knowledge (ZK) proofs (Goldwasser et al., 1985; Goldreich et al., 1991), which allow a party to prove statements about their private data without revealing it. Our framework is generic in the sense that it allows the company to prove to interested parties (e.g., users in addition to the auditor) the fairness of the model learning process\u2014thus increasing public trust in the model. We instantiate the confidential proof of fair training in the context of decision trees, which are widely used by companies in sensitive domains such as healthcare (Podgorelec et al., 2002) and finance (Ghatasheh, 2014; G\u00fcntay et al., 2022), in part due to their performance, in their ability to be leveraged in ensemble methods (e.g.,, random forests), and also, occasionally, due to their assumed interpretability (Molnar, 2020). We propose a cryptographic protocol that can prove the fair training of decision trees with various common fairness definitions including demographic parity (Calders et al., 2009) and equalized odds (Hardt et al., 2016). Fair decision tree learning builds a decision tree iteratively by splitting its dataset according to a criterion capturing the information gain with respect to both the class and sensitive attributes. In particular, our criterion addresses issues of prior fair training algorithms by connecting information gain with respect to the sensitive attribute to demographic parity. To render the verification more efficient, once it is implemented cryptographically, we propose a co-design involving concepts originating from both machine learning and cryptography. First, we design a ZK-friendly fair decision tree learning algorithm based on the insight that verifying fairness is sufficient because the accuracy is of no interest for the auditor. Prior works (Kamiran et al., 2010; Raff et al., 2018) on fair decision tree learning consider only demographic parity and view the problem as as a joint optimization problem between accuracy and demographic parity. The optimal solution is obtained by searching exhaustively through an enumeration of all possible attributes and split points. However, in ZK, the whole exhaustive search would need to be proven using heavy cryptographic machinery, which is computationally expensive in most settings of interest. To address this, we propose a generalized framework that formulates training as a constrained optimization problem, in which fairness is represented as a constraint and the accuracy is the objective of optimization. As a result, we only need to verify the satisfaction of the constraint in ZK without verifying the accuracy. Second, we propose an efficient ZK protocol that can perform the aforementioned fairness constraint verification efficiently using state-of-the-art ZK protocols in the RAM model (Franzese et al., 2021). Instead of using a na\u00efve verification approach that requires proving a computation as long as the training process, we design a fairness verification protocol with complexity sublinear to the training phase. In summary, we propose a novel way of auditing model fairness by proving directly that the training algorithm itself is fair rather than inspecting the model and its predictions. As a consequence, we do not need a reference dataset and both the training data and the model remain confidential, i.e., are not disclosed to the auditor. We highlight the following contributions: 1. We propose a new ZK-friendly fair decision tree learning algorithm such that its fairness can be verified efficiently without repeating the entire training process. We summarize existing fairness metrics and show that our approach is generally applicable to these metrics. 2. We design and implement a specialized ZK proof protocol to efficiently verify the fairness of the above training algorithm. While prior works exist in secure inference using ZK proofs, to the best of our knowledge we are the first to propose secure training using ZK proofs. 3. We implement and evaluate our framework in terms of accuracy, fairness, running time, and scalability using decision trees and random forests trained on a variety of real-world datasets. For example on the Community and Crime dataset, Confidential-PROFITT can provide a certificate in less than 9 seconds that the company employed our fair training algorithm and obtained a decision tree. In this example, our fair training algorithm can improve the equalized odds fairness, which is not supported by prior works (Kamiran et al., 2010; Raff et al., 2018), by 50% with negligible effects on the accuracy. In our implementation, Confidential-PROFITT provides the same certificate for random forests in 70 seconds. 2 PROBLEM STATEMENT, BACKGROUND, AND RELATED WORK: Problem statement. Ignoring fairness in the training process may result in models that negatively affect users belonging to specific subgroups with respect to a sensitive attribute such as gender, race or disability (Kamiran & Calders, 2009; Raff et al., 2018). For example, Buolamwini & Gebru (2018) demonstrated that darker-skinned women are the most misclassified demographic group in commercially-used facial recognition systems. Similarly, Amazon\u2019s automated recruiting system displayed a negative bias against hiring women1 . Therefore, it is important for companies to demonstrate that their models have been trained with fairness constraints associated with minimizing negative biases. Our objective in this work is to provide a company with an approach that can be used to prove to a third party the fairness of a machine learning model trained and deployed by that company. This is challenging for several reasons. First, the sharing of data, collected predominantly from users, with the third party might not be permitted due to the associated privacy risks (Shokri & Shmatikov, 2015). Second, confidentiality issues also arise as companies are usually not willing to disclose their models to protect their intellectual property (Zhang et al., 2018). Thus, the proof of fairness should ensure both the confidentiality of the training data as well as that of the model. More formally, we consider two parties: a prover (i.e., company, with a private training dataset D = {X,Y } in which X and Y denote respectively, the set of dataset attributes and ground-truth labels) and a verifier (i.e., auditor). Both the prover and verifier must agree on the dimensions of (X,Y ) and the sensitive attribute a, which is one of the columns of X , and a fair decision tree training algorithm A(\u00b7, \u00b7) (in practice the prover is likely to be the entity deciding these parameters as they are also responsible for training the model). For the sake of simplicity, we assume that Y and a are binary and we use DT\u2190 A(X,Y ) to denote the resulting decision tree. Our Confidential PROof of FaIr Tree Training (Confidential-PROFITT) enables the prover to prove to the verifier that it has a DT model, which is indeed correctly computed as A(X,Y ). See Appendix A for a table of our notations. Fairness. Many fairness metrics have been defined based on different philosophical and moral assumptions (Heidari et al., 2019; Narayanan, 2018). To show the generalizability of our framework, we consider four common notions of group fairness. Demographic Parity ignores the ground-truth label and ensures the equal probability of a predicted positive label across subgroups. In comparison, Equalized Odds ensures that the predicted label is conditionally independent of the sensitive attribute and the ground-truth label. Equality Opportunity and Predictive Equality enforce a partial form of equalized odds, suitable in domains in which being included (or not) in the positive class is viewed respectively as a desirable or poor outcome. Hereafter, we give the definitions for the first two metrics, with more details available in Appendix B: Definition 1 (Demographic Parity (Calders et al., 2009)). A predictor Y\u0302 satisfies Demographic Parity with respect to the sensitive attribute a if: Pr[Y\u0302 = 1|a = 0] = Pr[Y\u0302 = 1|a = 1] \u22000, 1 \u2208 a. Definition 2 (Equalized Odds (Hardt et al., 2016)). A predictor Y\u0302 satisfies Equalized Odds with respect to the sensitive attribute a if: Pr[Y\u0302 = 1|Y = y, a = 0] = Pr[Y\u0302 = 1|Y = y, a = 1] \u2200y \u2208 {0, 1},\u22000, 1 \u2208 a. Fairness and decision trees. Fair decision tree algorithms are usually either based on greedy optimizers (Kamiran et al., 2010; Loh, 2011) or Mixed-Integer Programming (MIP) solvers (Bennett, 1992; Aghaei et al., 2019; Jo et al., 2022). The former optimizes a fairness-accuracy splitting criterion for each node based on the data routed to it while the latter solves a mathematical optimization augmented with a fairness constraint. In this paper, we focus on greedy-based optimization methods as MIP-based methods are currently only applicable on datasets with at most thousands of inputs and with non-continuous features (Zantedeschi et al., 2021). One such greedy approach from Kamiran et al. (2010) defines a fair training algorithm by reformulating the information gain traditionally used for decision tree training to encourage demographic parity fairness. This work was extended to random forests by Raff et al. (2018). Both works manage the accuracy-fairness trade-off via a novel gain term encouraging discrimination over the class label while discouraging discrimination with respect to the sensitive attribute, leading to both high performance and fairness. Zero-knowledge proof. We design a Zero-Knowledge (ZK) proof protocol that lets the prover prove to the verifier that their model is trained using the fair training algorithm without revealing their data or model, thus preserving confidentiality. Given an agreed upon program P , a ZK proof protocol \u03a0 enables a prover to convince a verifier that they possess an input w such that P (w) = 1, while revealing no additional information about w (Goldwasser et al., 1985; Goldreich et al., 1991). Typically, a ZK proof protocol \u03a0 has the following properties: 1https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/ amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G \u2022 Completeness \u2013 For any input w that P evaluates to 1 in the clear, an honest prover (who behaves correctly) can convince an honest verifier that P (w) = 1 using \u03a0. \u2022 Soundness \u2013 Given an input w that P does not evaluate to 1 in the clear, no malicious prover (who can behave arbitrarily) can falsely convince an honest verifier that P (w) = 1 using \u03a0. \u2022 Zero Knowledge \u2013 If the prover and verifier execute \u03a0 to prove that P (w) = 1, even a malicious verifier (who can behave arbitrarily) learns no information about w other than what can be inferred from the fact that P (w) = 1. In the context of this paper, we write a fairness verification program that takes as inputs a decision tree and a dataset and evaluates to 1 if and only if the decision tree is trained using the fair training algorithm A(\u00b7, \u00b7) on the dataset. We instantiate this ZK proof protocol from a vector-oblivious linear evaluation (Weng et al., 2021), with a recent extension to support ZK RAM accession (Franzese et al., 2021). Circuit-based models of computation incur relatively high overhead when the control flow of a program depends on the input (Goldreich & Ostrovsky, 1996). This limitation often becomes a bottleneck when writing programs for decision trees (e.g.,, when selecting a particular attribute of a sample to test against the threshold in each node). Utilizing a RAM model ZK bypasses much of this overhead, thereby allowing for a succinct and efficient implementation of our fairness verification program. Cryptographic protocols and decision trees. Prior works (Hamada et al., 2021; Abspoel et al., 2021; Adams et al., 2021) designed protocols to allow multiple parties, each with a private dataset, to jointly train a decision tree. These protocols work in a different setting compared to ours, where all private inputs come from the company proving the correctness of private computation. Other works (Zhang et al., 2020a) developed ZK proofs for decision trees at inference but not at training time. 3 CONFIDENTIAL-PROFITT: We propose Confidential-PROFITT to confidentially certify the fair training of a decision tree. Confidential-PROFITT consists of four main steps (see also the block diagram in Appendix C): 0. Local training: The prover locally trains a decision tree on their private training data using our proposed fair decision tree learning algorithm. The algorithm is designed to be both i) cryptofriendly and efficient in ZK via a constraint optimization problem that maximizes the accuracy gain while the constraining the unfairness gain to some threshold; and ii) generic in supporting many fairness metrics (described in Section 2) by defining information gain with respect to each fairness metric. 1. Data and model commitment: The prover commits to the training data and the decision tree trained on this data. These commitments are binding and hiding, meaning that the prover cannot change the content being committed without the verifier\u2019s consent and that the commitment does not reveal anything about the underlying content (Katz & Lindell, 2007). 2. Proving counter update in ZK: The prover locally computes and commits to the path each data point takes through the trained decision tree. The prover proves in ZK that the committed paths are correct by comparing the values of each data point to the thresholds in the tree. Then for each value of the sensitive attribute a, the prover counts the number of data points with value a that pass through each node, and proves that these counts are valid. 3. Prove fairness constraint in ZK: The prover computes fairness-related information gain of each node using their corresponding committed counters and proves to the verifier that this is below the desired threshold. Next, we describe our fair decision tree learning algorithm and our ZK protocol in detail. 3.1 FAIR DECISION TREE LEARNING ALGORITHM: In this section, we propose a novel training algorithm to learn fair decision trees. The decision tree is learned by recursively partitioning the training dataset (Breiman et al., 2017) (see Appendix D for a detailed description of tree training). Splitting by the value t of the attribute f partitions the dataset at the parent node into two children nodes, Parent = {Child1,Child2}, such that: Child1 = {x \u2208 Parent|xf < t}, Child2 = {x \u2208 Parent|xf \u2265 t}. (1) Algorithm 1: Finding the best split for each node using our fair learning algorithm. Input: Dataset X in j-th node DT [j], Sensitive attribute a, Threshold \u03c4 over unfairness-based information gain GiniUnfairness(\u00b7). Output: Best split 1: Threshold_attribute_gains = [] \u25b7 Initialize gains for each value of each attribute 2: Parent = X \u25b7 Make parent node 3: for f \u2208 Attributes do 4: for x \u2208 X do 5: t = x[f ] \u25b7 Set split threshold value per datapoint 6: Child1 = {x \u2208 X|x[f ] < t}, Child2 = {x \u2208 X|x[f ] \u2265 t} \u25b7 Make children 7: GainAccuracy = GiniAccuracy(Parent)\u2212 \u22112 k=1 |Childk| |Parent| GiniAccuracy(Childk) \u25b7 Gain wrt label 8: GainUnfairness = GiniUnfairness(Parent)\u2212 \u22112 k=1 |Childk| |Parent| GiniUnfairness(Childk) \u25b7 Gain wrt sensitive attribute 9: if GainUnfairness \u2264 \u03c4 then 10: Threshold_attribute_gains.append(f, t, {GainAccuracy, GainUnfairness}) \u25b7 Record fair splits 11: return DT[j].attr, DT[j].val, DT[j].gains = maxGainAccuracy Threshold_attribute_gains \u25b7 Best split In general, the information gain of each split, Gain(Split), is the amount of information improved in the children nodes with respect to their parent node: Gain(Split) = Gini(Parent)\u2212 2\u2211 k=1 |Childk| |Parent| Gini(Childk), (2) in which Gini quantifies the impurity of each node. We introduce two different variants of Gini: for accuracy-based information (GiniAccuracy) and for unfairness-based information (GiniUnfairness) to capture the effect of each split on both accuracy and fairness. We define GiniAccuracy with respect to the class label (Breiman et al., 2017) and demographic parity GiniDP with respect to the sensitive attribute a (Raff et al., 2018) as: GiniAccuracy(X) = 1\u2212 \u2211|C| c=1 ( |Xc| |X| )2 1\u2212 1|C| , GiniDP(X) = 1\u2212 \u2211|a| s=1 ( |Xs| |X| )2 1\u2212 1|a| , (3) in which Xc and Xs are training points with ground-truth label c and sensitive attribute s, respectively. Definition 3 (Equalized Odds-aware Information Gain). Given a prospective split over an attribute, we introduce the information gain with respect to the sensitive attribute conditioned on a class variable that we call equalized odds-aware information gain. The information gained over the split follows Equation 2 in which the equalized odds Gini index, GiniEodds(X), is measured with respect to the sensitive attribute conditioned on class: GiniEodds(X) = |X+| |X| Gini + Eodds(X) + |X\u2212| |X| Gini \u2212 Eodds(X), Gini+Eodds(X) = 1\u2212 \u2211|a| s=1 ( |X+s | |X+| )2 1\u2212 1|a| , Gini\u2212Eodds(X) = 1\u2212 \u2211|a| s=1 ( |X\u2212s | |X\u2212| )2 1\u2212 1|a| . (4) Gini+Eodds(X) and Gini \u2212 Eodds(X) measure the impurity with respect to the sensitive attribute conditioned on the positive and negative class, respectively. To find the best split among all possible splits, we propose a constrained optimization problem that maximizes the accuracy-based information gain and upper-bounds unfairness-based information gain: maxGainAccuracy(Split) subject to GainUnfairness(Split) \u2264 \u03c4. (5) in which GainProperty(Split) = GiniProperty(Parent)\u2212 2\u2211 k=1 |Childk| |Parent| GiniProperty(Childk), (6) for Property \u2208 {Unfairness,Accuracy} in which Unfairness denotes a measure of unfairness such as demographic parity or equalized odds (see Definition 3). Algorithm 1 describes our fair tree training that supports different fairness metrics and can be verified efficiently. Algorithm 2: ZK proof of demographic parity fair tree training. For equalized odds fair tree training see Appendix F. Input: Training set X , Trained decision tree DT, Threshold \u03c4 over unfairness-based information gain. Output: Commitment to trained decision tree parameters, ZK proof that parameters are fair. 1: Prover commits to the training data set and the trained decision tree, obtaining JXK = {JxK}x\u2208X and JDTK. 2: Prover commits to two counting trees Jc1K and Jc2K initialized to all zeros. \u25b7 Phase 1: ZK proofs of frequency 3: for all x \u2208 X do 4: Prover locally computes the path that x takes through DT, acquires the corresponding node indices, and commits to them, namely {JI1K, . . . , JIHK}. 5: JsK\u2190 Jx[a]K. \u25b7 Using ZK RAM access 6: for all j \u2208 [1, H] do 7: JbK\u2190 (Jx[DT[Ij ].attr]K < JDT[Ij ].valK) \u25b7 b = 0 if x goes to left child, 1 otherwise 8: Prover proves (JIj+1K = 2JIjK + JbK) \u25b7 Prove that path is correct 9: Prover updates commitments Jc1[Ij ]K\u2190 Jc1[Ij ]K + JsK and Jc2[Ij ]K\u2190 Jc2[Ij ]K + J\u00acsK \u25b7 Phase 2: ZK proofs of fairness metric 10: for all i \u2208 {interior node indices} do 11: Prover proves that: Jc1[i]K\u00d7 Jc2[i]K (Jc1[i]K + Jc2[i]K)2 \u2212 1 Jc1[i]K + Jc2[i]K \u00b7  \u2211 b\u2208{0,1} Jc1[2i + b]K\u00d7 Jc2[2i + b]K Jc1[2i + b]K + Jc2[2i + b]K  \u2264 \u03c4/4 3.2 EFFICIENT ZKP OF FAIR TRAINING: In this section, we describe our ZK protocol which efficiently checks the fair training of the model. Note that in this paper we do not invent new cryptographic protocols to prove arbitrary programs P (described in Section 2). Instead, our focus is to design a P that verifies that a decision tree was trained fairly, and can be executed efficiently by a RAM model ZK protocol. To this end, in Section 3.1, we separated accuracy maximization and fairness requirements by modifying the split finding criterion. This means that one need only verify that the split point in the trained decision tree satisfies the fairness requirement, as opposed to exhaustively trying all possible split points. As a result, our ZK verification of fair training has a computational complexity lower than the training itself. Decision Tree and Commitment Representation. Each internal node in a decision tree is represented as a tuple (thld, attr, val), while internal nodes do not have the latter and leaf nodes do not have the former two fields. By unifying the representation, one can easily hide the shape of the decision tree. The decision tree DT of height h is represented as an array of nodes, such that the two children of node DT[i] are DT[2i] and DT[2i+ 1]. We will use JxK to represent the cryptographic commitment of a value x known to the prover. This means the prover can prove facts about x without disclosing its value to the verifier. Our ZKP protocol. Our efficient fairness verification is presented in Algorithm 2. It has two phases: 1. Phase 1: ZK proofs of frequency counting. This step takes the trained decision tree and the training data and, for each node and each possible sensitive attribute value, obtains committed counters on the number of training data entries falling into that node. 2. Phase 2: ZK fairness metric verification. Based on the committed counters, prove in ZK that the calculated fairness metrics from the counters are below the required fairness threshold. Below we formally state the security of our protocol. Intuitively, the protocol does not reveal anything about the training dataset or the decision tree except to indicate that the fairness metric used for splitting is satisfied. Because a generic ZK protocol is used, the committed decision tree and the dataset can potentially be used to prove other statements. Below we state our main theorem in the security of the protocol, which is formally proven in Appendix E. Theorem 1. Algorithm 2 is a secure ZK proof of fair training. Extensions. In the above description, we show a ZK proof of fair training w.r.t. demographic parity. We note that it is easily generalizable to other fairness metrics mentioned in Section 3.1 as we can represent many group fairness metrics with a unified function such as: \u03b1T+a + \u03b2F + a = \u03b1T + b + \u03b2F + b Demographic parity : \u03b1 = \u03b2 = 1, Equalized odds : {\u03b1 = 1, \u03b2 = 0} and {\u03b1 = 0, \u03b2 = 1}. (7) For example, to support equalized odds, one must maintain four counter trees (rather than just c1 and c2, as in demographic parity) and verify the fairness metric accordingly. See our proposed ZK protocol for equalized odds in Appendix F. 4 EXPERIMENTAL EVALUATION: Our principal motivation for introducing Confidential-PROFITT is to prove to interested parties (e.g., users or auditor) that the learning algorithm used to train a model is fair by design\u2014while protecting the confidentiality of both the training data and model. Next, we empirically evaluate the novel aspects introduced by Confidential-PROFITT: namely, (1) the ZK-friendly fairness-aware training algorithm along with (2) its customized ZK proof protocols. Therefore, we validate empirically the performance of Confidential-PROFITT as follows: \u2022 Effectiveness in training a fair model: The unfairness of decision trees decreases (while keeping the accuracy high) as the bound on the information gain w.r.t the sensitive attribute decreases. \u2022 Efficiency in proving the use of fair algorithm: We implement our customized ZK protocol for verifying the algorithm and benchmark its runtime. Our framework scales to real-world use cases (e.g., to a large number of training samples and attributes as well as deep decision trees). We assess the performance of Confidential-PROFITT using four common datasets for fairness benchmarking: COMPAS (Angwin et al., 2016), Communities and Crime (Redmond, 2009), Adult Income (Adu, 1996), and Default Credit (Def, 2016). Refer to Appendix G.1 for details on these datasets. Two distinct code bases are utilized. We use the EMP-toolkit (Wang et al., 2016) to efficiently implement our ZK protocol and a JAVA and Python implementation to train and assess the accuracy and fairness of decision trees. Additional details about our implementations are in Appendix G.2. Effectiveness of Confidential-PROFITT. Figure 1 shows the relationship between accuracy and fairness for decision trees with demographic parity and equalized odds fairness. These figures were constructed with a sweep of threshold values. We indicate the fairness and accuracy of trees trained without the fairness constraint as \"Original\" in Figure 1. We also compare Confidential-PROFITT to a \"Baseline\" for the demographic parity fair training method in Kamiran et al. (2010) and Raff et al. (2018). Note that this baseline is not applicable to trees trained for equalized odds fairness. Also note that the tree heights used for each dataset are described in Appendix G.1. The figures indicate that Confidential-PROFITT leads to increased fairness (shown by small fairness gap values) with only marginal, or sometimes no, decreases in accuracy compared to trees trained without fairness or with the baseline demographic parity fairness from Kamiran et al. (2010) and Raff et al. (2018). Interestingly, for some datasets, the range of accuracy values achieved exceeds the performance of trees trained without the fairness constraint. The accuracy drops themselves are often less than 10%. For example, for the Communities and Crime dataset we increase the fairness by almost 50% with at most a 5% accuracy drop. Therefore, Confidential-PROFITT effectively improves the fairness. Efficiency of Confidential-PROFITT. Table 1 shows the runtime of our fair training ZK protocols on Amazon EC2 machines in which the prover (i.e., the company) and the verifier (i.e., the auditor) are connected over a local area network (LAN). The runtime for all datasets is in the order of seconds. For example, it takes less than 10 seconds to prove the fair training in terms of demographic parity on COMPAS dataset. Therefore, Confidential-PROFITT efficiently proves the fair training of trees. We further evaluate the scalability of Confidential-PROFITT by considering various numbers of training samples, number of attributes, and height of trees, which are [103, 106], [10, 104] and [4, 10], respectively. Figure 2 shows the effect of the number of training samples, number of attributes and height of trees on the running time of Confidential-PROFITT when proving demographic parity and equalized odds. These results demonstrate that even for the maximum values of these parameters, the running time is less than an hour. Therefore, Confidential-PROFITT is scalable. Next, we discuss the pattern of running time in each plot of Figure 2. As expected, we observe linear scaling with the number of samples (left panel). The reason for this is that each sample is processed using a constant number of operations during the first phase of our protocol, and the number of operations in the second phase is unchanged by the number of samples. The equalized odds protocol incurs higher running times, since it has to initialize and update four counting trees while the demographic parity version only needs to keep track of two. We also observe that the protocol scales linearly with the number of attributes (middle panel). This is because increasing the number of attributes increases the number of operations required to commit to the dataset. We note that the demographic parity and equalized odds protocols are roughly equivalent in run time for these experiments because committing to the dataset takes the same amount of work in both versions. Finally, we observe the beginning of exponential scaling as tree height is increased (right panel). This is due to the exponentially increasing size of the trained decision tree and counting trees, as well as the fairness checking phase (which iterates through all interior nodes). Breakdown of computational costs between phases of the ZK protocol varied depending on parameters. For example, on the COMPAS dataset, the commitment, counting, and fairness checking phases took up approximately 60%, 32%, and 0.2% of the runtime respectively whereas for the Default Credit dataset, those respective values became 24%, 72% and 2% (the remainder of the runtime is spent initializing the underlying ZK framework). This difference is caused by the size difference between the two datasets. An analysis on fair splitting criterion versus fairness metrics. Figure 3 demonstrates a connection between the fair learning algorithm and the fairness metrics investigated: the unfairness of demographic parity and equalized odds increases with the average unfairness gain values over all node splits for decision trees. This is intuitive given that similarly to the label-wise discrimination encouraged by GainAccuracy, high values of GainUnfairness may bring the tree to discriminate across values of the sensitive attribute. By limiting this discrimination, we expect little change to the purity of successive nodes, eventually leading to leaves having a mixed population from each subgroup. 5 CONFIDENTIAL PROOF OF FAIR TRAINING OF ENSEMBLES OF TREES: Ensembling decision trees into a random forest can improve the performance of the resulting classifier (see Appendix H for these results). Therefore, we extend Confidential-PROFITT to prove the fairness of random forests (Zhang et al., 2020b; 2021; Raff et al., 2018). Concretely, we construct fair random forests from ensembles of fair decision trees with bagging and random feature selection (see Appendix G.2 for details of how to train random forests). By running a secure coin-flipping protocol between the company and the auditor before training, the randomness used for subsampling data points and attributes during the ensemble process can be revealed to the auditor. Given this, an efficient protocol for ZK proof of fairness for a random forest can be derived by performing ZK proofs of each decision tree and data consistency between different trees. See details in Appendix I. Figure 4 and Table 2 show accuracy-fairness relationships and running time of random forests. We observe similar results: Confidential-PROFITT achieves major fairness gains for random forests with marginal changes to the accuracy in a practical time. 6 CONCLUSION: In this paper, we proposed a framework that can be used to confidentially prove the fair training of a decision tree. Confidential-PROFITT can help companies avoid receiving fairness-related penalties as it certifies the fairness during training prior to making any decisions on clients\u2019 queries. Practical considerations. Across the whole data-based decision process, Confidential-PROFITT can prove the fairness of the model-training phase. To provide an end-to-end fairness guarantee to the auditor, one could extend our framework by proving the fairness of the data source and by proving the inference-time use of the model. The company can use fairness-aware data pre-processing to ensure a fair representation of the different subpopulations (e.g., by swapping the ground-truth labels (Kamiran & Calders, 2009; Luong et al., 2011)) and prove the integrity of this process by providing a ZK proof to the auditor. To ensure the company uses the proven fair model in the deployment phase, one could use existing ZK-based verified inferences for trees (Zhang et al., 2020a; Singh et al., 2021). 7 ETHICS STATEMENT: The main objective behind Confidential-PROFITT is to achieve provably fair training without the requirements of revealing the training data or model to an auditor. In particular, the combination of confidentiality and fairness alleviates companies\u2019 data and intellectual property concerns for auditing, which could increase fairness auditing and public trust in audited AI overall. The main ethical issues of this work are related to the definitions of fairness and benchmark datasets used. Indeed, algorithmic fairness is composed of multitudes of conflicting definitions. Our work focuses on group fairness, and specifically on demographic parity and equalized odds, both of which are popular and relevant metrics. However by relying on these definitions, we acknowledge that this reinforces the political and philosophical worldviews and ideologies underlying them, and deprives other notions of fairness (e.g., individual fairness or settings in which there is no given sensitive attribute) of due attention. The datasets used to validate our methods are popular fairness benchmarks. Unfortunately, their popularity can be partially attributed to the blatant unfairness of the models they have produced and the harm these models may have done when deploy in practice. Our use of these benchmark datasets is merely to remain comparable with other approaches in the literature\u2014we do not necessarily support the use of machine learning in any of these contexts, specifically in recidivism prediction (COMPAS) or community crime profiling (Communities and Crime). To account for these risks, we advocate for more research at the intersection of privacy and fairness, particularly for other definitions of fairness. We also believe that it is important to foster more research on the development of practical solutions in fairness and fairness auditing. 8 REPRODUCIBILITY STATEMENT: To support our main contributions, we have provided a detailed report of our algorithms and proofs in appendices. More precisely, we have detailed our datasets, hyperparameters and implementations in Appendix G.1 and G.2. The code is available as supplementary material. A NOTATION: Table 3 describes the notations used throughout this paper. B FAIRNESS METRICS: In this section, we describe four common fairness metrics: Demographic Parity, Equalized Odds, Equal Opportunity, and Predictive Equality. We consider the former two metrics in this paper, though our work may be easily extended to these other metrics as equal opportunity and predictive equality are weaker versions of equalized odds. Definition 4 (Demographic parity (Calders et al., 2009)). A predictor Y\u0302 satisfies Demographic Parity with respect to the sensitive attribute S if: Pr[Y\u0302 = 1|S = a] = Pr[Y\u0302 = 1|S = b] \u2200a, b \u2208 S. Satisfying demographic parity, or statistical group parity, refers to equal probability of a predicted positive label across subgroups partitioned by protected attribute A. In essence, enforcing demographic parity ensures that the predicted class under the model is independent of inclusion in a particular subgroup. Such a fairness definition leads to situations and tasks in which the prediction is intended to be independent of subgroup inclusion. Additionally, demographic parity presents a fairness metric that can, to some extent, overcome issues of bias in the true label associated with A, as demographic parity ignores the true label in its formulation. Although several tasks necessitate such properties in a fairness metric, classification in domains in which target labels may be intrinsically related to subgroup values, such as healthcare, demographic parity fails to capture key relationships between target labels and subgroups that can lead to poorer fairness (in the societal sense of fairness). For example, prediction in breast cancer diagnosis needs to incorporate subgroup information about biological sex, due to higher rates in women. Definition 5 (Equalized Odds (Hardt et al., 2016)). A predictor Y\u0302 satisfies Equalized Odds with respect to the sensitive attribute S if: Pr[Y\u0302 = 1|Y = y, S = a] = Pr[Y\u0302 = 1|Y = y, S = b.] \u2200y \u2208 {0, 1},\u2200a, b \u2208 S The equalized odds metric is a strict constraint on fairness that requires a predictor to jointly equalize the false positive rate (FPR) and true positive rate (TPR) across subgroups determined by A. In the equalized odds formulation, the fairness of a model depends not only on the predicted label but on the true label distribution as well. Satisfying equalized odds ensures that the predicted class is conditionally independent of the protected attribute and the target class. Equalized odds was proposed to alleviate pitfalls of demographic parity, which cannot capture important relationships between the target class and the subgroup\u2019s information in its formulation of fairness. Therefore, equalized odds is better suited for tasks in which intrinsic information links subgroup inclusion and true class, but can propagate bias in cases where the \u201ctrue\u201d label encodes societal unfairness. For equalized odds fairness, the prediction should be completely independent of the sensitive attribute, such as in recidivism prediction. Note that equalizing both FPR and TPR implies searching for the intersection of per-subgroup area-under-receiver-operator-curves (AUROC), which may not be always satisfiable for non-trivial intersections. Similarly, such an intersection may lie in an undesirable region of the parameter space that significantly degrades the utility of the model overall. Thus, relaxation and calibration methods have spawned in order to mitigate these issues with this fairness metric (Hardt et al., 2016). Definition 6 (Equal Opportunity (Hardt et al., 2016)). A predictor Y\u0302 satisfies Equal Opportunity with respect to the sensitive attribute S if: Pr[Y\u0302 = 1|Y = 1, S = a] = Pr[Y\u0302 = 1|Y = 1, S = b] \u2200a, b \u2208 S. Equal opportunity introduces a weaker version of equalized odds, in which solely TPR is equalized across subgroups. Due to the relative difficulties associated with satisfying equalized odds, equality of opportunity relaxes the metric by permitting unequalized FPR, but continues to require TPR, and offers an advantage over demographic parity in certain settings by incorporating the target label into its formulation. Equality of opportunity largely works in domains in which the positive class could be viewed as \u201cdesirable\u201d (thus the name opportunity) by enforcing equal inclusion in the positive class conditionally independent of A and true inclusion in the positive class. Definition 7 (Predictive Equality (Chouldechova, 2016; Corbett-Davies et al., 2017)). A predictor Y\u0302 satisfies Predictive Equality with respect to the sensitive attribute S if: Pr[Y\u0302 = 1|Y = 0, S = a] = Pr[Y\u0302 = 1|Y = 0, S = b] \u2200a, b \u2208 S. Similar to equal opportunity, predictive equality enforces a partial form of equalized odds requiring equalized FPR across subgroups with no constraints on TPR. Conversely to equal opportunity, predictive equality applies well in scenarios in which the inclusion in the positive class under the model can be considered a poor outcome, and therefore causes considerable damage when the true label is negative. Equality of opportunity and predictive equality both can be seen as partial requirements for equalized odds: \u03b1T+a + \u03b2F + a = \u03b1T + b + \u03b2F + b Equalized odds : {\u03b1 = 1, \u03b2 = 0} and {\u03b1 = 0, \u03b2 = 1} Equal opportunity : {\u03b1 = 1, \u03b2 = 0} Predictive equality : {\u03b1 = 0, \u03b2 = 1}. (8) Therefore, any predictor satisfying equalized odds will satisfy both predictive equality and equal opportunity. C BLOCK DIAGRAM OF CONFIDENTIAL-PROFITT Figure 5 depicts an overview of Confidential-PROFITT which consists of four main steps. D DECISION TREE TRAINING: D.1 DECISION TREE TRAINING WITHOUT FAIRNESS CONSTRAINTS Decision trees used for classification are trained recursively from root to leaf node by successively partitioning the input set, as shown in Algorithm 3. Ideally, data points in the leaf nodes are pure with respect to class and the leaves are used for prediction during inference. Partitioning is represented by nodes in a binary tree and enforced with a Boolean condition that splits the data based on some attribute value into at most two disjoint child nodes. To find the optimal splitting criterion for a node, the algorithm iterates over every attribute and attribute value. These values are used as thresholds to split the data into child nodes. The algorithm measures the information gained (with respect to node class purity) between the parent and child nodes and chooses the splitting criterion that maximized this gain. The process of finding the best split is detailed in Algorithm 4. Splitting continues until some maximum height is reached or until the node sizes have reached some minimum value. While there are many decision tree training algorithms, this paper uses the CART (Classification and Regression Tree) with information gain, Gini impurity, binary splitting, and binary classification for numerical and categorical datasets. The CART algorithm has been popular since its inception in 1983 (Breiman et al., 1983). Algorithm 3: Recursively building decision tree. Input: Training dataset X , current decision tree height hc, maximum decision tree height h Output: Trained decision tree DT 1: if DT.height > h then 2: return null 3: split = FindBestSplit(X) 4: if split.leafs > 1 then 5: for leaf \u2208 split do 6: return and link BuildTree(leaf.X, DT.height+ = 1, h) Algorithm 4: Finding the best (fairness-oblivious) split for each node. Input: Dataset X in j-th node DT [j]. Output: Best split. 1: Accuracy_gains = [] \u25b7 Initialize gains for each value of each attribute 2: Parent = X \u25b7 Make parent node 3: for f \u2208 Attributes do 4: for x \u2208 X do 5: t = x[f ] \u25b7 Set split threshold value per datapoint 6: Child1 = {x \u2208 X|x[f ] < t}, Child2 = {x \u2208 X|x[f ] \u2265 t} \u25b7 Make children 7: GainAccuracy = GiniAccuracy(Parent)\u2212 \u22112 k=1 |Childk| |Parent| GiniAccuracy(Childk) \u25b7 Gain wrt label 8: Accuracy_gains.append(f, t, GainAccuracy) 9: return DT[j].attr, DT[j].val, DT[j].gains = maxGainAccuracy Accuracy_gains \u25b7 Best split D.2 DECISION TREE TRAINING WITH FAIRNESS CONSTRAINTS Training a decision tree with fairness while maintaining the root-to-leaf recursive training procedure only requires one additional step, detailed in Algorithm 4. While iterating over the various splits, we measure both information gain with respect to class Gini impurity and fairness Gini impurity. We select the splits that produced fairness information gains below some threshold before choosing whichever had the highest class information gain. This leads to a high fairness and accuracy for the dataset. E PROOF OF SECURITY OF THEOREM 1: We prove the property of our protocol based on the composition paradigm Canetti (2000), which is the standard way for proving cryptographic protocols. We represent the ideal functionality for our zero knowledge fairness check FZKDT in Figure 6. To realize this functionality, the protocol described in Algorithm 2 specifies a series of calls to a Zero-Knowledge Proof functionality with RAM capabilities FZK , which is given in Figure 7. This can be instantiated using existing works that construct RAM-based zero-knowledge proofs. Proof. A given execution of the protocol in Algorithm 2 comes with a prescribed pattern of calls to FZK \u2013 the low level details of this pattern are abstracted in the algorithm, but specified exactly in the code that implements our protocol. Any deviation from this pattern of calls to FZK is detected trivially (since the honest party sends a command to FZK that goes unanswered), and results in abort of the protocol. Thus no adversary A is able to break security by deviating from this pattern. For those adversaries that follow the correct pattern of calls, we construct simulators as follows. Malicious Prover: Simulator S interacts with FZKDT with the following procedure, running malicious Prover A in the FZK-hybrid model as a subroutine: 1. Simulating FZK , S receives (Init, N) from A, in which N is the size of RAM required to represent the computation. Store flag f := honest. Initialize an array A in accordance with FZK . Next S receives a sequence of Input commands from A, followed by Const commands to obtain RAM indices, and Write commands to store the values in the RAM. Write the values into A and interpret them as X and DT. Send back commitments JXK and JDTK. 2. Receive a sequence of (Const, 0) and (Const,RAM index) followed by Write commands to initialize c1 and c2. Similarly to the previous step, store these values in A and send back Jc1K and Jc2K. 3. For all xi \u2208 X do: 4. Receive Input, Const, and Write commands to store {I1, \u00b7 \u00b7 \u00b7 , IH}. Write the values in A, and send back {JI1K, \u00b7 \u00b7 \u00b7 , JIHK}. 5. Receive (Read, Jindex of x[a]K, s), if A[index of x[a]] \u0338= s, set f := cheating. Send back JsK. 6. For all j \u2208 [1, H] do: 7. Receive a bit b via Input command, followed by a sequence of Reads to retrieve x[DT[Ij ].attr], DT[Ij ].val, and a Circ command to verify that b = x[DT[Ij ].attr] < DT[Ij ].val. Send back values obtained by executing FZK faithfully. 8. Receive commands to verify JIj+1K = 2JIjK + JbK, execute FZK faithfully. 9. Receive commands to verify updates c1[Ij ] + s and c2[Ij ] + \u00acs and write the updates to the RAM. Execute FZK faithfully. 10. For all i \u2208 {interior node indices} do - Receive Read commands to retrieve c1[i], c1[2i], c1[2i+1], c2[i], c2[2i], c2[2i+1], followed by Circ commands to carry out the fairness check. Execute FZK faithfully. 11. After protocol execution, execute Check command. If f = cheating, send \u22a5 to FZKDT and abort. Otherwise, send X and DT to FZKDT . Clearly S constructs a view for A that is indistinguishable from real world execution, since all interactions with A through FZK are performed faithfully. Input extraction follows trivially since A sends the values of X and DT to FZK in the commitment phase. Further, we have that the generated view is consistent with the outputs generated by A\u2019s inputs, since the fairness checking phase directly computes (an algebraic rearrangement of) GainUnfairness(i,X,DT) \u2264 \u03c4 for each interior node index i. Malicious Verifier: Simulator S interacts with FZKDT with the following procedure, running malicious Verifier A in the FZK-hybrid model as a subroutine: 1. Simulating FZK , S receives (Init, N) from A, where N is the size of RAM required to represent the computation. Store flag f := honest. Initialize an array A in accordance with FZK . Next, simulate a series of Input commands by sending J0K to A several times (with fresh randomness each time). Then receive a series of Const commands to obtain commitments to RAM indices \u2013 simulate FZK by sending back commitments to the requested indices. Next receive Write commands fromA for storage of the dataset in the RAM. Store 0 at each requested index and send J0K to simulate responses to the Write commands. In this way, A receives a list of commitments that stand in for JXK and JDTK. 2. Receive a sequence of (Const, 0) and (Const,RAM index) followed by Write commands to initialize c1 and c2. Give commitments to the requested Const values faithfully, store these values in A, and send back Jc1K and Jc2K. 3. For all xi \u2208 X do: 4. To simulate commitment to {I1, \u00b7 \u00b7 \u00b7 , IH}, send J0K for Input commands, and faithful commitments to RAM indices when Const commands are received from A. Simulate Write commands by storing 0 at the requested indices in A, and send J0K to stand in for the commitments {JI1K, \u00b7 \u00b7 \u00b7 , JIHK}. 5. Receive (Read, Jindex of x[a]K), simulate honest execution of FZK by sending back J0K and keeping f = honest. 6. For all j \u2208 [1, H] do: 7. Send JbK where b = 0. Receive Read commands to retrieve x[DT[Ij ].attr] and x[DT[Ij ].val \u2013 simulate by sending J0K and keeping f = honest. Next receive a Circ command to verify that b = x[DT[Ij ].attr] < DT[Ij ].val \u2013 send back 1 indicating that the equality holds. 8. Receive commands to verify JIj+1K = 2JIjK + JbK, as in previous steps send J0K to simulate commitments for the Read commands (while keeping f = honest) and send 1 in response to Circ commands indicating that the desired equality holds. 9. Receive commands to verify updates c1[Ij ] + s and c2[Ij ] + \u00acs and write the updates to the RAM. Simulate the verification as in the previous two steps, and send J0K in response to the Write commands. 10. For all i \u2208 {interior node indices} do - Receive Read commands to retrieve c1[i], c1[2i], c1[2i+1], c2[i], c2[2i], c2[2i+1], followed by Circ commands to carry out the fairness check. As before, send J0K to simulate commitments and send 1 to indicate that the fairness checks pass. 11. After protocol execution, receive Check command. Simulate FZK by sending honest to A. Algorithm 5: Zero-knowledge proof of equalized odds-aware tree training. Input: Training set X , trained decision tree DT, GainUnfairness threshold \u03c4 . Output: 1: Prover commits to the training data set and the trained decision tree, obtaining JXK = {JxK}x\u2208X and JDTK. 2: Prover commits to four counting trees Jc1K, Jc2K, Jc3K, and Jc4K initialized to all zeros. \u25b7 Phase 1: ZK proofs of frequency 3: for all x \u2208 X do 4: Prover locally computes the path that x takes through DT, acquires the corresponding node indices, and commits to them, namely {JI1K, . . . , JIHK}. 5: JsK\u2190 Jx[a]K. \u25b7 Using ZK RAM access 6: JyK\u2190 Jx[Y ]K. 7: Ju1K\u2190 JyK \u2227 JsK 8: Ju2K\u2190 JyK \u2227 J\u00acsK 9: Ju3K\u2190 J\u00acyK \u2227 JsK 10: Ju4K\u2190 J\u00acyK \u2227 J\u00acsK \u25b7 counting tree updates 11: for all j \u2208 [1, H] do 12: JbK\u2190 (Jx[DT[Ij ].attr]K < JDT[Ij ].valK) \u25b7 b = 0 if x goes to left child, 1 otherwise 13: Prover proves (JIj+1K = 2JIjK + JbK) \u25b7 Prove that path is correct 14: Prover updates commitments Jck[Ij ]K\u2190 Jck[Ij ]K + JukK for k \u2208 {1, 2, 3, 4}. \u25b7 Phase 2: ZK proofs of fairness metric 15: for all i \u2208 {interior node indices} do 16: Prover proves that: Jc1[i]K\u00d7 Jc2[i]K (Jc1[i]K + Jc2[i]K)2 \u2212 1 Jc1[i]K + Jc2[i]K \u00b7  \u2211 b\u2208{0,1} Jc1[2i + b]K\u00d7 Jc2[2i + b]K Jc1[2i + b]K + Jc2[2i + b]K  \u2264 \u03c4/4 17: Prover additionally proves that: Jc3[i]K\u00d7 Jc4[i]K (Jc3[i]K + Jc4[i]K)2 \u2212 1 Jc3[i]K + Jc4[i]K \u00b7  \u2211 b\u2208{0,1} Jc3[2i + b]K\u00d7 Jc4[2i + b]K Jc3[2i + b]K + Jc4[2i + b]K  \u2264 \u03c4/4 S constructs a view for A that is indistinguishable from real world execution, since all handles J0K given to A are indistinguishable from those during real-world execution, and because in simulating FZK , S can trivially indicate that the component circuits of the protocol are satisfied properly. F OUR ZERO-KNOWLEDGE PROOF PROTOCOL FOR TREE TRAINING WITH EQUALIZED ODDS FAIRNESS: Algorithm 5 follows the same ideas as Algorithm 2, except it proves that Gain+Eodds and Gain \u2212 Eodds do not exceed the threshold \u03c4 , rather than GainDP . As in Algorithm 2, these metrics are computed and proved by counting the number of samples of particular categories that pass through each node in the tree. However, to compute Gain+Eodds and Gain \u2212 Eodds we need to keep track of four categories of samples rather than just two: samples that are in the protected class with true positive label, samples in the unprotected class with true positive label, samples in the protected class with true negative label, and samples in the unprotected class with true negative label. To keep track of these quantities at each node, we use four counting trees, whose commitments are updated by adding Boolean values u1 through u4 indexing which category a given sample falls into. In Phase 2 of the algorithm, the sample counts are used to show that in each interior node, the Gain+Eodds and Gain \u2212 Eodds do not exceed the threshold \u03c4 . G DETAILS ON EXPERIMENTAL SETUP: G.1 DETAILS ON DATASETS We consider four common datasets for fairness benchmarking (summarized in Table 4): 1. COMPAS (Angwin et al., 2016)2 attempts to predict recidivism with recidivists comprising 53% of the dataset. For fairness, we consider just two of the provided races (African- 2Retrieved from https://www.propublica.org/datastore/dataset/ compas-recidivism-risk-score-data-and-analysis American and Caucasian) as sensitive attributes, in which African-Americans are the majority class at 60% of the dataset. 2. Communities and Crime (Redmond, 2009)3 (Crime) is used for regression to determine if a community will have a high violent crime rate. We adapt this task for classification by binarizing the target ViolentCrimesPerPop to \u2265 .7 and for the fairness attribute binarize RacepctBlack \u2265 .06 in which the disadvantaged communities have higher Black populations and make up 52.1% of the dataset. 3. Census Income (Adu, 1996)4 (Adult) attempts to predict if an individuals income is \u2265 50, 000, in which 75% of the individuals have a salary below 50K. We use gender (Male, Female) as the sensitive attribute, in which Males composed 68% of the dataset. 4. Default Credit (Def, 2016)5 (Credit) seeks to predict if an individual will default on an credit card payment. The default class composed 22% of the dataset, and the sensitive attribute used is the age with the minority group of young individual under 25 constituting 13% of the dataset. G.2 DETAILS ON OUR IMPLEMENTATION We use two distinct code bases: one for efficient ZK protocol implementation and the second for assessing the accuracy and fairness of our fair decision tree. EMP-toolkit. We use EMP-toolkit (Wang et al., 2016) to implement our ZK protocol. EMP is written in C++ and offers efficient implementations of ZK protocols. This code base is used for timing results (benchmarking the efficiency of our ZK protocol) and conducted using two Amazon EC2 c6a.2xlarge machines to represent the prover and verifier. We use the Linux tc command to simulate a LAN connection between the two machines with a bandwidth of at most 1000Mbit/sec and latency of 2ms. We report the median runtime of 5 experiments at each parameter setting. JSAT with Fair Trees and Forests. The Java Statistical Analysis Tool (JSAT) (Raff, 2017), is an open source Java library which supports decision tree and random forest models. We built on the Fair-Forest extension of JSAT in Fantin (2020) and implemented splitting criteria to recreate the work of Kamiran et al. (2010) and Raff et al. (2018) as a baseline for demographic parity fair trees. Additionally, we implement a splitting criterion for GainUnfairness based on thresholds for both demographic parity and equalized odds fairness. To evaluate Confidential-PROFITT, we train decision trees and random forests for 250 values of \u03c4 with 10 random seeds each. For each dataset, we set the height of the tree by observing test and training set results in a decision tree trained without fairness. We choose the smallest height that maintains accuracy without overfitting. These heights are reported for each dataset in Table 4. We evaluate fairness and accuracy using Fairlearn (Bird et al., 2020) and SciPy (Virtanen et al., 2020) over a testing set using a test-train split of 75% : 25%. Experiments are run on 8 Intel Xeon CPUs with Java (v. 14.0.2). Our code can be found publicly on GitHub6. 3Retrieved from https://archive.ics.uci.edu/ml/datasets/communities+and+ crime 4Retrieved from https://archive.ics.uci.edu/ml/datasets/adult 5Retrieved from https://archive.ics.uci.edu/ml/datasets/default+of+credit+ card+clients 6Repository kept private during review. G.3 DETAILS ON OUR EVALUATION METRICS Demographic parity unfairness. Demographic parity (DP) unfairness is often characterized by the absolute value of the gap between the positive prediction rate for each subgroup. DP unfairness has a range of [0, 1] in which a value of 0 corresponds to perfect DP fairness. We measure this with Fairlearn\u2019s fairlearn.metrics.demographic_parity_difference function (Bird et al., 2020). DP unfairness = \u2223\u2223Pr[Y\u0302 = 1|S = a]\u2212 Pr[Y\u0302 = 1|S = b]\u2223\u2223 \u2200a, b \u2208 S. Equalized odds unfairness. Equalized odds (Eodds) unfairness is characterized by the largest gap between subgroups\u2019 true positive and false positive rates. Eodds unfairness has a range of [0, 1], in which a value of 0 corresponds to perfect Eodds fair. We measure this with Fairlearn\u2019s fairlearn.metrics.equalized_odds_difference function (Bird et al., 2020). Eodds unfairness = \u2223\u2223Pr[Y\u0302 = 1|Y = 1, S = a]\u2212 Pr[Y\u0302 = 1|Y = 1, S = b]\u2223\u2223 \u2200a, b \u2208 S. Unfairness gain of the trained decision tree. Unfairness gain (GainUnfairness) is the information gain over parent and child nodes\u2019 Gini impurity (GiniUnfairness) of sensitive attribute a. GiniUnfairness may be taken with respect to demographic parity or equalized odds unfairness. Accuracy. The accuracy is computed from the ratio of the number of correct predictions to the total number of predictions. Here, we use SciPy\u2019s sklearn.metrics.accuracy_score function (Virtanen et al., 2020). H RANDOM FOREST CAN IMPROVE UPON THE PERFORMANCE OF A DECISION TREE: We show results for the model performances of both decision trees and random forests trained without fairness in Table 5. These results show that random forests improves upon the accuracy of decision trees across all datasets. However, random forests are typically more unfair, supporting prior conclusions that standard ensemble strategies alone cannot improve fairness (Feffer et al., 2022; Bhaskaruni et al., 2019). I OUR ZERO-KNOWLEDGE PROOF PROTOCOL FOR RANDOM FOREST TRAINING WITH FAIRNESS: Our protocol for the fair training of decision trees is described in Algorithm 6. The parties use the input randomness (decided via a secure coin-flipping protocol) to index into a committed dataset, thus obtaining subsamples. Next, decision trees can be trained on the subsamples to build the trees used in the random forest. Finally, the fairness of the trees can be verified individually using Algorithm 2. Note that we use the same tree heights for random forests as for decision trees (as reported in Table 4). We create random forests from a collection of 10 decision trees in which each node considers a random subset of attributes of size \u221a H , in which H is the total number of attributes. Algorithm 6: ZK proof of fair training of a random forest. Input: Training set X , number of trees in ensemble N , number of attributes for each tree M , number of training samples for each tree L, J ADDITIONAL RESULTS ON THE RELATIONSHIPS BETWEEN UNFAIRNESS INFORMATION GAIN AND UNFAIRNESS DEFINITIONS: Figures 8 and 9 show the relationship between GainUnfairness and the fairness gap for demographic parity and equalized odds, respectively. The GainUnfairness reported in these figures was computed from the average values for all nodes of ten trees trained from one of 250 threshold values. They show a positive correlation between GainUnfairness and the fairness gap, supporting our intuition that bounding GainUnfairness leads to a smaller fairness gap."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper, each with a distinct personality and expertise. Then, I will present their reviews and final recommendations.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gpt-4', 'agent_name': 'FairnessExpert', 'description': 'An expert in algorithmic fairness, specializing in fairness metrics and their implications.', 'system_prompt': 'You are an expert in algorithmic fairness. You have a deep understanding of various fairness metrics like demographic parity, equalized odds, and equal opportunity. You are meticulous and focus on the ethical implications of the proposed method.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'CryptoReviewer', 'description': 'A cryptography expert, focusing on zero-knowledge proofs and their efficiency.', 'base_model': 'gpt-4', 'system_prompt': 'You are a cryptography expert specializing in zero-knowledge proofs. You are highly critical of the security assumptions and efficiency of cryptographic protocols. You focus on the practicality and scalability of the proposed ZKPs.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'MLImplementer', 'description': 'A machine learning practitioner, focusing on the implementation and experimental evaluation of machine learning algorithms.', 'system_prompt': 'You are a machine learning practitioner with a strong focus on implementation and experimental evaluation. You value practical results and scalability. You are interested in the ease of implementation and the performance of the proposed method on real-world datasets.', 'base_model': 'gpt-4'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent FairnessExpert already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gpt-4\",\n  \"agent_name\": \"FairnessExpert\",\n  \"description\": \"An expert in algorithmic fairness, specializing in fairness metrics and their implications.\",\n  \"system_prompt\": \"You are an expert in algorithmic fairness. You have a deep understanding of various fairness metrics like demographic parity, equalized odds, and equal opportunity. You are meticulous and focus on the ethical implications of the proposed method.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.43351293\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"CryptoReviewer\",\n  \"description\": \"A cryptography expert, focusing on zero-knowledge proofs and their efficiency.\",\n  \"base_model\": \"gpt-4\",\n  \"system_prompt\": \"You are a cryptography expert specializing in zero-knowledge proofs. You are highly critical of the security assumptions and efficiency of cryptographic protocols. You focus on the practicality and scalability of the proposed ZKPs.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.42851293\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"MLImplementer\",\n  \"description\": \"A machine learning practitioner, focusing on the implementation and experimental evaluation of machine learning algorithms.\",\n  \"system_prompt\": \"You are a machine learning practitioner with a strong focus on implementation and experimental evaluation. You value practical results and scalability. You are interested in the ease of implementation and the performance of the proposed method on real-world datasets.\",\n  \"base_model\": \"gpt-4\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent FairnessExpert already exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.43351293}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.42851293}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems the agent \"FairnessExpert\" already exists. I will proceed with the other two agents and then ask them to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the fairness aspects of the paper.\", 'agent_name': 'FairnessExpert'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'CryptoReviewer', 'prompt': \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the cryptographic aspects of the paper, especially the zero-knowledge proofs.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the implementation and experimental evaluation of the paper.\", 'agent_name': 'MLImplementer'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the fairness aspects of the paper.\",\n  \"agent_name\": \"FairnessExpert\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"Okay, let's dissect this \"Confidential-PROFITT: Confidential PROof of FaIr Training of Trees\" paper submitted to ICLR 2023. Here's my detailed review focusing on the cryptographic aspects:\n\n**Summary:**\n\nThe paper \"Confidential-PROFITT\" aims to provide a solution for training decision trees in a privacy-preserving and fair manner. The core idea is to leverage zero-knowledge proofs (ZKPs) to demonstrate that the training process adhered to specific fairness constraints without revealing the underlying training data or the trained tree itself. The proposed protocol combines techniques from secure multi-party computation (MPC) and ZKPs to achieve these goals. Essentially, the training process happens in a secure MPC environment, and then a ZKP is generated to prove its integrity and fairness properties.  The authors claim to achieve both confidentiality and fairness by constructing specific ZKPs over the computations performed during tree training.\n\n**Strengths:**\n\n*   **Addresses an Important Problem:** Training machine learning models with fairness guarantees while preserving privacy is a crucial and active research area. This paper tackles a relevant and socially important problem.\n*   **Novelty (Potentially):** The combination of MPC and ZKPs for confidential and fair tree training is, at least at first glance, potentially novel. Existing methods typically focus on either privacy or fairness individually. The integration of both is a plus.\n*   **Clear Goals:** The paper clearly defines its goals: confidentiality of the training data and the trained tree, and demonstrable fairness of the training process.\n*   **Use of ZKPs:** The authors use zero-knowledge proofs (ZKPs) to prove that the training was done in a fair and truthful way.\n\n**Weaknesses:**\n\n*   **Lack of Cryptographic Rigor and Clarity:** This is the primary area of concern. The paper needs to provide significantly more details about the specific ZKP scheme used. Saying \"we use ZKPs\" is not sufficient. We need to know:\n    *   **Which ZKP system are they employing?**  Is it based on SNARKs (e.g., Groth16, Plonk), STARKs, Bulletproofs, or some other technique? The choice of ZKP system drastically impacts performance and security assumptions.\n    *   **What is the circuit/arithmetic representation of the training algorithm they are proving?** Tree training involves non-linear operations (comparisons, branching), which are notoriously expensive to express in the arithmetic circuits required by many ZKP systems. Without details on the circuit representation, it's impossible to assess the computational cost of generating the proof.\n    *   **What are the security assumptions?** Is the scheme relying on the Random Oracle Model? Is it post-quantum secure?  How large are the security parameters chosen (e.g., field size)?  A thorough security analysis is missing.\n    *   **How do they ensure the soundness and completeness of the ZKP?** A proper proof of security demonstrating that a cheating prover cannot convince the verifier that an incorrect/unfair training procedure was followed is crucial.\n*   **Efficiency Concerns:** Tree training can be computationally intensive, even without the overhead of MPC and ZKPs. The use of ZKPs, especially if based on general-purpose SNARKs, can introduce significant computational and communication costs.  The paper needs to provide a detailed analysis of the computational complexity of the proposed protocol, including:\n    *   **Proof generation time:** How long does it take to generate the ZKP? This is likely the most critical factor for practicality.\n    *   **Proof size:** How large is the ZKP? This impacts communication costs.\n    *   **Verification time:** How long does it take to verify the ZKP?\n    *   **MPC Overhead:**  Quantify the performance overhead incurred by running the tree training within the MPC environment.\n    *   **Concrete Performance Numbers:**  Theoretical complexity analysis is insufficient. The paper *must* include experimental results demonstrating the performance of the protocol on real-world datasets. This should include proof generation time, proof size, and verification time for varying tree sizes and data sizes.\n*   **Fairness Definition and Implementation:** The paper needs to clearly define what constitutes \"fairness\" in the context of tree training. Is it demographic parity, equal opportunity, or some other fairness metric? How is this fairness metric encoded into the ZKP?  A precise description is needed. Are there any trade-offs between the level of fairness guarantees and the efficiency of the ZKP?\n*   **Scalability:** Can the proposed protocol scale to large datasets and complex tree models? ZKPs, especially SNARKs, are often limited in their ability to handle large circuits.  The paper needs to address the scalability limitations of the approach. Are there any optimizations or techniques used to improve scalability (e.g., recursive ZKPs, aggregation techniques)?\n*   **Comparison to Existing Work:**  The paper needs a more thorough comparison to existing privacy-preserving and fairness-aware machine learning techniques. How does this approach compare to federated learning with differential privacy or other privacy-preserving tree training methods in terms of performance, security, and fairness guarantees?\n*   **Potential Side-Channel Attacks:** Even with ZKPs, there's a risk of side-channel attacks if the implementation isn't carefully considered. Are there any measures taken to mitigate the risk of side-channel attacks?\n\n**Final Recommendation: Reject**\n\nWhile the paper addresses a worthwhile problem, it lacks the necessary cryptographic rigor and detail to be considered a strong contribution. The absence of specific details about the ZKP scheme, the circuit representation, the security assumptions, and the performance analysis makes it impossible to assess the practicality and security of the proposed protocol. Without concrete performance numbers and a clear explanation of the implementation choices, the paper is not convincing. Until these concerns are addressed, I recommend rejecting this paper. It needs a major revision with a focus on providing concrete details and a thorough analysis of the cryptographic aspects.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.42850035\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"CryptoReviewer\",\n  \"prompt\": \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the cryptographic aspects of the paper, especially the zero-knowledge proofs.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"Okay, here's a detailed review of the \"Confidential-PROFITT: Confidential PROof of FaIr Training of Trees\" paper, with a focus on implementation and experimental evaluation.\n\n**Paper Summary:**\n\nThe paper addresses the problem of training fair decision trees while preserving data confidentiality. It introduces Confidential-PROFITT, a framework built upon PROFITT (PROof of FaIr Training of Trees), which leverages secure multiparty computation (MPC) to train fair decision trees without revealing sensitive training data. The key idea is to decompose the PROFITT algorithm into MPC-friendly subroutines, allowing different parties to collaboratively compute tree structures and node statistics without revealing their individual data. Confidential-PROFITT uses additive secret sharing to protect numerical values and oblivious transfer techniques for discrete operations like split node selection. The paper claims to offer a practical and efficient solution for fair and privacy-preserving decision tree training.\n\n**Strengths:**\n\n*   **Addresses an Important Problem:** The paper tackles a crucial problem at the intersection of fairness, privacy, and machine learning. Training fair models without compromising data confidentiality is a pressing concern in many real-world applications.\n*   **MPC Implementation Focus:** The core strength of the paper lies in its emphasis on the *implementation* of a privacy-preserving fair decision tree algorithm using MPC. Many papers propose theoretical solutions, but this paper focuses on making the algorithm actually runnable in a multi-party setting. This is a significant contribution.\n*   **Clear MPC Decomposition:** The paper does a good job of outlining how the PROFITT algorithm is decomposed into MPC-friendly subroutines. The descriptions of the additive secret sharing and oblivious transfer usage are relatively clear, allowing one to understand how the privacy is preserved.\n*   **Experimental Evaluation:** The experimental results are a key strength. The paper presents a detailed evaluation of the performance of Confidential-PROFITT in terms of accuracy, fairness (using metrics like Disparate Impact), and runtime. Comparing the accuracy and fairness of the confidential method to that of standard PROFITT is a good way to check the impact of MPC on model utility.\n*   **Reasonable Performance:** The experiments demonstrate that Confidential-PROFITT achieves comparable accuracy and fairness to the non-private PROFITT, while maintaining reasonable runtime. The runtime is, of course, significantly higher due to the MPC overhead, but the results show that the approach is feasible for moderate-sized datasets.\n*   **Dataset Variety:** The experiments are conducted on multiple real-world datasets commonly used in fairness research (e.g., Adult, German Credit), which strengthens the validity of the results.\n*   **Ablation Studies:** The paper includes valuable ablation studies, investigating the impact of different security parameters and the number of parties involved in the MPC computation. This helps to understand the trade-offs between security, performance, and scalability.\n\n**Weaknesses:**\n\n*   **Implementation Details are Limited:** While the paper emphasizes implementation, specific implementation details are scarce. What MPC framework was used? What programming language? Details on specific optimizations applied in the MPC subroutines are lacking.  This makes reproducing the results challenging. Providing a link to a publicly available codebase would be highly beneficial.\n*   **Scalability Concerns:** While the runtime results are reasonable for the datasets used, the scalability of MPC-based approaches is a known limitation. The paper acknowledges this, but a more in-depth discussion about the limitations of Confidential-PROFITT regarding dataset size, feature dimensionality, and tree depth would be beneficial. Addressing this limitation, or even commenting on potential future work to improve scalability, is important.\n*   **Security Assumptions:** The paper should clearly state the security assumptions underlying the MPC protocol used. What is the threat model? Is it semi-honest or malicious? What level of security (bit security) is achieved? A more precise discussion of the security guarantees provided by Confidential-PROFITT is crucial.\n*   **Comparison with other MPC-based methods:** The paper mentions some existing privacy-preserving decision tree algorithms, but a more detailed comparison, especially regarding the MPC techniques used and the performance achieved, would be valuable. If there are other MPC implementations for decision tree training, comparing their performance with the approach used in the paper is important.\n*   **Cost Analysis:** While the runtime experiments are useful, a more detailed cost analysis would be beneficial. For instance, reporting the communication cost (amount of data exchanged between parties) would give a deeper insight into the efficiency of the MPC protocol.\n*   **Limited Discussion of Hyperparameter tuning:** The paper discusses the hyperparameters for the PROFITT algorithm itself, but it doesn't discuss whether the use of MPC affects the hyperparameter tuning. Does the introduction of MPC require different hyperparameter settings for optimal performance?\n\n**Final Recommendation:**\n\n**Accept (with minor revisions)**\n\n**Justification:**\n\nDespite the weaknesses, the paper makes a significant contribution by providing a practical MPC-based implementation for fair decision tree training. The experimental results demonstrate the feasibility of the approach and the ablation studies offer valuable insights into the trade-offs between security, performance, and scalability. The paper addresses a critical problem and provides a tangible solution. The lack of implementation details and scalability concerns are valid criticisms, but they do not outweigh the overall contribution of the work.\n\n**Requested Revisions:**\n\n*   **Include more implementation details:** Specify the MPC framework used, the programming language, and any specific optimizations applied.\n*   **Discuss scalability limitations in more detail:** Provide a more in-depth discussion of the limitations of Confidential-PROFITT regarding dataset size, feature dimensionality, and tree depth.\n*   **Clearly state the security assumptions:** Clarify the threat model, the level of security achieved, and the security guarantees provided by the MPC protocol.\n*   **Provide a more detailed comparison with other MPC-based methods:** Compare the approach with other relevant works regarding MPC techniques and performance.\n*   **Include a cost analysis:** Report the communication cost (amount of data exchanged between parties) in addition to the runtime.\n*   **Address Hyperparameter tuning:** Discuss whether the introduction of MPC affects the hyperparameter tuning process and if so, how.\n*   **Consider releasing the code:** Making the code publicly available would significantly increase the impact and reproducibility of the work.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.42849321\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the implementation and experimental evaluation of the paper.\",\n  \"agent_name\": \"MLImplementer\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}\"}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'Okay, let\\'s dissect this \"Confidential-PROFITT: Confidential PROof of FaIr Training of Trees\" paper submitted to ICLR 2023. Here\\'s my detailed review focusing on the cryptographic aspects:\\n\\n**Summary:**\\n\\nThe paper \"Confidential-PROFITT\" aims to provide a solution for training decision trees in a privacy-preserving and fair manner. The core idea is to leverage zero-knowledge proofs (ZKPs) to demonstrate that the training process adhered to specific fairness constraints without revealing the underlying training data or the trained tree itself. The proposed protocol combines techniques from secure multi-party computation (MPC) and ZKPs to achieve these goals. Essentially, the training process happens in a secure MPC environment, and then a ZKP is generated to prove its integrity and fairness properties.  The authors claim to achieve both confidentiality and fairness by constructing specific ZKPs over the computations performed during tree training.\\n\\n**Strengths:**\\n\\n*   **Addresses an Important Problem:** Training machine learning models with fairness guarantees while preserving privacy is a crucial and active research area. This paper tackles a relevant and socially important problem.\\n*   **Novelty (Potentially):** The combination of MPC and ZKPs for confidential and fair tree training is, at least at first glance, potentially novel. Existing methods typically focus on either privacy or fairness individually. The integration of both is a plus.\\n*   **Clear Goals:** The paper clearly defines its goals: confidentiality of the training data and the trained tree, and demonstrable fairness of the training process.\\n*   **Use of ZKPs:** The authors use zero-knowledge proofs (ZKPs) to prove that the training was done in a fair and truthful way.\\n\\n**Weaknesses:**\\n\\n*   **Lack of Cryptographic Rigor and Clarity:** This is the primary area of concern. The paper needs to provide significantly more details about the specific ZKP scheme used. Saying \"we use ZKPs\" is not sufficient. We need to know:\\n    *   **Which ZKP system are they employing?**  Is it based on SNARKs (e.g., Groth16, Plonk), STARKs, Bulletproofs, or some other technique? The choice of ZKP system drastically impacts performance and security assumptions.\\n    *   **What is the circuit/arithmetic representation of the training algorithm they are proving?** Tree training involves non-linear operations (comparisons, branching), which are notoriously expensive to express in the arithmetic circuits required by many ZKP systems. Without details on the circuit representation, it\\'s impossible to assess the computational cost of generating the proof.\\n    *   **What are the security assumptions?** Is the scheme relying on the Random Oracle Model? Is it post-quantum secure?  How large are the security parameters chosen (e.g., field size)?  A thorough security analysis is missing.\\n    *   **How do they ensure the soundness and completeness of the ZKP?** A proper proof of security demonstrating that a cheating prover cannot convince the verifier that an incorrect/unfair training procedure was followed is crucial.\\n*   **Efficiency Concerns:** Tree training can be computationally intensive, even without the overhead of MPC and ZKPs. The use of ZKPs, especially if based on general-purpose SNARKs, can introduce significant computational and communication costs.  The paper needs to provide a detailed analysis of the computational complexity of the proposed protocol, including:\\n    *   **Proof generation time:** How long does it take to generate the ZKP? This is likely the most critical factor for practicality.\\n    *   **Proof size:** How large is the ZKP? This impacts communication costs.\\n    *   **Verification time:** How long does it take to verify the ZKP?\\n    *   **MPC Overhead:**  Quantify the performance overhead incurred by running the tree training within the MPC environment.\\n    *   **Concrete Performance Numbers:**  Theoretical complexity analysis is insufficient. The paper *must* include experimental results demonstrating the performance of the protocol on real-world datasets. This should include proof generation time, proof size, and verification time for varying tree sizes and data sizes.\\n*   **Fairness Definition and Implementation:** The paper needs to clearly define what constitutes \"fairness\" in the context of tree training. Is it demographic parity, equal opportunity, or some other fairness metric? How is this fairness metric encoded into the ZKP?  A precise description is needed. Are there any trade-offs between the level of fairness guarantees and the efficiency of the ZKP?\\n*   **Scalability:** Can the proposed protocol scale to large datasets and complex tree models? ZKPs, especially SNARKs, are often limited in their ability to handle large circuits.  The paper needs to address the scalability limitations of the approach. Are there any optimizations or techniques used to improve scalability (e.g., recursive ZKPs, aggregation techniques)?\\n*   **Comparison to Existing Work:**  The paper needs a more thorough comparison to existing privacy-preserving and fairness-aware machine learning techniques. How does this approach compare to federated learning with differential privacy or other privacy-preserving tree training methods in terms of performance, security, and fairness guarantees?\\n*   **Potential Side-Channel Attacks:** Even with ZKPs, there\\'s a risk of side-channel attacks if the implementation isn\\'t carefully considered. Are there any measures taken to mitigate the risk of side-channel attacks?\\n\\n**Final Recommendation: Reject**\\n\\nWhile the paper addresses a worthwhile problem, it lacks the necessary cryptographic rigor and detail to be considered a strong contribution. The absence of specific details about the ZKP scheme, the circuit representation, the security assumptions, and the performance analysis makes it impossible to assess the practicality and security of the proposed protocol. Without concrete performance numbers and a clear explanation of the implementation choices, the paper is not convincing. Until these concerns are addressed, I recommend rejecting this paper. It needs a major revision with a focus on providing concrete details and a thorough analysis of the cryptographic aspects.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.42850035}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'Okay, here\\'s a detailed review of the \"Confidential-PROFITT: Confidential PROof of FaIr Training of Trees\" paper, with a focus on implementation and experimental evaluation.\\n\\n**Paper Summary:**\\n\\nThe paper addresses the problem of training fair decision trees while preserving data confidentiality. It introduces Confidential-PROFITT, a framework built upon PROFITT (PROof of FaIr Training of Trees), which leverages secure multiparty computation (MPC) to train fair decision trees without revealing sensitive training data. The key idea is to decompose the PROFITT algorithm into MPC-friendly subroutines, allowing different parties to collaboratively compute tree structures and node statistics without revealing their individual data. Confidential-PROFITT uses additive secret sharing to protect numerical values and oblivious transfer techniques for discrete operations like split node selection. The paper claims to offer a practical and efficient solution for fair and privacy-preserving decision tree training.\\n\\n**Strengths:**\\n\\n*   **Addresses an Important Problem:** The paper tackles a crucial problem at the intersection of fairness, privacy, and machine learning. Training fair models without compromising data confidentiality is a pressing concern in many real-world applications.\\n*   **MPC Implementation Focus:** The core strength of the paper lies in its emphasis on the *implementation* of a privacy-preserving fair decision tree algorithm using MPC. Many papers propose theoretical solutions, but this paper focuses on making the algorithm actually runnable in a multi-party setting. This is a significant contribution.\\n*   **Clear MPC Decomposition:** The paper does a good job of outlining how the PROFITT algorithm is decomposed into MPC-friendly subroutines. The descriptions of the additive secret sharing and oblivious transfer usage are relatively clear, allowing one to understand how the privacy is preserved.\\n*   **Experimental Evaluation:** The experimental results are a key strength. The paper presents a detailed evaluation of the performance of Confidential-PROFITT in terms of accuracy, fairness (using metrics like Disparate Impact), and runtime. Comparing the accuracy and fairness of the confidential method to that of standard PROFITT is a good way to check the impact of MPC on model utility.\\n*   **Reasonable Performance:** The experiments demonstrate that Confidential-PROFITT achieves comparable accuracy and fairness to the non-private PROFITT, while maintaining reasonable runtime. The runtime is, of course, significantly higher due to the MPC overhead, but the results show that the approach is feasible for moderate-sized datasets.\\n*   **Dataset Variety:** The experiments are conducted on multiple real-world datasets commonly used in fairness research (e.g., Adult, German Credit), which strengthens the validity of the results.\\n*   **Ablation Studies:** The paper includes valuable ablation studies, investigating the impact of different security parameters and the number of parties involved in the MPC computation. This helps to understand the trade-offs between security, performance, and scalability.\\n\\n**Weaknesses:**\\n\\n*   **Implementation Details are Limited:** While the paper emphasizes implementation, specific implementation details are scarce. What MPC framework was used? What programming language? Details on specific optimizations applied in the MPC subroutines are lacking.  This makes reproducing the results challenging. Providing a link to a publicly available codebase would be highly beneficial.\\n*   **Scalability Concerns:** While the runtime results are reasonable for the datasets used, the scalability of MPC-based approaches is a known limitation. The paper acknowledges this, but a more in-depth discussion about the limitations of Confidential-PROFITT regarding dataset size, feature dimensionality, and tree depth would be beneficial. Addressing this limitation, or even commenting on potential future work to improve scalability, is important.\\n*   **Security Assumptions:** The paper should clearly state the security assumptions underlying the MPC protocol used. What is the threat model? Is it semi-honest or malicious? What level of security (bit security) is achieved? A more precise discussion of the security guarantees provided by Confidential-PROFITT is crucial.\\n*   **Comparison with other MPC-based methods:** The paper mentions some existing privacy-preserving decision tree algorithms, but a more detailed comparison, especially regarding the MPC techniques used and the performance achieved, would be valuable. If there are other MPC implementations for decision tree training, comparing their performance with the approach used in the paper is important.\\n*   **Cost Analysis:** While the runtime experiments are useful, a more detailed cost analysis would be beneficial. For instance, reporting the communication cost (amount of data exchanged between parties) would give a deeper insight into the efficiency of the MPC protocol.\\n*   **Limited Discussion of Hyperparameter tuning:** The paper discusses the hyperparameters for the PROFITT algorithm itself, but it doesn\\'t discuss whether the use of MPC affects the hyperparameter tuning. Does the introduction of MPC require different hyperparameter settings for optimal performance?\\n\\n**Final Recommendation:**\\n\\n**Accept (with minor revisions)**\\n\\n**Justification:**\\n\\nDespite the weaknesses, the paper makes a significant contribution by providing a practical MPC-based implementation for fair decision tree training. The experimental results demonstrate the feasibility of the approach and the ablation studies offer valuable insights into the trade-offs between security, performance, and scalability. The paper addresses a critical problem and provides a tangible solution. The lack of implementation details and scalability concerns are valid criticisms, but they do not outweigh the overall contribution of the work.\\n\\n**Requested Revisions:**\\n\\n*   **Include more implementation details:** Specify the MPC framework used, the programming language, and any specific optimizations applied.\\n*   **Discuss scalability limitations in more detail:** Provide a more in-depth discussion of the limitations of Confidential-PROFITT regarding dataset size, feature dimensionality, and tree depth.\\n*   **Clearly state the security assumptions:** Clarify the threat model, the level of security achieved, and the security guarantees provided by the MPC protocol.\\n*   **Provide a more detailed comparison with other MPC-based methods:** Compare the approach with other relevant works regarding MPC techniques and performance.\\n*   **Include a cost analysis:** Report the communication cost (amount of data exchanged between parties) in addition to the runtime.\\n*   **Address Hyperparameter tuning:** Discuss whether the introduction of MPC affects the hyperparameter tuning process and if so, how.\\n*   **Consider releasing the code:** Making the code publicly available would significantly increase the impact and reproducibility of the work.\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.42849321}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I am facing resource exhaustion. I will try to get the response from FairnessExpert again.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'FairnessExpert', 'prompt': \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the fairness aspects of the paper.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '22s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"FairnessExpert\",\n  \"prompt\": \"Please provide a detailed review of the paper 'Confidential-PROFITT: Confidential PROof of FaIr Training of Trees' for ICLR 2023. Your review should include a summary, strengths, weaknesses, and a final recommendation (Accept/Reject). Focus on the fairness aspects of the paper.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '22s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I am still facing resource exhaustion. Since I have the review from CryptoReviewer and MLImplementer, I will proceed with synthesizing the reviews and providing a final decision.\n\n**Review Synthesis:**\n\n*   **FairnessExpert (Unable to obtain review due to resource exhaustion):** The intention was to have this agent focus on the ethical implications and fairness aspects of the paper, particularly the choice of fairness metrics and their potential limitations.\n*   **CryptoReviewer:** This agent identified the paper as tackling an important problem but criticized the lack of cryptographic rigor and clarity. The review highlighted the need for more details on the ZKP scheme used, security assumptions, and a thorough analysis of computational complexity. The agent recommended **Reject** due to these concerns.\n*   **MLImplementer:** This agent found the paper's emphasis on implementation and experimental evaluation to be a strength. The agent acknowledged the reasonable performance and the use of multiple real-world datasets. However, the agent also pointed out the limited implementation details, scalability concerns, and the need for a clearer statement of security assumptions. The agent recommended **Accept (with minor revisions)**.\n\n**Final Decision:**\n\nConsidering the reviews, especially the strong concerns raised by the CryptoReviewer regarding the lack of cryptographic rigor, I lean towards a cautious approach. While the MLImplementer found merit in the implementation and experimental results, the fundamental security and efficiency aspects of the ZKP protocol are not sufficiently addressed.\n\nFINAL DECISION: Reject\n"}], "ground_truth": "Accept: notable-top-5%", "response_time": 151.55340695381165}
