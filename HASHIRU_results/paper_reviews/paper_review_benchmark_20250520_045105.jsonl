{"paper_id": "xTWoeTdHgH-", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Evaluating Unsupervised Denoising Requires Unsupervised Metrics\n\n1 INTRODUCTION: Image denoising is a fundamental challenge in image and signal processing, as well as a key preprocessing step for computer vision tasks. Convolutional neural networks achieve state-of-the-art performance for this problem, when trained using databases of clean images corrupted with simulated noise Zhang et al. (2017a). However, in real-world imaging applications such as microscopy, noiseless ground truth videos are often not available. This has motivated the development of unsupervised denoising approaches that can be trained using only noisy measurements Lehtinen et al. (2018); Xie et al. (2020); Laine et al. (2019); Sheth et al. (2021); Huang et al. (2021). These methods have demonstrated impressive performance on natural-image benchmarks, essentially on par with the supervised state of the art. However, to the best of our knowledge, no unsupervised metrics are currently available to evaluate them using only noisy data. Reliance on supervised metrics makes it very challenging to create benchmark datasets using realworld measurements, because obtaining the ground-truth clean images required by these metrics is often either impossible or very constraining. In practice, clean images are typically estimated through temporal averaging, which suppresses dynamic information that is often crucial in scientific applications. Consequently, quantitative evaluation of unsupervised denoising methods is currently almost completely dominated by natural image benchmark datasets with simulated noise Lehtinen et al. (2018); Xie et al. (2020); Laine et al. (2019); Sheth et al. (2021); Huang et al. (2021), which are not always representative of the signal and noise characteristics that arise in real-world imaging applications. The lack of unsupervised metrics also limits the application of unsupervised denoising techniques in practice. In the absence of quantitative metrics, domain scientists must often rely on visual inspection to evaluate performance on real measurements. This is particularly restrictive for deep-learning approaches, because it makes it impossible to perform systematic hyperparameter optimization and model selection on the data of interest. In this work, we propose two novel unsupervised metrics to address these issues: the unsupervised mean-squared error (uMSE) and the unsupervised peak signal-to-noise ratio (uPSNR), which are computed exclusively from noisy data. These metrics build upon existing unsupervised denoising methods, which minimize an unsupervised cost function equal to the difference between the denoised estimate and additional noisy copies of the signal of interest Lehtinen et al. (2018). The uMSE is equal to this cost function modified with a correction term, which renders it an unbiased estimator of the supervised MSE. We provide a theoretical analysis of the uMSE and uPSNR, proving that they are asymptotically consistent estimators of the supervised MSE and PSNR respectively. Controlled numerical experiments on supervised benchmarks, where the true MSE and PSNR can be computed exactly, confirm that the uMSE and uPSNR provide accurate approximations in practice. In addition, we validate the metrics on a dataset of videos in RAW format, contaminated with real noise Yue et al. (2020). In order to illustrate the potential impact of the proposed metrics on imaging applications where no ground-truth is available, we apply them to transmission-electron-microscopy (TEM) data. Recent advances in direct electron detection systems make it possible for experimentalists to acquire highly time-resolved movies of dynamic events at frame rates in the kilohertz range Faruqi & McMullan (2018); Ercius et al. (2020), which is critical to advance our understanding of functional materials. Acquisition at such high temporal resolution results in severe degradation by shot noise. We show that unsupervised methods based on deep learning can be effective in removing this noise, and that our proposed metrics can be used to evaluate their performance quantitatively using only noisy data. To summarize, our contributions are (1) two novel unsupervised metrics presented in Section 3, (2) a theoretical analysis providing an asymptotic characterization of their statistical properties (Section 4), (3) experiments showing the accuracy of the metrics in a controlled situation where ground-truth clean images are available (Section 5), (4) validation on real-world videos in RAW format (Section 6), and (5) an application to a real-world electron-microscopy dataset, which illustrates the challenges of unsupervised denoising in scientific imaging (Section 7). 2 BACKGROUND AND RELATED WORK: Unsupervised denoising The past few years have seen ground-breaking progress in unsupervised denoising, pioneered by Noise2Noise, a technique where a neural network is trained on pairs of noisy images corresponding to the same clean image Lehtinen et al. (2018). Our unsupervised metrics are inspired by Noise2Noise, which optimizes a cost function equal to our proposed unsupervised MSE, but without a correction term (which is not needed for training models). Subsequent work focused on performing unsupervised denoising from single images using variations of the blind-spot method, where a model is trained to estimate each noisy pixel value using its neighborhood but not the noisy pixel itself (to avoid the trivial identity solution) Krull et al. (2019); Laine et al. (2019); Batson & Royer (2019); Sheth et al. (2021); Xie et al. (2020), or Stein\u2019s unbiased risk estimator of the MSE (in the case of additive Gaussian noise) Raphan & Simoncelli (2008); Metzler et al. (2018); Soltanayev & Chun (2018); Mohan et al. (2021). More recently, Neighbor2Neighbor revisited the Noise2Noise method, generating noisy image pairs from a single noisy image via spatial subsampling Huang et al. (2021), an insight that can also be leveraged in combination with our proposed metrics, as explained in Section B. Existing evaluation approaches In the literature, quantitative evaluation of unsupervised denoising techniques has mostly relied on images and videos corrupted with synthetic noise Lehtinen et al. (2018); Krull et al. (2019); Laine et al. (2019); Batson & Royer (2019); Sheth et al. (2021); Xie et al. (2020). Recently, a few datasets containing real noisy data have been created Abdelhamed et al. (2018); Plotz & Roth (2017); Xu et al. (2018); Zhang et al. (2019). Evaluation on these datasets is based on supervised MSE and PSNR computed from estimated clean images obtained by averaging multiple noisy frames. Unfortunately, as a result, the metrics cannot capture dynamically-changing features, which are of interest in many applied domains. In addition, unless the signal-to-noise ratio is quite high, it is necessary to average over a large number of frames to accurately approximate the supervised MSE. For example, as explained in Section C, for CNN based denoiser and an image corrupted by additive Gaussian noise with standard deviation \u03c3 = 15 we need to average > 1500 noisy images to achieve the same approximation accuracy as our proposed approach (see Figure 9), which only requires 3 noisy images, and can also be computed from a single noisy image (via spatial subsampling, as explained in Section B). 3 UNSUPERVISED METRICS FOR UNSUPERVISED DENOISING:  3.1 THE UNSUPERVISED MEAN SQUARED ERROR: The goal of denoising is to estimate a clean signal from noisy measurements. Let x \u2208 Rn be a signal or a set of signals with n total entries. We denote the corresponding noisy data by y \u2208 Rn. A denoiser f : Rn \u2192 Rn is a function that maps the input y to an estimate of x. A common metric to evaluate the quality of a denoiser is the mean squared error between the clean signal and the estimate, MSE := 1 n n\u2211 i=1 (xi \u2212 f(y)i)2 . (1) Unfortunately, in most real-world scenarios clean ground-truth signals are not available and evaluation can only be carried out in an unsupervised fashion, i.e. exclusively from the noisy measurements. In this section we propose an unsupervised estimator of MSE inspired by recent advances in unsupervised denoising Lehtinen et al. (2018). The key idea is to compare the denoised signal to a noisy reference, which corresponds to the same clean signal corrupted by independent noise. In order to motivate our approach, let us assume that the noise is additive, so that y := x + z for a zero-mean noise vector z \u2208 Rn. Imagine that we have access to a noisy reference a := x + w corresponding to the same underlying signal x, but corrupted with a different noise realization w \u2208 Rn independent from z (Section 3.3 explains how to obtain such references in practice). The mean squared difference between the denoised estimate and the reference is approximately equal to the sum of the MSE and the variance \u03c32 of the noise, 1 n n\u2211 i=1 (ai \u2212 f(y)i)2 = 1 n n\u2211 i=1 (xi + wi \u2212 f(y)i)2 (2) \u2248 1 n n\u2211 i=1 (xi \u2212 f(y)i)2 + 1 n n\u2211 i=1 w2i \u2248 MSE+ \u03c32, (3) because the cross-term 1n \u2211n i=1 wi (xi \u2212 f(y)i) 2 cancels out if wi and yi (and hence f(yi)) are independent (and the mean of the noise is zero). Approximations to equation 3 are used by different unsupervised methods to train neural networks for denoising Lehtinen et al. (2018); Xie et al. (2020); Laine et al. (2019); Huang et al. (2021). The noise term 1n \u2211n i=1 w 2 i in equation 3 is not problematic for training denoisers as long as it is independent from the input y. However, it is definitely problematic for evaluating denoisers. In order to estimate the MSE we need to cancel it out. We propose to achieve this by using two other noisy references b := x + v and c := x + u, which are noisy measurements corresponding to the clean signal x, but corrupted with different, independent noise realizations v and u (just like a). Subtracting these references and dividing by two yields an estimate of the noise variance, 1 n n\u2211 i=1 (bi \u2212 ci)2 2 = 1 n n\u2211 i=1 (vi \u2212 ui)2 2 \u2248 1 2n n\u2211 i=1 v2i + 1 2n n\u2211 i=1 u2i \u2248 \u03c32, (4) which can then be subtracted from equation 3 to estimate the MSE. This yields our proposed unsupervised metric, which we call unsupervised mean squared error (uMSE), depicted in Figure 1. Definition 1 (Unsupervised mean squared error). Given a noisy input signal y \u2208 Rn and three noisy references a, b, c \u2208 Rn the unsupervised mean squared error of a denoiser f : Rn \u2192 Rn is uMSE := 1 n n\u2211 i=1 (ai \u2212 f(y)i)2 \u2212 (bi \u2212 ci)2 2 . (5) Theorem 4 in Section 4 establishes that the uMSE is a consistent estimator of the MSE as long as (1) the noisy input and the noisy references are independent, (2) their means equal the corresponding entries of the ground-truth clean signal, and (3) their higher-order moments are bounded. These conditions are satisfied by most noise models of interest in signal and image processing, such as Poisson shot noise or additive Gaussian noise. In Section 3.3 we address the question of how to obtain the noisy references required to estimate the uMSE. Section A explains how to compute confidence intervals for the uMSE via bootstrapping. 3.2 THE UNSUPERVISED PEAK SIGNAL-TO-NOISE RATIO: Peak signal-to-noise ratio (PSNR) is currently the most popular metric to evaluate denoising quality. It is a logarithmic function of MSE defined on a decibel scale, PSNR := 10 log ( M2 MSE ) , (6) where M is a fixed constant representing the maximum possible value of the signal of interest, which is usually set equal to 255 for images. Our definition of uMSE can be naturally extended to yield an unsupervised PSNR (uPSNR). Definition 2 (Unsupervised peak signal-to-noise ratio). Given a noisy input signal y \u2208 Rn and three noisy references a, b, c \u2208 Rn the peak signal-to-noise ratio of a denoiser f : Rn \u2192 Rn is uPSNR := 10 log ( M2 uMSE ) , (7) where M is the maximum possible value of the signal of interest. Corollary 5 establishes that the uPSNR is a consistent estimator of the PSNR, under the same conditions that guarantee consistency of the uMSE. Section A explains how to compute confidence intervals for the uPSNR via bootstrapping. 3.3 COMPUTING NOISY REFERENCES IN PRACTICE: Our proposed metrics rely on the availability of three noisy references, which ideally should correspond to the same clean image contaminated with independent noise. Deviations between the clean signal in each reference violate Condition 2 in Section 4, and therefore introduce a bias in the proposed metrics. The references should therefore be constructed to minimize this deviation. We propose two approaches to compute the references in practice, illustrated in Figure 2. 1. Using consecutive frames acquired within a short time interval is appropriate if there is no dynamic content that changes from frame to frame. We apply this approach to the RAW videos in Section 6, where the content is static. 2. Applying spatial subsampling to a single image, as described in Section B. We apply this approach to the electron-microscopy data in Section 7, where preserving dynamic content is important. We study the bias introduced by spatial subsampling in Figure 4 (see also Figure 8). 4 STATISTICAL PROPERTIES OF THE PROPOSED METRICS: In this section, we establish that the proposed unsupervised metrics provide a consistent estimate of the MSE and PSNR. In our analysis, the ground truth signal or set of signals is represented as a deterministic vector x \u2208 Rn. The corresponding noisy data are also modeled as a deterministic vector y \u2208 Rn that is fed into a denoiser f : Rn \u2192 Rn to produce the denoised estimate f(y). The MSE of the estimate is a deterministic quantity equal to MSE := 1 n n\u2211 i=1 SEi, SEi := (xi \u2212 f(y)i)2 . (8) The uMSE estimator in Definition 1 depends on three noisy references a\u0303, b\u0303, c\u0303, which we model as random variables.1 Our analysis assumes that these random variables satisfy two conditions: Condition 1 (independence): The entries of a\u0303, b\u0303, c\u0303 are all mutually independent. Condition 2 (centered noise): The mean of the ith entry of a\u0303, b\u0303, c\u0303 equals the corresponding entry of the clean signal, E [a\u0303i] = E[b\u0303i] = E [c\u0303i] = xi, 1 \u2264 i \u2264 n. Two popular noise models that satisfy these conditions are: \u2022 Additive Gaussian, where a\u0303i := xi + w\u0303i, b\u0303i := xi + v\u0303i, c\u0303i := xi + u\u0303i, for i.i.d. Gaussian w\u0303i, v\u0303i, u\u0303i. \u2022 Poisson, where a\u0303i, b\u0303i, c\u0303i are i.i.d. Poisson random variables with parameter xi. Our goal is to study the statistical properties of the uMSE computed from the noisy references, u\u0303MSE := 1 n n\u2211 i=1 u\u0303SEi, u\u0303SEi := (a\u0303i \u2212 f(y)i)2 \u2212 (b\u0303i \u2212 c\u0303i)2 2 . (9) As indicated by the tilde, under our modeling assumptions, the uMSE is a random variable. We first show that the correction factor in the definition of uMSE succeeds in debiasing the estimator, so that its mean is equal to the MSE. Theorem 3 (The uMSE is unbiased, proof in Section E.1). If Conditions 1 and 2 hold, the uMSE is an unbiased estimator of the MSE, i.e. E [ u\u0303MSE ] = MSE. Theorem 3 establishes that the distribution of the uMSE is centered at the MSE. We now show that its variance around the MSE shrinks at a rate inversely proportional to the number of signal entries n, and therefore converges to the MSE in mean square and probability as n \u2192 \u221e (see Figure 3 for a numerical demonstration). This occurs as long as the higher central moments of noise and the entrywise denoising error are bounded by a constant, which is to be expected in most realistic scenarios. Theorem 4 (The uMSE is consistent, proof in Section E.2). Let \u00b5[k]i denote the kth central moment of a\u0303i, b\u0303i, c\u0303i, and \u03b3 := max1\u2264i\u2264n |xi \u2212 f(y)i| the maximum entrywise denoising error. If Conditions 1 and 2 hold, and there exists a constant \u03b1 such that max1\u2264i\u2264n max { \u00b5 [4] i , \u00b5 [3] i \u03b3, \u03b3 4 } \u2264 \u03b1, then the 1In our analysis, all random quantities are marked with a tilde for clarity. Natural images (Gaussian noise):  Electron microscopy (Poisson noise):  20.18 20.20 20.21 24.86 24.87 24.74 25.74 25.68 25.86 24.65 24.69 24.79: mean squared error between the MSE and the uMSE satisfies the bound E [( u\u0303MSE\u2212MSE )2] \u2264 \u03b1 n . (10) Consequently, limn\u2192\u221e E[(u\u0303MSE\u2212MSE)2] = 0, so the uMSE converges to the MSE in mean square and therefore also in probability. Consistency of the uMSE implies consistency of the uPSNR. Corollary 5 (The uPSNR is consistent, proof in Section E.3). Under the assumptions of Theorem 4, the uPSNR defined as u\u0303PSNR := 10 log ( M2 u\u0303MSE ) , (11) where M is a fixed constant, converges in probability to the PSNR, as n \u2192 \u221e. The following theorem establishes that the uMSE converges to a Gaussian random variable asymptotically as n \u2192 \u221e. Theorem 6 (The uMSE is asymptotically normally distributed, proof in Section E.4). If the first six central moments of a\u0303i, b\u0303i, c\u0303i and the maximum entrywise denoising error max1\u2264i\u2264n |xi \u2212 f(y)i| are bounded, and Conditions 1 and 2 hold, the uMSE is asymptotically normally distributed as n \u2192 \u221e. Our numerical experiments show that the distribution of the uMSE is well approximated as Gaussian even for relatively small values of n (see Figure 3). This can be exploited to build confidence intervals for the uMSE and uPSNR, as explained in Section A. 5 CONTROLLED EVALUATION OF THE PROPOSED METRICS: In this section, we study the properties of the uMSE and uPSNR through numerical experiments in a controlled scenario where the ground-truth clean images are known. We use a dataset of natural images (Martin et al., 2001; Zhang et al., 2017b; Franzen, 1993) corrupted with additive Gaussian noise, and a dataset of simulated electron-microscopy images Vincent et al. (2021) corrupted with Poisson noise. For the two datasets, we compute the supervised MSE and PSNR using the groundtruth clean image. To compute the uMSE and uPSNR we use noisy references corresponding to the same clean image corrupted with independent noise. We also compute the uMSE and uPSNR using noisy references obtained from a single noisy image via spatial subsampling, as described in Section B, which we denote by uMSES and uPSNRS respectively.2 All metrics are applied to multiple denoising approches, as described in more detail in Section F. The results are reported in Tables 1 and 3. When the noisy references correspond exactly to the same clean image (and therefore satisfy the conditions in Section 4), the unsupervised metrics are extremely accurate across different noise levels for all denoising methods. When the noisy references are computed via spatial subsampling, the metrics are still very accurate for the electron-microscopy dataset, but less so for the natural-image dataset if the PSNR is high (above 20 dB). The culprit is the difference between the clean images corresponding to each noisy reference produced via spatial subsampling (see Figure 2), which introduces a bias in the unsupervised metric, depicted in Figure 4. Figure 8 shows that the difference is more pronounced in natural images than in electron-microscopy images, which are smoother and have higher spatial resolution. 6 APPLICATION TO VIDEOS IN RAW FORMAT: We evaluate our proposed metrics on a dataset of videos in raw format, consisting of direct readings from the sensor of a surveillance camera contaminated with real noise at five different ISO levels Yue et al. (2020). The dataset contains 11 unique videos divided into 7 segments, each consisting of 10 noisy frames that capture the same static object. We consider four different denoisers: a wavelet-based method, temporal averaging and two versions of a state-of-the-art unsupervised deep-learning method using images and videos respectively Sheth et al. (2021). A detailed description of the experiments is provided in Section G. Tables 4 and 2 compare our proposed unsupervised metrics (computed using three noisy frames in each segment) with MSE and PSNR estimates obtained via averaging from ten noisy frames. The two types of metric yield similar results: the deep-learning methods clearly outperform the other baselines, and the video-based methods outperform the image-based methods. As explained in Section C, the averaging-based MSE and PSNR are not consistent estimators of the true MSE and PSNR, and can be substantially less accurate than the uMSE and uPSNR (see Figure 9), so they should not be considered ground-truth metrics. 7 APPLICATION TO ELECTRON MICROSCOPY: Our proposed unsupervised metrics provide quantitative evaluation of denoising methods on data where no ground-truth clean images are available. We showcase this for transmission electron microscopy (TEM), a key imaging modality in material sciences. Recent developments enable the acquisition of high frame-rate images, capturing high temporal resolution dynamics, thought to be crucial in catalytic processes Crozier et al. (2019). Images acquired under these conditions are severely limited by noise. Recent work suggest that deep learning methods provide an effective 2The other metrics are applied to subsampled images in order to make them directly comparable to the uMSES and uPSNRS. solution Sheth et al. (2021); Mohan et al. (2022; 2021), but, instead of quantitative metrics, evaluation on real data has been limited to visual inspection. The TEM dataset consists of 18,597 noisy frames depicting platinum nanoparticles on a cerium oxide support. A major challenge for the application of unsupervised metrics is the presence of local correlations in the noise (see Figure 11). We address this by performing spatial subsampling to reduce the correlation and selecting two contiguous test sets with low correlation: 155 images with moderate signal-to-noise ratio (SNR), and 383 images with low SNR, which are more challenging. We train an unsupervised Neighbor2Neighbor convolutional-neural network Huang et al. (2021) on a training set containing 70% of the data, and compare its performance to a Gaussian-smoothing baseline on the two test sets. Section H provides a more detailed description of the dataset and the models. Figure 5 shows examples from the data and the corresponding denoised images, as well as the uPSNR of each method for the two test sets. Figure 10 shows a histogram of the uMSE values for each individual test image. The unsupervised metrics indicate that the deep-learning method achieves effective denoising on the moderate SNR set, clearly outperforming the Gaussian-smoothing baseline, and also that both methods produce significantly worse results for the low-SNR test set. These conclusions are consistent with the visual appearance of the images. 8 CONCLUSION AND OPEN QUESTIONS: In this work we introduce two novel unsupervised metrics computed exclusively from noisy data, which are asymptotically consistent estimators of the corresponding supervised metrics, and yield accurate approximations in practice. These results are limited to denoising under the assumption of independent noise. Several important open questions remain: (1) How to address the bias introduced by spatial subsampling and achieve an unbiased approximation to the MSE from a single noisy image. (2) How to extend the approach to noise distributions that are not pixel-wise independent. (3) How to obtain unsupervised approximations of perceptual metrics such as SSIM Wang et al. (2004). (4) How to perform unsupervised evaluation for more general inverse problems. A CONFIDENCE INTERVALS FOR UNCERTAINTY QUANTIFICATION: The uMSE and uPSNR are estimates of the MSE and PSNR computed from noisy data, so they are inherently uncertain. We propose to quantify this uncertainty using confidence intervals obtained via bootstrapping. Algorithm 7 (Bootstrap confidence intervals). We assume access to a noisy input signal y \u2208 Rn and three noisy references a, b, c \u2208 Rn. For 1 \u2264 k \u2264 K, build an index set Bk by sampling n entries from {1, 2, . . . , n} uniformly and independently at random with replacement. Then set uMSEk := 1 n \u2211 i\u2208Bk (ai \u2212 f(y)i)2 \u2212 (bi \u2212 ci)2 2 , uPSNRk := 10 log ( M2 uMSEk ) . (12) To build 1\u2212 \u03b1 confidence intervals, 0 < \u03b1 < 1 for the uMSE and uPSNR set: IuMSE := [ quMSE\u03b1/2 , q uMSE 1\u2212\u03b1/2 ] , IuPSNR := [ quPSNR\u03b1/2 , q uPSNR 1\u2212\u03b1/2 ] , (13) where quMSE\u03b1/2 and q uMSE 1\u2212\u03b1/2 are the \u03b1/2 and 1\u2212\u03b1/2 quantiles of the set {uMSE1, . . . ,uMSEK}, and quPSNR\u03b1/2 and q uPSNR 1\u2212\u03b1/2 are the \u03b1/2 and 1\u2212 \u03b1/2 quantiles of the set {uPSNR1, . . . ,uPSNRK}. Theorem 6 establishes that the uMSE is asymptotically normal. In addition, our numerical experiments show that the distribution of the uMSE is well approximated as Gaussian even for relatively small values of n (see Figure 3). As a result, the bootstrap confidence intervals for the uMSE produced by Algorithm 7 contain the MSE with probability approximately 1 \u2212 \u03b1 (see Section 13.3 in Efron & Tibshirani (1994)). This also implies that the PSNR belongs to the bootstrap confidence intervals for the uPSNR with probability approximately 1\u2212 \u03b1 because the function that maps the uMSE to the uPSNR and the MSE to the PSNR is monotone (see Section 13.6 in Efron & Tibshirani (1994)). Figure 6 shows a numerical verification that the proposed approach yields valid confidence intervals for MSE in the controlled experiments of Section 5, where the ground-truth clean images are known. It also shows that the bias introduced by spatial subsampling for natural images (see Section B), shifts the confidence intervals away from the true MSE. B SPATIAL SUBSAMPLING: In this section, we propose a method to obtain the noisy references required to estimate uMSE and uPSNR. We focus our discussion on images, but similar ideas can be applied to videos and time-series data. In order to simplify the notation, we consider N \u00d7N images. The n-dimensional signals in other sections can be interpreted as vectorized versions of these images with n = N2. We assume that we have available a noisy image I of dimensions 2N \u00d7 2N . We extract four noisy references from I by spatial subsampling. The method is inspired by the Neighbor2Neighbor unsupervised denoising method, which uses random subsampling to generate noisy image pairs during training Huang et al. (2021). Figure 7 illustrates the approach. Algorithm 8 (Decomposition via spatial subsampling). Given an image I \u2208 R2N\u00d72N , let S1(i, j) := I (2i\u2212 1, 2j \u2212 1) , S2(i, j) := I (2i, 2j \u2212 1) , S3(i, j) := I (2i\u2212 1, 2j) , S4(i, j) := I (2i, 2j) , 1 \u2264 i, j \u2264 n. (14) The spatial decomposition of I is equal to four sub-images Y , A, B, C \u2208 RN\u00d7N where Y (i, j), A(i, j), B(i, j), C(i, j) are set equal to S1(i, j), S2(i, j), S3(i, j), S4(i, j), or to a random permutation of the four values. Spatial subsampling generates four noisy sub-images that correspond to the noisy input y and the three noisy references a, b and c in Definition 1. In our derivation of the uMSE, we assume that these four noisy signals are generated by corrupting the same ground-truth clean signal with independent noise. This assumption holds for the sub-images in Definition 8 if (1) the underlying clean image is smooth, so that adjacent pixels are approximately equal, and (2) the noise is pixel-wise independent. Tables 1 and 3, and Figures 4, 6 and 8 show that these assumptions don\u2019t hold completely for natural images, which introduces a bias in the uMSE. This bias also exists for the electron-microscopy images but it is much smaller, because the images are smoother. C COMPARISON WITH AVERAGING-BASED MSE ESTIMATION: Existing denoising benchmarks containing images corrupted with real noise perform evaluation by computing the MSE or PSNR using an estimate of the clean image obtained by averaging multiple noisy frames Abdelhamed et al. (2018); Plotz & Roth (2017); Xu et al. (2018); Zhang et al. (2019). In this section, we show both theoretically and numerically that this approach produces a poor estimate of the MSE and PSNR, unless the signal-to-noise ratio of the data is very low, or we use a large number of noisy frames. The following lemma shows that in contrast to our proposed metric uMSE, the approximation to the MSE obtained via averaging is biased and not consistent, in the sense that it does not converge to the true MSE when the number of pixels tends to infinity. The metric does converge to the MSE as the number of noisy images tends to infinity, but this is of little practical significance, since this number cannot be increased arbitrarily in actual applications. Lemma 9 (MSE via averaging). Consider a clean signal x \u2208 Rn, an estimate f(y) \u2208 Rn (obtained by applying a denoiser f to the data y \u2208 Rn), and m noisy references r\u0303 [m] i := xi + z\u0303 [m] i , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m, (15) where z\u0303[m]i , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m, are i.i.d. zero-mean Gaussian random variables with variance \u03c32. We define the averaging-based MSE as MSEavg := 1 n n\u2211 i=1  1 m m\u2211 j=1 r\u0303 [m] i \u2212 f(y)i 2 . (16) The MSEavg is a biased estimator of the true MSE MSE := 1 n n\u2211 i=1 (xi \u2212 f(y)i)2 , (17) since its mean equals E [MSEavg] = MSE + \u03c32 m . (18) Proof. By the assumptions, and linearity of expectation, E [MSEavg] = E  1 n n\u2211 i=1  1 m m\u2211 j=1 z\u0303 [m] i + xi \u2212 f(y)i 2  (19) = 1 n n\u2211 i=1 (xi \u2212 f(y)i)2 + 1 n n\u2211 i=1 E   1 m m\u2211 j=1 z\u0303 [m] i 2  (20) = MSE + \u03c32 m . (21) As established in Section 4, the proposed uMSE metric is an unbiased estimator of the MSE that is consistent as n \u2192 \u221e and only requires m := 3 noisy references. Figure 9 shows a numerical comparison between uMSE and the averaging-based MSE for one of the natural images used in the experiments of Section 5. We observe that averaging-based MSE requires m := 1510 in order to match the accuracy achieved by the uMSE with only three noisy references. D ADDITIONAL RESULTS: This section contains additional results, which are not included in the main paper due to space constraints. They include: \u2022 Table 3 shows a controlled comparison of MSE and uMSE on clean ground-truth images, and noisy frames for both natural images with additive Gaussian noise and TEM images with Poisson noise. \u2022 Table 4 shows a comparison between averaging-based MSE and uMSE on RAW videos with real noise described in Sections 6 and G. \u2022 Figure 10 Shows the estimates of uMSE evaluated on TEM data for two denoisers, described in Section 7. \u2022 Figure 11 shows the empirical correlation of neighboring pixels in the TEM data, necessitating subsampling in order to evaluate uMSE, uPSNR. Electron microscopy (Poisson noise) \u00b710\u22123:  Natural images (Gaussian noise) \u00b710\u22123:  E PROOFS:  E.1 PROOF OF THEOREM 3: The following lemma shows that each individual term in the uMSE is unbiased. Lemma 10 (Proof in Section E.5.2). If Conditions 1 and 2 in Section 4 hold, E [ u\u0303SEi ] = SEi, 1 \u2264 i \u2264 n. (22) The proof then follows immediately from linearity of expectation, E [ u\u0303MSE ] = E [ 1 n n\u2211 i=1 u\u0303SEi ] = 1 n n\u2211 i=1 E [ u\u0303SEi ] = 1 n n\u2211 i=1 SEi = MSE. (23) E.2 PROOF OF THEOREM 4: The following lemma bounds the variance of each individual term in the uMSE. Lemma 11 (Proof in Section E.5.3). Under the assumptions of the theorem, Var [ u\u0303SEi ] \u2264 14\u03b1, 1 \u2264 i \u2264 n. (24) The proof then follows from the fact that the variance of a sum of independent random variables is equal to the sum of their variances, E [( u\u0303MSE\u2212MSE )2] = Var [ u\u0303MSE ] = Var [ 1 n n\u2211 i=1 u\u0303SEi ] = 1 n2 n\u2211 i=1 Var [ u\u0303SEi ] \u2264 14\u03b1 n . (25) The bound immediately implies convergence in mean square as n \u2192 \u221e, which in turn implies convergence in probability. E.3 PROOF OF COROLLARY 5: The uPSNR is a continuous function of the uMSE, which is the same function mapping the MSE to the PSNR. The result then follows from Theorem 4 and the continuous mapping theorem. E.4 PROOF OF THEOREM 6: To prove Theorem 6, we express the uMSE as a sum of zero-mean random variables, u\u0303MSE = n\u2211 i=1 t\u0303i, t\u0303i := u\u0303SEi \u2212 SEi n , (26) and apply the following version of the Lyapunov central limit theorem. Theorem 12 (Theorem 9.2 Breiman (1992)). Let t\u0303i, 1 \u2264 i \u2264 n, be independent zero-mean random variables with bounded second and third moments, and let s2n := n\u2211 i=1 E [ t\u03032i ] . (27) If the Lyapunov condition: lim n\u2192\u221e \u2211n i=1 E [\u2223\u2223t\u0303i\u2223\u22233] s3n = 0 (28) holds, then the random variable 1 sn n\u2211 i=1 t\u0303i (29) converges in distribution to a standard Gaussian as n \u2192 \u221e. To complete the proof we show that the random variable t\u0303i := u\u0303SEi \u2212 SEi n (30) satisfies the conditions of Theorem 12. By Lemma 10 its mean is zero. By Lemma 11 its second moment is bounded. To control sn, we apply the following auxiliary lemma, which provides a lower bound on the variance of each term in the uMSE. Lemma 13. Under the assumptions of Theorem 6, Var [ u\u0303SEi ] \u2265 \u00b5 [4] i + \u03c3 4 i 2 , (31) where \u00b5[4]i and \u03c3 2 i denote the fourth central moment and the variance of a\u0303i, b\u0303i and c\u0303i. The lemma yields a lower bound for s2n, s2n := n\u2211 i=1 E [ t\u03032i ] (32) = 1 n2 n\u2211 i=1 E [( u\u0303SEi \u2212 SEi )2] (33) = 1 n2 n\u2211 i=1 Var [ u\u0303SEi ] (34) \u2265 2\u00b5 [4] i + 2\u03c3 4 n . (35) The following lemma controls the numerator in the Lyapunov condition, and also shows that the third moment of t\u0303i is bounded. Lemma 14 (Proof in Section E.5.5). Under the assumptions of Theorem 6, there exists a numerical positive constant D such that n\u2211 i=1 E [\u2223\u2223t\u0303i\u2223\u22233] \u2264 D\u03b7 n2 . (36) Combining equation 35 and Lemma 14, we obtain\u2211n i=1 E [\u2223\u2223t\u0303i\u2223\u22233] s3n \u2264 D\u03b7 (2\u00b5 [4] i + 2\u03c3 4)1.5 \u221a n , (37) which converges to zero as n \u2192 \u221e. The Lyapunov condition therefore holds and the proof is complete. E.5 PROOF OF AUXILIARY RESULTS:  E.5.1 NOTATION: To alleviate notation in our proofs, we define the denoising error erri := f(y)i \u2212 xi and the centered random variables C(a\u0303i) := a\u0303i \u2212 xi, C(b\u0303i) := b\u0303i \u2212 xi and C(c\u0303i) := c\u0303i \u2212 xi, which are independent, have zero mean and satisfy Var [a\u0303i] = E [ a\u03032i ] = Var[b\u0303i] = E[b\u03032i ] = Var [c\u0303i] = E [ c\u03032i ] . E.5.2 PROOF OF LEMMA 10: By linearity of expectation and the fact that the variance of independent random variables equals the sum of their variances, E [ u\u0303SEi ] = E [ (a\u0303i \u2212 f(y)i)2 \u2212 (b\u0303i \u2212 c\u0303i)2 2 ] (38) = E [ (C(a\u0303i)\u2212 erri))2 ] \u2212 E [ (C(b\u0303i)\u2212 C(c\u0303i))2 ] 2 (39) = E [ C(a\u0303i) 2 ] \u2212 2erriE [C(a\u0303i)] + SEi \u2212 Var [ C(b\u0303i)\u2212 C(c\u0303i) ] 2 (40) = Var [a\u0303i] + SEi \u2212 Var[C(b\u0303i)] + Var [C(c\u0303i)] 2 (41) = Var [a\u0303i] + SEi \u2212 Var[b\u0303i] + Var [c\u0303i] 2 (42) = SEi. (43) E.5.3 PROOF OF LEMMA 11: By linearity of expectation, the fact that the variance of independent random variables equals the sum of their variances and the fact that the mean square is an upper bound on the variance, Var [ u\u0303SEi ] = Var [ (a\u0303i \u2212 f(y)i)2 ] + Var [ (b\u0303i \u2212 c\u0303i)2 ] 4 (44) \u2264 E [ (a\u0303i \u2212 f(y)i)4 ] + E [ (b\u0303i \u2212 c\u0303i)4 ] 4 (45) = E [ (C(a\u0303i)\u2212 erri)4 ] + E [( C(b\u0303i)\u2212 C(c\u0303i) )4] 4 (46) \u2264 E [ C(a\u0303i) 4 ] + 4E [ C(a\u0303i) 3 ] |erri|+ 6E [ C(a\u0303i) 2 ] err2i + err 4 i + E [ C(b\u0303i) 4 ] + 6E [ C(b\u0303i) 2 ] E [ C(c\u0303i) 2 ] + [ C(c\u0303i) 4 ] 4 (47) \u2264 14\u03b1, (48) where we have also used the fact that \u00b522 \u2264 \u00b5 [4] i by Jensen\u2019s inequality, which implies E [ C(a\u0303i) 2 ] SEi \u2264 \u221a \u00b5 [4] i \u03b3 4 \u2264 \u03b1, (49) E [ C(b\u0303i) 2 ] E [ C(c\u0303i) 2 ] \u2264 \u00b5[4]i \u2264 \u03b1. (50) E.5.4 PROOF OF LEMMA 13: The variance of independent random variables equals the sum of their variances, so Var [ u\u0303SEi ] = Var [ (a\u0303i \u2212 f(y)i)2 ] + Var [ (b\u0303i \u2212 c\u0303i)2 ] 4 (51) \u2265 Var [ (b\u0303i \u2212 c\u0303i)2 ] 4 . (52) and since the mean of b\u0303i \u2212 c\u0303i is zero, E [ (b\u0303i \u2212 c\u0303i)2 ] = Var[b\u0303i \u2212 c\u0303i] (53) = Var[b\u0303i] + Var[c\u0303i] (54) = 2\u03c32i . (55) By the definition of variance and linearity of expectation, Var [ (b\u0303i \u2212 c\u0303i)2 ] = E [ (b\u0303i \u2212 c\u0303i)4 ] \u2212 E [ (b\u0303i \u2212 c\u0303i)2 ]2 (56) = E [( C(b\u0303i)\u2212 C(c\u0303i) )4] \u2212 4\u03c34i (57) = E [ C(b\u0303i) 4 ] + E [ C(c\u0303i) 4 ] + 6E [ C(b\u0303i) 2C(c\u0303i) 2 ] \u2212 4\u03c34i (58) = 2\u00b5 [4] i + 2\u03c3 4 i . (59) E.5.5 PROOF OF LEMMA 14: By linearity of expectation, E [\u2223\u2223\u2223u\u0303SEi \u2212 SEi\u2223\u2223\u22233] = E \u2223\u2223\u2223\u2223\u2223(a\u0303i \u2212 f(y)i)2 \u2212 (b\u0303i \u2212 c\u0303i)22 \u2212 err2i \u2223\u2223\u2223\u2223\u2223 3  (60) = E \u2223\u2223\u2223\u2223\u2223(C(a\u0303i)\u2212 erri)2 \u2212 (C(b\u0303i)\u2212 C(c\u0303i))22 \u2212 err2i \u2223\u2223\u2223\u2223\u2223 3  (61) \u2264 E [ (C(a\u0303i)\u2212 erri)6 ] + 1 8 E [ (C(b\u0303i)\u2212 C(c\u0303i))6 ] + err6i (62) + 3 2 E [ (C(a\u0303i)\u2212 erri)4 ] E [ (C(b\u0303i)\u2212 C(c\u0303i))2 ] (63) + 3 4 E [ (C(a\u0303i)\u2212 erri)2 ] E [ (C(b\u0303i)\u2212 C(c\u0303i))4 ] (64) + 3E [ (C(a\u0303i)\u2212 erri)4 ] err2i + 3E [ (C(a\u0303i)\u2212 erri)2 ] err4i (65) + 3 4 E [ (C(b\u0303i)\u2212 C(c\u0303i))4 ] err2i + 3 2 E [ (C(b\u0303i)\u2212 C(c\u0303i))2 ] err4i (66) + 3E [ (C(a\u0303i)\u2212 erri)2 ] E [ (C(b\u0303i)\u2212 C(c\u0303i))2 ] err2i (67) \u2264 D\u03b7. (68) The final bound in equation 68 is obtained by bounding each term in the sum using the assumption that the maximum entrywise denoising error and the central moments of a\u0303i, b\u0303i and c\u0303i are bounded. For example, E [ (C(a\u0303i)\u2212 erri)6 ] = E [ C(a\u0303i) 6 ] + err6i + 6E [ C(a\u0303i) 5 ] erri + 15E [ C(a\u0303i) 4 ] err2i (69) + 15E [ C(a\u0303i) 2 ] err4i + 20E [ C(a\u0303i) 3 ] err3i . (70) Finally, by linearity of expectation we have n\u2211 i=1 E [\u2223\u2223t\u0303i\u2223\u22233] = 1 n3 n\u2211 i=1 E [\u2223\u2223\u2223u\u0303SEi \u2212 SEi\u2223\u2223\u22233] (71) \u2264 D\u03b7 n2 . (72) F DESCRIPTION OF CONTROLLED EXPERIMENTS: In this section, we describe the architectures and training procedure for models used in Section 5. For our experiments with natural images, we use the pre-trained weights released in Zhang et al. (2017a) and Mohan et al. (2020). All models are trained on 180 \u00d7 180 natural images from the Berkeley Segmentation Dataset (Martin et al., 2001) synthetically corrupted with Gaussian noise with standard deviation uniformly sampled between 0 and 100. The training set contains 400 images and is augmented via downsampling, random flips, and random rotations of patches in these images (Zhang et al., 2017a; Mohan et al., 2020). We use the standard test set containing 68 images for evaluation. We describe each of the models we use in detail below. 1. Bilateral filter OpenCV implementation for the Bilateral filter with a filter diameter of 15 pixels and \u03c3value = \u03c3space = 1. 2. DnCNN. DnCNN Zhang et al. (2017a) consists of 20 convolutional layers, each consisting of 3\u00d7 3 filters and 64 channels, batch normalization (Ioffe & Szegedy, 2015), and a ReLU nonlinearity. It has a skip connection from the initial layer to the final layer, which has no nonlinear units. We use the pre-trained weights released by the authors. 3. UNet. Our UNet model (Ronneberger et al., 2015) has the following layers: (a) conv1 - Takes in input image and maps to 32 channels with 5\u00d7 5 convolutional kernels. (b) conv2 - Input: 32 channels. Output: 32 channels. 3\u00d7 3 convolutional kernels. (c) conv3 - Input: 32 channels. Output: 64 channels. 3 \u00d7 3 convolutional kernels with stride 2. (d) conv4- Input: 64 channels. Output: 64 channels. 3\u00d7 3 convolutional kernels. (e) conv5- Input: 64 channels. Output: 64 channels. 3 \u00d7 3 convolutional kernels with dilation factor of 2. (f) conv6- Input: 64 channels. Output: 64 channels. 3 \u00d7 3 convolutional kernels with dilation factor of 4. (g) conv7- Transpose Convolution layer. Input: 64 channels. Output: 64 channels. 4\u00d7 4 filters with stride 2. (h) conv8- Input: 96 channels. Output: 64 channels. 3 \u00d7 3 convolutional kernels. The input to this layer is the concatenation of the outputs of layer conv7 and conv2. (i) conv9- Input: 32 channels. Output: 1 channels. 5\u00d7 5 convolutional kernels. We use pre-trained weights released by the authors of Mohan et al. (2020). 4. DenseNet The simplified version of the DenseNet architecture (Huang et al., 2017) has 4 blocks in total. Each block is a fully convolutional 5-layer CNN with 3\u00d7 3 filters and 64 channels in the intermediate layers with ReLU nonlinearity. The first three blocks have an output layer with 64 channels, while the last block has an output layer with only one channel. The output of the ith block is concatenated with the input noisy image and then fed to the (i+ 1)th block, so the last three blocks have 65 input channels. We use pre-trained weights released by the authors of Mohan et al. (2020). For our experiments with electron microscopy data, we use the simulated dataset of Pt nanoparticles introduced in Mohan et al. (2022). Specifically, we used a subset of 5,583 images corresponding to white contrast (the simulated dataset is divided into white, black and intermediate contrast by a domain expert, see Mohan et al. (2022) for more details). 90% of the data were used for training. The remaining 559 images were evenly split into validation and test sets. The UNet architecture used in these experiments is the one introduced in Mohan et al. (2022) with 4 scales and 32 base channels. In addition to bilateral filter, UNet, and DnCNN models described for natural images, we used a blindspot based network. BlindSpot Laine et al. (2019) is a CNN which is constrained to predict the intensity of a pixel as a function of the noisy pixels in its neighbourhood, without using the pixel itself. Following Laine et al. (2019); Sheth et al. (2021), we use a UNet architecture as the model backbone. G DESCRIPTION OF EXPERIMENTS WITH VIDEOS IN RAW FORMAT: As explained in Section 6, the dataset contains 11 unique videos, each containing 7 frames, captured at five different ISO levels using a surveillance camera. Each video has 10 different noise realizations per frame, which are averaged to obtain an estimated clean version of the video. Following Sheth et al. (2021), we perform evaluation on five videos from the test test. The methods we use: 1. Image Denoiser (Wavelet). We use Daubechies wavelet to perform denoising, which is the default choice in skimage.restoration, a widely used image restoration package. We implement denoising using the function denoise_wavelet() from the package using the default options. We set sigma=0.01. 2. Image Denoiser (CNN). We perform image denoising by re-purposing the video denoiser (UDVD) trained for RAW videos in Ref. Sheth et al. (2021). UDVD takes in five consecutive frames, and output the denoised image corresponding to the frame in the middle. To simulate image denoising using UDVD, we repeat the same frame 5 times (i.e, all frames are the same image), and provide it as input to the trained network. 3. Video Denoiser (Temp. Avg.). We use 5 consecutive frames to compute the denosied image corresponding to the middle frame. We assign a weight of 0.75 to the middle noisy frame, 0.1 to each of the previous and next frame, and 0.025 to the rest of the two frames. 4. Video Denoiser (CNN). We use the unsupervised video denoiser (UDVD) trained for RAW videos in Ref. Sheth et al. (2021). As explained, UDVD takes in five consecutive frames, and output the denoised image corresponding to the frame in the middle. We use the pre-trained weights, and follow the experimental setup described in Ref. Sheth et al. (2021). We use the pre-trained weights released by the authors of Sheth et al. (2021) as our image and video denoiser. These weights are obtained by training UDVD on the first 9 realizations of the 5 videos from the test set of the raw video dataset, holding out the last realization for early stopping (see Sheth et al. (2021) for more details). H DESCRIPTION OF EXPERIMENTS ON ELECTRON MICROSCOPY DATA:  Data acquisition:: The dataset contains TEM images of Pt nanoparticles on a CeO2 substrate. An electron beam interacts with the sample, and then its intensity is recorded on an imaging plane by the detector. The pixel intensity approximately follows a Poisson distribution with parameter equal to the intensity of the electron beam. The data were recorded at room temperature at a pressure of \u223c 10 \u2212 6 Torr. The electron beam intensity was 600e/\u00c5 2 /s. The instances are part of 25 videos, taken at a frame rate of 75 frames per second. The instances show Pt particles in the size range 1 - 5 nm. In a subset of frame series, the particles become unstable and undergo structural dynamic re-arrangements. The periods of instability are punctuated by periods of relative stability. Consequently, the nanoparticles show a variety of different sizes and shapes and are also viewed along many different crystallographic directions. Data were collected using a FEI Titan ETEM in EFTEM mode, Gatan Tantalum hot stage, K3 camera in CDS counting mode. Pixel-wise correlation: Our proposed unsupervised metrics rely on the assumption that the noise is pixel-wise independence. This is not the case for this dataset, as shown in Figure 11.We address this by performing spatial subsampling by a factor of two, which reduces the pixel-wise correlation by an order of magnitude. After this, some frames still present relatively high pixel-wise correlations. We therefore select the test sets from two sets of contiguous frames with low correlation. Training and test sets: The data were divided into three sets: training & validation set, consisting of 70% of the data, and two contiguous test sets with pixel-wise correlation: one containing 155 images with moderate signal-to-noise ratio (SNR), and one containing 383 images with low SNR, which are more challenging. The moderate SNR test set is interspersed with the training and validation sets, and contains frames similar to those used to train the network. The low SNR test set contains frames which are temporally separated from the training and validation sets, and contains nanoparticle with different structures. Denoisers We compare the performance of two denoisers: (1) a convolutional neural network based on Neighbor2Neighbor with the same architecture as in Huang et al. (2021); (2) Gaussian smoothing with kernel size =25 and \u03c3 = 25. CNN training parameters The CNN was based on the architecture in Huang et al. (2021). The network consists of a modified UNet, and is trained for 500 epochs using an Adam optimizer with an initial learning rate of 0.0001, and scheduled reduction of the learning rate every 100 epochs. The network has a total of 1,256,689 parameters."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "I will create three agents to review the paper and provide a final decision. First, I need to check the available agents.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {\n    \"DrMetrics\": {\n      \"description\": \"Expert in evaluation metrics, focusing on theoretical soundness and statistical properties.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"DrDenoise\": {\n      \"description\": \"Expert in image denoising techniques, focusing on practical applicability and impact on denoising research.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"DrApplication\": {\n      \"description\": \"Expert in applying denoising techniques to real-world scientific imaging problems.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {'DrMetrics': {'description': 'Expert in evaluation metrics, focusing on theoretical soundness and statistical properties.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'DrDenoise': {'description': 'Expert in image denoising techniques, focusing on practical applicability and impact on denoising research.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'DrApplication': {'description': 'Expert in applying denoising techniques to real-world scientific imaging problems.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems like I already have agents that fit the roles I need:\n\n*   **Dr. Metrics:** Expert in evaluation metrics, focusing on theoretical soundness and statistical properties.\n*   **Dr. Denoise:** Expert in image denoising techniques, focusing on practical applicability and impact on denoising research.\n*   **Dr. Application:** Expert in applying denoising techniques to real-world scientific imaging problems.\n\nNow I need to check the budget before I invoke these agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 1.0071524875,\n    \"current_remaining_expense_budget\": 8.9928475125\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 1.0071524875, 'current_remaining_expense_budget': 8.9928475125}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have enough budget to invoke the three agents. I will now invoke each agent with the prompt to review the paper.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Please provide a detailed review of the paper 'Evaluating Unsupervised Denoising Requires Unsupervised Metrics' for ICLR 2023. Your review should focus on the theoretical soundness and statistical properties of the proposed unsupervised metrics (uMSE and uPSNR). Include strengths and weaknesses, and provide a final recommendation (Accept/Reject).\", 'agent_name': 'DrMetrics'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper 'Evaluating Unsupervised Denoising Requires Unsupervised Metrics' for ICLR 2023. Your review should focus on the theoretical soundness and statistical properties of the proposed unsupervised metrics (uMSE and uPSNR). Include strengths and weaknesses, and provide a final recommendation (Accept/Reject).\",\n  \"agent_name\": \"DrMetrics\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems I have exceeded my quota. I will try again after waiting for a while.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Please provide a detailed review of the paper 'Evaluating Unsupervised Denoising Requires Unsupervised Metrics' for ICLR 2023. Your review should focus on the theoretical soundness and statistical properties of the proposed unsupervised metrics (uMSE and uPSNR). Include strengths and weaknesses, and provide a final recommendation (Accept/Reject).\", 'agent_name': 'DrMetrics'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '42s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper 'Evaluating Unsupervised Denoising Requires Unsupervised Metrics' for ICLR 2023. Your review should focus on the theoretical soundness and statistical properties of the proposed unsupervised metrics (uMSE and uPSNR). Include strengths and weaknesses, and provide a final recommendation (Accept/Reject).\",\n  \"agent_name\": \"DrMetrics\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '42s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I am still exceeding my quota. I will try to reduce the prompt length.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'DrMetrics', 'prompt': \"Review 'Evaluating Unsupervised Denoising Requires Unsupervised Metrics' for ICLR 2023. Focus: theoretical soundness & statistical properties of uMSE/uPSNR. Strengths, weaknesses, Accept/Reject.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '40s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"DrMetrics\",\n  \"prompt\": \"Review 'Evaluating Unsupervised Denoising Requires Unsupervised Metrics' for ICLR 2023. Focus: theoretical soundness & statistical properties of uMSE/uPSNR. Strengths, weaknesses, Accept/Reject.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '40s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I am still hitting the quota limit. Since I cannot reliably invoke the agents at this time, I will have to provide a mock review based on my understanding of the paper and the expected expertise of the agents.\n\n**Review from Dr. Metrics (Expert in evaluation metrics):**\n\nThis paper introduces uMSE and uPSNR, novel unsupervised metrics for evaluating denoising algorithms. The core strength lies in the theoretical justification provided. The authors rigorously prove that uMSE is an asymptotically consistent estimator of MSE under reasonable assumptions (independence of noise, centered noise). The derivation of the correction term to achieve unbiasedness is clever and well-explained. The paper also provides a consistency proof for uPSNR.\n\nA potential weakness is the reliance on the independence assumption. While the paper acknowledges this limitation and discusses spatial subsampling as a practical workaround, the bias introduced by this subsampling, especially for natural images, needs further investigation. The asymptotic normality result is a valuable addition, enabling the construction of confidence intervals. However, the practical utility of these confidence intervals might be limited by the speed of convergence to normality, which is not explicitly analyzed.\n\nOverall, the paper makes a significant contribution by providing theoretically sound unsupervised metrics for denoising. The consistency proofs and the discussion of limitations are commendable.\n\nFINAL DECISION: Accept\n\n**Review from Dr. Denoise (Expert in image denoising):**\n\nThe paper addresses a critical gap in the denoising literature: the lack of reliable unsupervised evaluation metrics. The proposed uMSE and uPSNR metrics are inspired by Noise2Noise and offer a practical way to assess denoising performance without clean ground truth. The experimental results on both synthetic and real-world datasets are convincing. The application to electron microscopy data is particularly compelling, as it demonstrates the potential of these metrics to guide hyperparameter optimization and model selection in scientific imaging.\n\nA weakness is the requirement of multiple noisy references. While the paper suggests using consecutive frames or spatial subsampling, these approaches may not always be feasible or may introduce artifacts. The performance of the metrics in scenarios with non-stationary noise or signal-dependent noise could also be explored further.\n\nDespite these limitations, the paper presents a valuable tool for the denoising community. The ability to quantitatively evaluate unsupervised denoising methods on real-world data is a significant step forward.\n\nFINAL DECISION: Accept\n\n**Review from Dr. Application (Expert in applying denoising to scientific imaging):**\n\nThis paper is highly relevant to the scientific imaging community, where obtaining clean ground truth data is often impossible. The uMSE and uPSNR metrics provide a much-needed way to quantitatively evaluate denoising performance on real-world data, such as electron microscopy images. The application to TEM data is a strong demonstration of the practical utility of these metrics. The paper clearly highlights the challenges of unsupervised denoising in scientific imaging and offers a promising solution.\n\nA potential concern is the sensitivity of the metrics to noise correlations. The paper acknowledges this issue and proposes spatial subsampling as a mitigation strategy. However, the effectiveness of this approach may vary depending on the specific imaging modality and noise characteristics. It would be beneficial to provide more detailed guidelines on how to choose appropriate subsampling parameters.\n\nOverall, this paper makes a significant contribution to the field of scientific imaging by providing a practical and reliable way to evaluate unsupervised denoising methods.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Reject", "response_time": 203.88909196853638}
