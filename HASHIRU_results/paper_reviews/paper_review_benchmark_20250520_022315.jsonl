{"paper_id": "n-bvaLSCC78", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\n\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves of unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et al., 2009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will create three agents to review the paper \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\". Each agent will provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. Finally, I will provide a FINAL DECISION based on the agent reviews.\n\nBefore creating the agents, I need to check the available budget and the cost of creating and invoking agents.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10,\n    \"current_expense\": 0.23584183500000028,\n    \"current_remaining_expense_budget\": 9.764158165\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10, 'current_expense': 0.23584183500000028, 'current_remaining_expense_budget': 9.764158165}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have a resource budget of 100 and an expense budget of 9.76. I will create three agents using the `gemini-1.5-flash` model, as it offers a balance of performance and cost-effectiveness. The expense cost for creating each agent is 0.005, and the invoke cost is 0.00017. Creating three agents will cost 0.015, which is within my budget.\n\nNow, I will create the three agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'Reviewer1', 'base_model': 'gemini-1.5-flash', 'description': 'Expert in NAS, focuses on benchmark design and methodology.', 'system_prompt': 'You are an expert reviewer in Neural Architecture Search (NAS). You have a strong background in benchmark design and methodology. Your reviews are detailed and focus on the soundness of the experimental design, the validity of the conclusions, and the clarity of the presentation. You are very critical of any methodological flaws or unsubstantiated claims.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in HPO, focuses on surrogate models and optimization algorithms.', 'system_prompt': 'You are an expert reviewer in Hyperparameter Optimization (HPO). You have a strong background in surrogate models and optimization algorithms. Your reviews are detailed and focus on the novelty and effectiveness of the proposed surrogate model, the efficiency of the optimization algorithm, and the thoroughness of the experimental evaluation. You are very critical of any weaknesses in the surrogate model or optimization algorithm.', 'agent_name': 'Reviewer2', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in energy-aware computing, focuses on the practical impact and energy efficiency.', 'system_prompt': 'You are an expert reviewer in energy-aware computing. You have a strong background in the practical impact and energy efficiency of machine learning models. Your reviews are detailed and focus on the potential energy savings, the real-world applicability of the proposed methods, and the environmental impact of the research. You are very critical of any unrealistic assumptions or limited practical benefits.', 'agent_name': 'Reviewer3', 'base_model': 'gemini-1.5-flash'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent Reviewer1 already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"Reviewer1\",\n  \"base_model\": \"gemini-1.5-flash\",\n  \"description\": \"Expert in NAS, focuses on benchmark design and methodology.\",\n  \"system_prompt\": \"You are an expert reviewer in Neural Architecture Search (NAS). You have a strong background in benchmark design and methodology. Your reviews are detailed and focus on the soundness of the experimental design, the validity of the conclusions, and the clarity of the presentation. You are very critical of any methodological flaws or unsubstantiated claims.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent Reviewer2 already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in HPO, focuses on surrogate models and optimization algorithms.\",\n  \"system_prompt\": \"You are an expert reviewer in Hyperparameter Optimization (HPO). You have a strong background in surrogate models and optimization algorithms. Your reviews are detailed and focus on the novelty and effectiveness of the proposed surrogate model, the efficiency of the optimization algorithm, and the thoroughness of the experimental evaluation. You are very critical of any weaknesses in the surrogate model or optimization algorithm.\",\n  \"agent_name\": \"Reviewer2\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent Reviewer3 already exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in energy-aware computing, focuses on the practical impact and energy efficiency.\",\n  \"system_prompt\": \"You are an expert reviewer in energy-aware computing. You have a strong background in the practical impact and energy efficiency of machine learning models. Your reviews are detailed and focus on the potential energy savings, the real-world applicability of the proposed methods, and the environmental impact of the research. You are very critical of any unrealistic assumptions or limited practical benefits.\",\n  \"agent_name\": \"Reviewer3\",\n  \"base_model\": \"gemini-1.5-flash\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent Reviewer1 already exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent Reviewer2 already exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'error', 'message': 'Error occurred: Agent Reviewer3 already exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It seems the agents already exist. I will proceed to ask the agents to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'Reviewer1', 'prompt': \"Review the following paper and provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of 'FINAL DECISION: <Accept/Reject>'. The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\\n\\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves of unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et al., 2009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\"\n\nThis paper introduces EA-HAS-Bench, a benchmark designed to evaluate the energy efficiency of AutoML algorithms.  The authors address the critical issue of energy consumption in the context of Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO), arguing that existing benchmarks often overlook the actual energy costs associated with the search process itself.  They propose a large joint search space combining RegNet architectures and various training hyperparameters, along with a novel B\u00e9zier curve-based surrogate model (BCS) to efficiently explore this space.  The paper concludes by demonstrating the use of EA-HAS-Bench to benchmark existing AutoML algorithms and introducing an energy-aware modification to improve the energy-performance trade-off.\n\n\n**Strengths:**\n\n* **Addresses a significant and timely problem:** The focus on energy-aware AutoML is crucial, given the growing environmental concerns surrounding deep learning. This is a valuable and under-explored area of research.\n* **Large-scale and comprehensive benchmark:** EA-HAS-Bench represents a substantial effort in creating a large joint search space encompassing both architectural and hyperparameter configurations, providing a more realistic evaluation setting than many existing benchmarks. The sheer size of the search space (10 billion configurations) is impressive.\n* **Novel surrogate model:** The proposed BCS model is a creative approach to address the challenge of predicting learning curves of variable lengths, a problem inherent to the variable epoch settings in the search space. The use of B\u00e9zier curves offers a flexible and potentially powerful way to model the diverse shapes of learning curves.\n* **Thorough evaluation:** The paper includes a comprehensive evaluation of the BCS model, comparing it against various baseline methods and presenting relevant metrics (R\u00b2, \u03c4, spike anomalies).  The analysis of the benchmark dataset itself provides valuable insights into the relationships between different configurations, performance, and energy consumption.\n* **Practical demonstration:** The authors demonstrate the utility of EA-HAS-Bench by benchmarking existing AutoML algorithms and proposing an energy-aware modification, showcasing the practical impact of their work.\n\n\n**Weaknesses:**\n\n* **Limited novelty in the energy-aware modifications:** While the energy-aware modification of existing HPO algorithms is a necessary step, the approach itself (adding a penalty term to the objective function) lacks significant novelty.  More sophisticated techniques for integrating energy considerations into the optimization process would strengthen the contribution.\n* **Lack of rigorous comparison with other energy-aware methods:** The paper focuses on modifying existing algorithms rather than directly comparing against other potential energy-aware AutoML approaches, if any exist. This limits the ability to fully assess the novelty and effectiveness of the proposed modifications.\n* **Hardware specifics and reproducibility:** The energy consumption figures are heavily reliant on the specific hardware used (Tesla V100).  The paper should provide more detail on the hardware setup to ensure reproducibility and generalizability of the results.  Ideally, an analysis of energy consumption across different hardware would be beneficial.\n* **Potential for bias in data collection:** While the authors mention using random search for sampling, the discussion of potential biases in the sampling process is limited. A more in-depth analysis of the potential for sampling bias and its impact on the accuracy of the surrogate model would enhance the paper's robustness.\n* **Clarity of the BCS model:** While the mathematical formulation of the BCS model is presented, a more intuitive explanation of its mechanics and the reasons for its effectiveness would improve readability and understanding.  Visualizations might further aid in comprehension.\n\n\n**Overall Assessment:**\n\nThe paper presents a valuable contribution to the field by addressing the crucial issue of energy efficiency in AutoML.  The creation of EA-HAS-Bench, a large-scale and comprehensive benchmark, is a significant achievement.  The proposed BCS model is a novel and potentially useful tool for handling variable-length learning curves. However, the limited novelty of the proposed energy-aware modifications and the lack of a thorough comparison with existing (if any) energy-aware methods weaken the overall impact.  The paper would benefit from a more detailed discussion of hardware specifics, a deeper analysis of sampling bias, and a clearer explanation of the BCS model.\n\n**Recommendation:**\n\nWhile the benchmark itself is a significant contribution, the methodological advancements are somewhat incremental. The paper is strong enough for publication in ICLR 2023, but improvements are needed to fully realize its potential.\n\n\n**FINAL DECISION: Accept (with major revisions)**\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.763231495\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"Reviewer1\",\n  \"prompt\": \"Review the following paper and provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of 'FINAL DECISION: <Accept/Reject>'. The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\n\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves of unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et al., 2009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\"\\n\\nThis paper introduces EA-HAS-Bench, a benchmark designed to evaluate the energy efficiency of AutoML algorithms.  The authors address the critical issue of energy consumption in the context of Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO), arguing that existing benchmarks often overlook the actual energy costs associated with the search process itself.  They propose a large joint search space combining RegNet architectures and various training hyperparameters, along with a novel B\u00e9zier curve-based surrogate model (BCS) to efficiently explore this space.  The paper concludes by demonstrating the use of EA-HAS-Bench to benchmark existing AutoML algorithms and introducing an energy-aware modification to improve the energy-performance trade-off.\\n\\n\\n**Strengths:**\\n\\n* **Addresses a significant and timely problem:** The focus on energy-aware AutoML is crucial, given the growing environmental concerns surrounding deep learning. This is a valuable and under-explored area of research.\\n* **Large-scale and comprehensive benchmark:** EA-HAS-Bench represents a substantial effort in creating a large joint search space encompassing both architectural and hyperparameter configurations, providing a more realistic evaluation setting than many existing benchmarks. The sheer size of the search space (10 billion configurations) is impressive.\\n* **Novel surrogate model:** The proposed BCS model is a creative approach to address the challenge of predicting learning curves of variable lengths, a problem inherent to the variable epoch settings in the search space. The use of B\u00e9zier curves offers a flexible and potentially powerful way to model the diverse shapes of learning curves.\\n* **Thorough evaluation:** The paper includes a comprehensive evaluation of the BCS model, comparing it against various baseline methods and presenting relevant metrics (R\u00b2, \u03c4, spike anomalies).  The analysis of the benchmark dataset itself provides valuable insights into the relationships between different configurations, performance, and energy consumption.\\n* **Practical demonstration:** The authors demonstrate the utility of EA-HAS-Bench by benchmarking existing AutoML algorithms and proposing an energy-aware modification, showcasing the practical impact of their work.\\n\\n\\n**Weaknesses:**\\n\\n* **Limited novelty in the energy-aware modifications:** While the energy-aware modification of existing HPO algorithms is a necessary step, the approach itself (adding a penalty term to the objective function) lacks significant novelty.  More sophisticated techniques for integrating energy considerations into the optimization process would strengthen the contribution.\\n* **Lack of rigorous comparison with other energy-aware methods:** The paper focuses on modifying existing algorithms rather than directly comparing against other potential energy-aware AutoML approaches, if any exist. This limits the ability to fully assess the novelty and effectiveness of the proposed modifications.\\n* **Hardware specifics and reproducibility:** The energy consumption figures are heavily reliant on the specific hardware used (Tesla V100).  The paper should provide more detail on the hardware setup to ensure reproducibility and generalizability of the results.  Ideally, an analysis of energy consumption across different hardware would be beneficial.\\n* **Potential for bias in data collection:** While the authors mention using random search for sampling, the discussion of potential biases in the sampling process is limited. A more in-depth analysis of the potential for sampling bias and its impact on the accuracy of the surrogate model would enhance the paper\\'s robustness.\\n* **Clarity of the BCS model:** While the mathematical formulation of the BCS model is presented, a more intuitive explanation of its mechanics and the reasons for its effectiveness would improve readability and understanding.  Visualizations might further aid in comprehension.\\n\\n\\n**Overall Assessment:**\\n\\nThe paper presents a valuable contribution to the field by addressing the crucial issue of energy efficiency in AutoML.  The creation of EA-HAS-Bench, a large-scale and comprehensive benchmark, is a significant achievement.  The proposed BCS model is a novel and potentially useful tool for handling variable-length learning curves. However, the limited novelty of the proposed energy-aware modifications and the lack of a thorough comparison with existing (if any) energy-aware methods weaken the overall impact.  The paper would benefit from a more detailed discussion of hardware specifics, a deeper analysis of sampling bias, and a clearer explanation of the BCS model.\\n\\n**Recommendation:**\\n\\nWhile the benchmark itself is a significant contribution, the methodological advancements are somewhat incremental. The paper is strong enough for publication in ICLR 2023, but improvements are needed to fully realize its potential.\\n\\n\\n**FINAL DECISION: Accept (with major revisions)**\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.763231495}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'Reviewer2', 'prompt': \"Review the following paper and provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of 'FINAL DECISION: <Accept/Reject>'. The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\\n\\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves with unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\"\n\nThis paper introduces EA-HAS-Bench, a benchmark for energy-aware hyperparameter and architecture search (HAS).  The core contribution is a large-scale joint search space combining RegNet architectures and training hyperparameters, along with a novel B\u00e9zier curve-based surrogate model (BCS) to predict learning curves and energy consumption.  The authors argue this benchmark addresses the critical need for reproducible and accessible research in energy-efficient AutoML.\n\n**Strengths:**\n\n* **Addresses an important problem:** The growing energy consumption of deep learning is a significant concern.  Focusing on the energy cost of the *search process* itself is a novel and valuable contribution.\n* **Large-scale benchmark:** The size of the search space (10 billion configurations) is impressive and provides a challenging and realistic testbed for energy-aware AutoML algorithms.\n* **Novel surrogate model:** The BCS model is a creative approach to handling the variability in learning curve lengths within the large search space. The use of B\u00e9zier curves offers flexibility and allows for direct constraints on the final performance.  The comparison with existing methods demonstrates its effectiveness.\n* **Comprehensive evaluation:** The paper includes a thorough evaluation of the surrogate model's accuracy, using appropriate metrics (R\u00b2, Kendall's tau, spike anomaly detection).  The analysis of correlations within the benchmark data is also insightful.\n* **Practical use cases:** The authors demonstrate the utility of EA-HAS-Bench by benchmarking existing AutoML algorithms and proposing a modified energy-aware HPO baseline.  The results show that considering energy consumption during the search can lead to significant savings.\n\n\n**Weaknesses:**\n\n* **Reproducibility concerns:** While the paper provides details about the experimental setup, crucial information is missing for full reproducibility.  The precise hardware specifications (GPU model, CPU, memory) used for energy measurements are not stated.  The code and pre-trained models are promised to be released after publication, but this is a significant limitation for review.  The random seeds used for sampling the search space and the training of the surrogate model are not specified, impeding independent verification.\n* **Lack of baseline comparison for energy-aware methods:** The paper compares existing AutoML methods with and without energy considerations on the proposed benchmark. However, it lacks a thorough comparison against other, possibly existing, energy-aware NAS/HPO methods. This makes it difficult to fully assess the novelty and impact of the proposed energy-aware modifications.\n* **Limited discussion of the B\u00e9zier curve order:**  The paper mentions an ablation study on the degree of the B\u00e9zier curve, but the results are only briefly summarized in the appendix. More detail and justification for choosing the optimal degree (4) would be beneficial.\n* **Clarity and presentation:**  While the paper is generally well-written, some sections could be improved for clarity.  The description of the RegNet search space and the BCS model could be more concise and easier to follow. Some figures are referred to in the text but then placed in the appendix, disrupting the flow of reading.\n\n**Overall Assessment:**\n\nThe paper tackles a crucial and timely topic, offering a substantial contribution in the form of a large-scale, energy-aware benchmark for AutoML.  The proposed BCS model is a notable innovation.  However, the lack of full reproducibility details and a more comprehensive comparison against other energy-aware methods are significant weaknesses. The promised code release addresses the reproducibility partially but does not mitigate the lack of detail currently in the paper.\n\n\n**Recommendation:**\n\nI recommend a conditional acceptance, contingent upon the authors addressing the reproducibility concerns by providing complete details about the hardware used for energy measurements, providing the random seeds, and making the code and pre-trained models publicly available *before* publication.  Additionally, including a more detailed description of the ablation study and expanding the comparison to other existing energy-aware methods would strengthen the paper considerably.\n\n**FINAL DECISION: Accept (Conditional)**\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.762305164999999\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"Reviewer2\",\n  \"prompt\": \"Review the following paper and provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of 'FINAL DECISION: <Accept/Reject>'. The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\n\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves with unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\"\\n\\nThis paper introduces EA-HAS-Bench, a benchmark for energy-aware hyperparameter and architecture search (HAS).  The core contribution is a large-scale joint search space combining RegNet architectures and training hyperparameters, along with a novel B\u00e9zier curve-based surrogate model (BCS) to predict learning curves and energy consumption.  The authors argue this benchmark addresses the critical need for reproducible and accessible research in energy-efficient AutoML.\\n\\n**Strengths:**\\n\\n* **Addresses an important problem:** The growing energy consumption of deep learning is a significant concern.  Focusing on the energy cost of the *search process* itself is a novel and valuable contribution.\\n* **Large-scale benchmark:** The size of the search space (10 billion configurations) is impressive and provides a challenging and realistic testbed for energy-aware AutoML algorithms.\\n* **Novel surrogate model:** The BCS model is a creative approach to handling the variability in learning curve lengths within the large search space. The use of B\u00e9zier curves offers flexibility and allows for direct constraints on the final performance.  The comparison with existing methods demonstrates its effectiveness.\\n* **Comprehensive evaluation:** The paper includes a thorough evaluation of the surrogate model\\'s accuracy, using appropriate metrics (R\u00b2, Kendall\\'s tau, spike anomaly detection).  The analysis of correlations within the benchmark data is also insightful.\\n* **Practical use cases:** The authors demonstrate the utility of EA-HAS-Bench by benchmarking existing AutoML algorithms and proposing a modified energy-aware HPO baseline.  The results show that considering energy consumption during the search can lead to significant savings.\\n\\n\\n**Weaknesses:**\\n\\n* **Reproducibility concerns:** While the paper provides details about the experimental setup, crucial information is missing for full reproducibility.  The precise hardware specifications (GPU model, CPU, memory) used for energy measurements are not stated.  The code and pre-trained models are promised to be released after publication, but this is a significant limitation for review.  The random seeds used for sampling the search space and the training of the surrogate model are not specified, impeding independent verification.\\n* **Lack of baseline comparison for energy-aware methods:** The paper compares existing AutoML methods with and without energy considerations on the proposed benchmark. However, it lacks a thorough comparison against other, possibly existing, energy-aware NAS/HPO methods. This makes it difficult to fully assess the novelty and impact of the proposed energy-aware modifications.\\n* **Limited discussion of the B\u00e9zier curve order:**  The paper mentions an ablation study on the degree of the B\u00e9zier curve, but the results are only briefly summarized in the appendix. More detail and justification for choosing the optimal degree (4) would be beneficial.\\n* **Clarity and presentation:**  While the paper is generally well-written, some sections could be improved for clarity.  The description of the RegNet search space and the BCS model could be more concise and easier to follow. Some figures are referred to in the text but then placed in the appendix, disrupting the flow of reading.\\n\\n**Overall Assessment:**\\n\\nThe paper tackles a crucial and timely topic, offering a substantial contribution in the form of a large-scale, energy-aware benchmark for AutoML.  The proposed BCS model is a notable innovation.  However, the lack of full reproducibility details and a more comprehensive comparison against other energy-aware methods are significant weaknesses. The promised code release addresses the reproducibility partially but does not mitigate the lack of detail currently in the paper.\\n\\n\\n**Recommendation:**\\n\\nI recommend a conditional acceptance, contingent upon the authors addressing the reproducibility concerns by providing complete details about the hardware used for energy measurements, providing the random seeds, and making the code and pre-trained models publicly available *before* publication.  Additionally, including a more detailed description of the ablation study and expanding the comparison to other existing energy-aware methods would strengthen the paper considerably.\\n\\n**FINAL DECISION: Accept (Conditional)**\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.762305164999999}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Review the following paper and provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of 'FINAL DECISION: <Accept/Reject>'. The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\\n\\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves with unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et al., 2009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy.\", 'agent_name': 'Reviewer3'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Review of \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\"\n\nThis paper introduces EA-HAS-Bench, a new benchmark for energy-aware hyperparameter and architecture search (HAS).  The authors address the growing concern of energy consumption in AutoML, arguing that existing benchmarks inadequately capture the total energy cost of the search process.  EA-HAS-Bench aims to remedy this by providing a large joint search space encompassing both architectural and hyperparameter configurations, along with associated energy consumption and performance metrics. A novel B\u00e9zier curve-based surrogate model (BCS) is proposed to efficiently predict performance across this vast search space.\n\n**Strengths:**\n\n* **Addresses a crucial problem:** The focus on energy consumption in AutoML is timely and important.  The authors correctly highlight the significant environmental impact of computationally intensive AutoML methods.\n* **Large-scale and comprehensive benchmark:**  The sheer size of the EA-HAS-Bench search space (10 billion configurations) is a significant contribution. The inclusion of both architectural and hyperparameter variations is crucial for realistic evaluation of energy-aware methods.\n* **Novel surrogate model:** The BCS model offers a potentially effective approach to handle the large search space and variable-length learning curves. The use of B\u00e9zier curves is innovative and addresses a limitation of existing surrogate models.\n* **Thorough empirical evaluation:**  The paper includes extensive experiments comparing the proposed BCS model to existing methods and demonstrating the effectiveness of energy-aware AutoML algorithms on the benchmark.\n\n\n**Weaknesses:**\n\n* **Theoretical justification of BCS:** While the BCS model is novel, the paper lacks a rigorous theoretical analysis of its properties.  The authors should provide a more formal justification for the choice of B\u00e9zier curves and discuss potential limitations or biases of this approach.  What are the guarantees on the accuracy and generalization ability of the BCS model?  How does the choice of degree affect the model's performance and susceptibility to overfitting?  A more detailed analysis of the model's capacity and expressiveness would strengthen the paper significantly.\n* **Generalization ability of the benchmark:**  The paper focuses on CIFAR-10 and TinyImageNet.  How well do the findings generalize to other datasets and tasks?  The benchmark's value would be significantly enhanced by demonstrating its applicability beyond these two datasets.  A discussion of potential limitations and biases due to dataset selection is needed.\n* **Energy modeling assumptions:** The paper mentions that training time is not a perfect proxy for energy consumption. While this is true, the authors need to detail their methodology for accurately measuring and modeling energy consumption.  What hardware was used?  What factors were considered (e.g., idle time, GPU utilization)?  Transparency in this aspect is critical for reproducibility.\n* **Energy-aware AutoML methods:** The proposed modifications to existing AutoML algorithms are relatively simple.  A more in-depth investigation of novel energy-aware optimization strategies would significantly improve the paper's contribution.\n\n\n**Clarity and Presentation:**\n\nThe paper is generally well-written and organized. However, some sections could benefit from improved clarity.  The explanation of the BCS model, in particular, could be more concise and intuitive.  More visual aids (e.g., diagrams) would be helpful in explaining the complex interactions between architectural parameters, hyperparameters, and energy consumption.\n\n**Overall Assessment:**\n\nThe paper presents a valuable contribution by addressing the critical issue of energy consumption in AutoML. The creation of EA-HAS-Bench is a significant step towards more reproducible and responsible AutoML research. However, the weaknesses related to the theoretical justification of the BCS model and the limited generalization analysis prevent the paper from reaching its full potential. Addressing these weaknesses would significantly strengthen the paper.\n\n\n**Recommendation:**\n\nWhile the paper's overall contribution is significant, the lack of rigorous theoretical justification for the key methodological components (BCS model) and the limited scope of the empirical evaluation currently prevent acceptance in its current form.  I recommend a **Reject**, but encourage the authors to address the weaknesses outlined above, particularly the theoretical analysis of the BCS model and generalization experiments on other datasets, before resubmission to a suitable venue.\n\n\nFINAL DECISION: Reject\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.761378494999999\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and provide a detailed review, including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of 'FINAL DECISION: <Accept/Reject>'. The paper title is: EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\n\nNone: The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EANAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Be\u0301zier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance. 1 INTRODUCTION: As deep learning technology progresses rapidly, its alarming increased rate of energy consumption causes growing concerns (Schwartz et al., 2020; Li et al., 2021a; Strubell et al., 2019). Neural architecture search (NAS) (Elsken et al., 2019), hyperparameter optimization (HPO) (Feurer & Hutter, 2019) lifted the manual effort of neural architecture and hyperparameter tuning but require repeatedly training large numbers of computationally intensive deep models, leading to significant energy consumption and carbon emissions. For instance, training 10K models on CIFAR-10 (Krizhevsky et al., 2009) for 100 epochs consume about 500,000 kWh of energy power, which is equivalent to the annual electricity consumption of about 600 households in China. As a result, it is essential to develop search energy cost aware (EA) AutoML methods, which are able to find models with good performance while minimising the overall energy consumption throughout the search process. However, existing NAS studies mainly focus on the resource cost of the searched deep model, such as parameter size, the number of float-point operations (FLOPS) or latency (Tan et al., 2019; Wu et al., 2019; He et al., 2021). Exploiting the trade-off between model performance and energy cost during the searching process has been rarely studied (Elsken et al., 2019). In this paper, we propose to take one of the most essential steps in developing energy-aware (EA) NAS methods that make EA-NAS research more reproducible and accessible. Specifically, we provide a benchmark for EA-NAS called Energy Aware Hyperparameter and Architecture Search Benchmark (EA-HAS-Bench), where the researchers can easily obtain the training energy cost and model performance of a certain architecture and hyperparameter configuration, without actually training the model. In order to support developing energy-aware HPO and NAS methods, the proposed EA-HAS-Bench should satisfy three requirements. Search Energy Cost Our dataset needs to provide the total search energy cost of running a specific AutoML method, which is the sum of the training energy cost of all the hyperparameter and architecture configurations the search method traverses. As shown in Table 1, most of the existing conventional benchmarks (Ying et al., 2019; Dong & Yang, 2020; Siems et al., 2020) like NASBench-101 do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. HW-NAS-bench (Li et al., 2021b) provides the inference latency and inference energy consumption of different model architectures, but also does not provide the search energy cost. Energy Related Joint Search Space The search space of EA-HAS-Bench should include the configurations that affect both the model training energy cost and performance. Since both model architectures (e.g., scales, width, depth) and training hyperparameters (e.g., number of epochs) are correlated to the training energy cost, designing a joint hyperparameter and architecture search (HAS) search space is essential. Most NAS benchmarks use a single fixed training hyperparameter configuration for all architectures. Existing HAS benchmarks (Klein & Hutter, 2019; Hirose et al., 2021) use small toy search space which does not cover enough critical factors affecting the search energy cost. As a result, EA-HAS-Bench provides the first dataset with a ten-billion-level architecture/hyperparameter joint space, covering diverse types of configurations related to search energy cost. Surrogate Model for Joint Search Space Due to the enormous size of the proposed joint search space, a surrogate model is needed to fill out the entire search space with a subset of sampled configurations. Existing surrogate methods (Zela et al., 2022; Yan et al., 2021) are not applicable to our proposed large scale joint space. For instance, those methods do not consider the situation of maximum epoch variation and predict only a fixed length learning curve or final performance. Thus, we propose the Be\u0301zier Curve-based Surrogate (BCS) model to fit accuracy learning curves with unlimited shape and length. We summarize the contribution of this paper as follows: \u2022 EE-HAS-Bench is the first HAS dataset aware of the overall search energy cost. 1. Based on this dataset, we further propose a energy-aware AutoML method with search energy cost related penalties, showing energy-aware AutoML achieves a better trade-off between model performance and search energy cost. \u2022 We provide the first large-scale benchmark containing an architecture / hyperparameter joint search space with over 10 billion configurations, covering various configurations related to search energy cost. \u2022 We develop a novel surrogate model suitable for more general and complex joint HAS search space, which outperforms NAS-Bench-X11 and other parametric learning curvebased methods. 1The dataset of EA-HAS-Bench will be released after paper publication 2 CREATING ENERGY AWARE BENCHMARKS:  2.1 EA-HAS-BENCH SEARCH SPACE: Unlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our EA-HAS-Bench consists of a combination of two parts: the network architecture spaceRegNet (Radosavovic et al., 2020) and the hyperparameter space for optimization and training, in order to cover a diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table 2. RegNet Search Space. Our benchmark applies RegNet as the architecture search space because it contains several essential factors that control the scale and shape of the deep model, which corresponds to the training energy cost. Specifically, RegNet is a macro search space with the variation in width and depth explained by a quantized linear function. Specifically, for widths of residual blocks: uj = w0 +wa \u00d7 j, where 0 \u2264 j \u2264 d and d denotes the depth. An additional parameter wm is introduced to quantize uj , i.e. uj = w0\u00d7w sj m and the quantized per-block widths wj is computed by wj = w0 \u00d7 w\u230asj\u2309m (1) where \u230asj\u2309 denotes round sj . For different datasets (i.e., CIFAR-10 and TinyImageNet), we adjust this space accordingly via constraining parameters and FLOPs. Training hyperparameter Search Space. Hyperparameter space (e.g., learning rate and maximum epoch) also has a great impact on energy consumption throughout the training phase. For example, training epochs is almost proportional to training time, which is also proportional to training energy cost. Different learning rate also causes different convergence rate and different total training time, resulting in different training energy cost. Thus, for hyperparameter search space, EA-HAS-Bench considers training epochs, and the most important factors in training schemes, i.e., base learning rate, decay policy of learning rate, optimizer and data augmentation. As a result, the search space of EA-HAS-Bench contains a total of 3.26 \u00d7 1010 configurations, including 6\u00d7 107 architectures and 540 training hyperparameter settings with variant training epochs. 2.2 EVALUATION METRICS: The EA-HAS-Bench dataset provides the following three types of metrics to evaluate different configurations. Model Complexity. Metrics related to model complexity include parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model. Model Performance. In order to support multi-fidelity NAS algorithms such as Hyperband (Li et al., 2017), EA-HAS-Bench provides the full training information including training and test accuracy learning curves for each hyperparameter and architecture configuration. Search Cost. Firstly, the energy cost (in kWh) and time (in seconds) to train a model under a certain configuration for one epoch are obtained. Then, by accumulating the energy consumption and runtime at each epoch, we obtain the total search cost of a configuration, which allows NAS/HPO methods to search optimal models under a limited cost budget. 2.3 BE\u0301ZIER CURVE-BASED SURROGATE MODEL: Due to the large EA-HAS-Bench search space, directly training all configurations in the space is infeasible even for a small dataset like CIFAR10. As a result, some of the metrics can not be directly measured, including model performance curve, search energy cost and runtime. Thus, similar to other recent works (Zela et al., 2022; Yan et al., 2021), we develop a surrogate model that expands the entire search space from a sampled subset of configurations. As for energy cost and training time, we follow the Surrogate NAS Benchmark (Zela et al., 2022) and train LGB models to predict these. However, for learning curve prediction, surrogate models proposed by the existing NAS-Bench are not applicable to EA-HAS-Bench. Since EA-HAS-Bench contains various maximum training epochs in the search space, non of the existing surrogate model can cope with dimensionally varying inputs and performs a dimensionality reduction operation on the learning curve. More importantly, it is not possible to directly constrain the learning curve after using the dimensionality reduction operation (e.g., the error rate should be between 0 and 1). In our early experiments, using the hyperparameter optimization algorithm on the NAS-BenchX11 surrogate model would yield results with more than 100% accuracy. To design a surrogate model that is specially designed for EA-HAS-Bench search space, we propose a Be\u0301zier Curvedbased Surrogate (BCS) Model to achieve learning curve prediction for arbitrary maximum epoch and direct constraint on final performance. Convert Learning Curve to Be\u0301zier Curve Control Points. For each configuration of network architecture and training hyperparameters, the surrogate model outputs the learning curve containing the accuracy or error rate of each epoch. Previous methods are not very efficient and robust for predicting learning curves with arbitrary lengths. Inspired by the success of the Be\u0301zier curve in other areas (Liu et al., 2020), we choose the Be\u0301zier curve to fit learning curves of arbitrary length. The shape of a Be\u0301zier curve is entirely determined by its control points, and degree n control points correspond to a Be\u0301zier curve of order n\u2212 1. The Be\u0301zier curve can be formulated in a recursive way as follows: P (t) = n\u2211 i=0 PiBi,n(t), t \u2208 [0, 1] (2) where Pi denotes control points, Bi,n(t) = Cint i(1\u2212 t)n\u2212i and i = 0, 1, \u00b7 \u00b7 \u00b7 , n. As a result, the task of regressing a learning curve of arbitrary length is simplified to predicting Be\u0301zier curve control points. Given a learning curve, {exi , eyi}mi=1 where ey is the error of the exth epoch and m is the maximum epoch, we need to get the optimal control points to generate Quartic Be\u0301zier curves to fit the learning curve. The control points are learned with the standard least square method as follows: B0,5 (t0) \u00b7 \u00b7 \u00b7 B5,5 (t0) B0,5 (t1) \u00b7 \u00b7 \u00b7 B5,5 (t1) ... . . . ... B0,5 (tm) \u00b7 \u00b7 \u00b7 B5,5 (tm)   ixstart iystart cx1 cy1 ... ... ixend iyend  =  ex0 ey0 ex1 ey1 ... ... exm eym  (3) Note that since the horizontal coordinates of the start and end points of the learning curve are fixed (i.e., ixstart = 1 and ixend = maximum epoch), the surrogate model only predicts the vertical coordinates of these two control points. An illustration of generated Be\u0301zier curves is shown in Figure 8 in Appendix B.3. Surrogate Model Structure. Given a RegNet architecture and hyperparameter configurations, BCS estimates the Be\u0301zier curve control points with a neural network. As shown in Fig.1, the proposed Be\u0301zier curve-based Surrogate model is composed of a hyperparameter encoder Eh, architecture encoder Ea, and learning curve prediction network f . The training hyperparameter configurations are represented as one-hot vector vh and then fed into Eh. The RegNet configuration parameters are normalized to values between 0 and 1, concatenated to a vector and fed into Ea. Finally, the hyperparameter representation and architecture representation are fed into the learning curve predictor to estimate Be\u0301zier curve starting/ending points and control points: {iystart , iyend , cx1 , cy1 , \u00b7 \u00b7 \u00b7 , cx3 , cy3} = f(Ea(va), Eh(vh)) (4) The learning curve predictor consists of two branches. One predicts the vertical coordinates of the starting and ending points of the Be\u0301zier curve, and the other branch predicts the other control points. With the control points obtained, we can generate Be\u0301zier curves with equation (2), and then obtain the accuracy of each epoch based on the horizontal coordinates exi of the curve. 2.4 DATASET COLLECTION: Some previous works (Eggensperger et al., 2015) propose to sample more training data from the high-performance regions of the search space because an effective optimizer spends most of its time in high-performance regions of the search space. However, this sampling strategy causes a distribution shift between the sampled training data and the actual search space, which hurts the prediction accuracy of the surrogate model. As discussed and verified in recent work (Zela et al., 2022), a search space containing hyper-parameters is more likely to produce dysfunctional models which are rarely covered in a sampled sub-set focusing on high-performance regions, and hence purely random sampled training data yields more satisfactory performance. In summary, for EAHAS-Bench\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark. The sampled architecture and hyperparameter configurations are trained and evaluated on two of the most popular image classification datasets, namely CIFAR-10 (Krizhevsky et al., 2009) and MicroImageNet challenge\u2019s (Tiny ImageNet) dataset (Le & Yang, 2015). 2.5 SURROGATE BENCHMARK EVALUATION: Comparison Methods. We compare the proposed method with six parametric learning curve based model (Domhan et al., 2015) (Exp3, ilog2, pow2, log power, log linear, vapor pressure) and three surrogate models (SVD-XGB, SVD-LGB, SVD-MLP) from NAS-Bench-X11 Yan et al. (2021). For a fair comparison, the parametric learning curve-based model applies the same network structure as our proposed BCS. For NAS-Bench-X11, we compress learning curves of different lengths (50, 100, and 200 epochs) into the hidden space with the same dimension with three different SVDs respectively (although this is not convenient to cope with learning curves of arbitrary length). Tree-of-Parzen-Estimators (TPE) (Bergstra et al., 2011) is adopted for BCS to search for the best hyperparameter configuration. The details of the experiments and ablation study are in Appendix B. Testing Set. The proposed surrogate model and the comparison methods are evaluated on a separate testing set trained on two sets of random seeds. The sampled configurations on CIFAR10 and TinyImageNet are split into training, validation and testing sets containing 11597, 1288, and 1000 samples respectively. Evaluation Metrics. Following Wen et al. (2020), White et al. (2021b) and Zela et al. (2022), we use the coefficient of determination R2 and the Kendall rank correlation coefficient \u03c4 as the evaluation metrics. Since the prediction results may have anomalies that make the prediction performance much higher than the actual results, we introduce spike anomalies (Yan et al., 2021) to evaluate these anomalies. Evaluation Results. The performance of surrogate models is shown in Table 3. First, the parametric learning curve-based models do not perform well, because the parametric learning curve function does not well fit the real learning curve in EA-HAS-Bench, some of the higher order functions even fail to fit, such as pow3 (c \u2212 ax\u2212\u03b1) and pow4 (c \u2212 (ax + b)\u2212\u03b1). On the other hand, the importance of the different parameters varies considerably, especially exponentially. We also show that the percentage of spike anomalies for real vs. BCS is 3.72% and 4.68% on CIFAR-10 and 0.83% and 1.31% on TinyImageNet, respectively. The energy cost surrogate models achieve 0.787 for R2 and 0.686 for KT on CIFAR-10 and 0.959 for R2 and 0.872 for KT on TinyImageNet. 3 DIFFERENCE WITH EXISTING NAS BENCHMARKS: Compared with existing NAS benchmarks such as NAS-Bench-101 (Ying et al., 2019), NAS-Bench201 (Dong & Yang, 2020), NAS-Bench-301 (Dong & Yang, 2020) or Surrogate NAS Bench (Zela et al., 2022), EA-HAS-Bench has three significant differences. Diverse and Large Scale Joint Search Space. EA-HAS-Bench is more diverse in terms of the types of configurations in the search space, which contains both model architectures and training hyperparameters. Although NAS-HPO-Bench (Klein & Hutter, 2019) and NAS-HPO-Bench-II (Hirose et al., 2021) also consider both architectures and hyperparameters, both benchmarks are based on small and homogeneous search spaces. Specifically, NAS-HPO-Bench focuses only on 2-layer feed-forward network training on tabular data, and the hyperparameters search space of NAS-HPOBench-II only contains learning rate and batch size. Besides the search space, diversity is also reflected in the evaluated performance ranges. As shown in Figure 2, the performance range of DARTS (Liu et al., 2019) used by NAS-Bench-301 is the smallest for the validation performance on CIFAR-10. Although DARTS contains more architectures, the performance of the models in this space is significantly less diverse (Yang et al., 2020). Compared with NAS-Bench-101/201, the configurations in EA-HAS-Bench cover a much larger performance range. Modeling Learning Curves for complex joint space. To the best of our knowledge, NAS-Bench-X11 (Yan et al., 2021) is the only existing surrogate model that provides the full training status over the entire training process. However, NAS-Bench-X11 is only available for learning curves with fixed maximum epochs. The number of training epochs required for convergence is not always the same for different architectures and it also directly affects the training energy cost. As a result, for a more realistic search space like EA-HAS-Bench, we propose BSC to predict learning curves with different maximum epochs. Full-cycle Energy Consumption. Most existing benchmarks use model training time as the training resource budget, which is not an accurate estimation of energy cost. Firstly, the GPU does not function at a stable level throughout the entire training process, or it may not even be working at all for a significant time period during training, and hence longer training time does not always mean more GPU usage and more energy cost. Secondly, energy cost not only corresponds to training time but also relates to the energy cost per unit time, which is affected by the architecture and hyperparameter configurations. Our experimental analysis in the next section also verifies that training time and energy cost are not equivalent. HW-NAS-Bench (Li et al., 2021b) also consider energy consumption, but its focus is on the model inference energy cost. On the other hand, EA-HAS-Bench provides a full-cycle energy consumption, both during training and inference. The energy consumption metric allows HPO algorithms to optimize for accuracy under an energy cost limit (Section 5). 4 ANALYSIS ON EA-HAS-BENCH: Impact of Hyperparameter Configurations. Since most existing large-scale computer vision NAS benchmarks focus solely on network architectures and apply the same hyperparameters for all models, we examine how different hyperparameters affect the searched model performance. We use a empirical cumulative distribution (ECDF) (Radosavovic et al., 2020) to assess the quality of search space. Specifically, we take a set of configurations from the search space and characterize its error distribution. Figure 3 shows the empirical cumulative distribution (ECDF) of different training hyperparameters on CIFAR-10 and TinyImageNet. We observe that the learning rate and the choice of optimizer may have the most significant impact on the search space quality. The size of the maximum number of epochs is almost proportional to the quality of the search space and increases the search cost linearly. In addition, through investigation of data augmentation, we found that cutout achieves opposite effects on different datasets. Correlation Between Training Energy Cost and Training time. Figure 4 investigates the relationship between the training energy consumption (TEC), training time and the test accuracy of models in TinyImageNet. Firstly, we observe that the points in the left figure of Figure 4 is rather scattered. This means the correlation between training time and energy cost is not strong. Although training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The middle and right plots of Figure 4 also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (middle figure), showing that training time and energy cost are not equivalent. Meanwhile, training a model longer (or with more energy) does not guarantee better accuracy. In the middle and right plots of Figure 4, we see that many models with high TECs, still fail to train, due to improper neural architectures or hyperparameter configurations. On the other hand, simply searching for the best accuracy might not be cost-efficient, since there are quite a few configurations with the same level of accuracy. The observation motivates finding the best trade-off between accuracy and energy consumption. Configurations\u2013Accuracy/Energy Correlation. Figure 5 shows the correlation between architecture/ hyperparameter configurations and accuracy/energy cost. We observe that hyperparameters like learning rate also have a high correlation with model performance, which further verifies the importance to consider hyperparameters in NASBench. Both network architecture like network depth and width and hyperparameter like training epoch have a relatively strong correlation with energy cost, showing the importance of considering both perspectives in our energy-aware Benchmark. 5 EA-HAS-BENCH AS A BENCHMARK: EA-HAS-Bench saves tremendous resources to train and evaluate the configurations for real. We demonstrate how to leverage the proposed dataset to conduct energy-aware AutoML research with two use cases. Firstly, we evaluate the trade-off between search energy cost and model performance of some of the existing AutoML methods (Li et al., 2017; White et al., 2021a; 2020; Falkner et al., 2018; Li & Talwalkar, 2019; Real et al., 2019) with the proposed EA-HAS-Bench dataset. Then, in the second sub-section, as the first step toward a long journey of energy-aware AutoML, we arm several existing AutoML algorithms with another energy-related objective and verify the effectiveness of energy-aware AutoML on our hyperparameter-architecture benchmark. 5.1 BENCHMARKING EXISTING ALGORITHMS: Experimental setup. Since EA-HAS-Bench focuses on the trade-off between model performance and search energy cost, in this experiment we use the total search energy cost as the resource limitation, instead of training time. As a result, we set the maximum search cost to roughly 40,000 kWh for CIFAR 10 and 250,000kWh for TinyImageNet, which is equivalent to running a single-fidelity HPO algorithm for about 1,000 iterations. Results. In Figure 6 left and middle, we compare single- and multi-fidelity algorithms on the search space of EA-HAS-Bench. For single-fidelity algorithms, LS is the top-performing algorithm across two datasets. This shows that similar to NAS-Bench, HAS-Bench with a joint search space also has locality, a property by which \u201dclose\u201d points in search space tend to have similar performance. 5.2 A NEW ENERGY-AWARE HPO BASELINE: Energy Aware HPO Most existing HPO methods do not directly consider the search energy cost. In order to verify the importance to consider the energy cost during the search process, we propose a new energy-aware HPO baseline by modifying existing HPO methods. Specifically, we modify the BANANAS (White et al., 2021a) and LS (White et al., 2020) by changing the optimization goal to a metric that considers both accuracy and energy cost: ACC \u00d7 (TECT ) w, where T is the target TEC. For CIFAR10, we set the T to 0.45 and w to -0.07. Experimental Setup and Result We explore another important usage scenario, where the goal is to achieve a target model performance using as little energy cost as possible. As a result, we use model performance rather than energy consumption as a resource limitation, and stop the search when the model hits a target performance and compare the corresponding search energy cost. For CIFAR10, the target accuracy is set to 97%. As shown in Figure 6 right, EA algorithms that consider TEC save close to 20% of search energy consumption compared to the origin algorithms in achieving the target accuracy. 6 CONCLUSION: EA-HAS-Bench is the first large-scale energy-aware hyperparameter and architecture search benchmark. The search space of EA-HAS-Bench consists of both network architecture scale and training hyperparameters, covering diverse conficurations related to energy cost. A novel Be\u0301zier curve-based surrogate model is proposed for the new joint search space. Furthermore, we analyse the difference between existing NAS benchmarks and EA-HAS-Bench and dataset statistics and correlation of the collected data. Finally, we provide use cases of EA-HAS-Bench to show that energy-aware algorithms can save significant energy in the search process. We expect that our EA-HAS-Bench expedites and facilitates the EA-NAS and HAS research innovations. A RELATED WORK: A.1 EXISTING NAS BENCHMARKS Tabular NAS Benchmarks. While neural architecture search (NAS) has succeeded in various practical tasks such as image recognition (Pham et al., 2018) and sequence modelling (So et al., 2019), the non-reproducibility of NAS has been contested (Li & Talwalkar, 2019). One of the main reasons that complicate NAS reproducibility studies is the high computational cost as well as the large carbon emissions that result(Li et al., 2021a). For the reproducibility of NAS research, NASBench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong & Yang, 2020) have been proposed. Due to the success of these tabular benchmarks in image classification, corresponding benchmarks have been proposed in areas such as NLP and speech recognition (Mehta et al., 2022). Surrogate NAS Benchmarks. The recent Surrogate NAS Benchmarks (Zela et al., 2022) builds surrogate benchmarks on a realistic search space and shows strong generalization performance. To extend the surrogate model, NAS-Bench-X11 (Yan et al., 2021) uses singular value decomposition and noise modelling to output the learning curve. NAS Benchmarks joint HPO Most NAS Benchmarks have fixed training hyperparameters, but the training hyperparameters strongly influence the performance of the model obtained by NAS (Dong et al., 2020). To alleviate this problem, NAS-HPO-Bench (Klein & Hutter, 2019) and NASHPO-Bench-II Hirose et al. (2021) are proposed. However, as shown in Table 1, the size of the two Benchmarks is small and the search spaces are simple. The architecture space of NAS-HPO-Bench is multi-layer perception (MLP) trained on the tabular datasets for regression tasks. NAS-HPOBench-II only really measure 12 epochs using the CIFAR-10 dataset Krizhevsky et al. (2009) and the training hyperparameters space only contains learning rate and batch size. A.2 RESOURCE-AWARE NAS Early NAS algorithms (Zoph & Le, 2016; Real et al., 2019) focused only on performance and ignored some of the associated hardware consumption. To this end, many resource-aware NAS algorithms are proposed to balance performance and resource budgets (He et al., 2021). These resourceaware NAS algorithms focus on the four types of computational costs that are included the FLOPs, parameter size, the number of Multiply-ACcumulate (MAC) operations and real latency. Among the NAS algorithms, two classic works are MnasNet (Tan et al., 2019) and FBNet (Wu et al., 2019). MnasNet proposes a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. Similar to MansNet, FBNet designs a loss function to trade off the cross-entropy loss and latency of architecture. However, none of the above on NAS algorithms focus on the huge energy consumption in search. B MORE DETAILS OF EA-HAS-BENCH: B.1 RegNet VS RegNet + HPO As shown in the figure 7, we compare the learning curves of one hundred samples actually measured from under two search spaces-RegNet and RegNet+HPO. The different architectures converge effectively under well-designed training hyperparameters. However, many architectures fail to converge under the search space of RegNet+HPO. But the latter may find a better combination to produce better performance. With a fixed training hyperparameter, the best model obtained by searching only in the network architecture space is not necessarily the true best model. At these fixed training hyperparameters, the sub-optimal model obtained by searching may get better performance under another set of training hyperparameters. In addition, the optimal hyperparameters are difficult to determine when facing a new dataset, while searching for training hyperparameters and model architecture is a more realistic scenario. Parametric Learning Curves Based Models. We use the parametric models to fit the entire learning curve and the formula of those parametric models is shown in Table 4. Following Domhan et al. (2015), we first try the Levenberg-Marquardt algorithm and fall back to Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) in case that fails. In the experiment, we found that the initial parameters are important in the fitting process. Some parametric models of a high order cannot be fitted to the learning curve in the training set because suitable initial parameters cannot be found. In contrast, the initial point of the BSC model is the starting and ending point of the learning curve. Hyperparameters of Surrogate Models. For the optimal three surrogate models in Table 3 and the LGB energy cost model (LGB-E), the optimal parameters found using TPE and the search space are listed in Table 5. We used a fixed budget of 500 trials for all surrogate models and average R2 as the optimal target. Models of Runtime Metrics Although our benchmark focuses on energy consumption, we also provide runtimes to allow NAS methods to use runtimes as budgets. We train an LGB model with the average runtime of each epoch as a target and the model achieves 0.926 for R2 and 0.849 for KT on runtime prediction on CIFAR10. B.3 ABLATION STUDY In Table 6, we show the effect of different degrees of the Be\u0301zier curve on the prediction performance. The higher the degree, the better the Be\u0301zier curve fits the real learning curve, but it also leads to overfitting. As the degree increases, the prediction performance becomes worse instead. When the Degree is 4, the Quadratic Be\u0301zier curve achieves the best results. C MORE ANALYSIS OF EA-HAS-BENCH: Distributions of Power. The correlation between energy consumption and runtime is energy = runtime \u00d7 Avg.power. As shown in Figur9, since the power of different models trained on Tesla V100 is not a constant, the energy consumption is not linearly related to the runtime. D MORE DETAILS OF SECTION 5: Details of NAS algorithms. All algorithms we use in Section 5 are based on NASLib (Ruchte et al., 2020). For regularized evolution and local search, we modify them to suit our joint search space. The algorithm description and implementation details are as follows. \u2022 Random Search is a classic baseline of HPO algorithms and is the basis for some complex algorithms (Li & Talwalkar, 2019). For EA-HAS-Bench, we sample randomly both the architecture and the hyperparameter space. The sampling type is the same as the dataset that we build for training the surrogate model. \u2022 Local search iteratively evaluates all architectures in the neighbourhood of the current best architecture found so far (White et al., 2020). For the search space of EA-HAS-Bench, we define neighbourhood as having the same network architecture or the same training hyperparameters, i.e., points with the same subspace are neighbouring points. \u2022 Regularized evolution mutates the best architecture from a sample of all architectures evaluated so far. For EA-HAS-Bench, we define a mutation as a random change of one dimension in the architecture or hyperparameters. \u2022 BANANAS is based on Bayesian optimization and samples the next point by acquisition function (White et al., 2021a). We used a modified version from NAS-Bench-X11 to construct the candidate pool by mutating the best four points 10 times each. \u2022 Hyperband is based on the random search with successive halving (Li et al., 2017). Since the space size of EA-NAS-Bench is 3\u00d7 1010, we expect to explore more points and set the maximum budget to 512 and the minimum budget to 4. \u2022 Bayesian optimization Hyperband is based on Hyperband with Bayesian optimization (Falkner et al., 2018). We use the same parameters as Hyperhand. EA algorithms on TinyImageNet. For TinyImageNet, we set T to 2.5 and target performance to 57%. The result is shown in Figure 10. Same as the experiment on CIFAR-10, compared with the baseline without the energy cost penalty, our EA algorithm cost a significantly lower search energy to reach the target accuracy.\",\n  \"agent_name\": \"Reviewer3\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Review of \"EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark\"\\n\\nThis paper introduces EA-HAS-Bench, a new benchmark for energy-aware hyperparameter and architecture search (HAS).  The authors address the growing concern of energy consumption in AutoML, arguing that existing benchmarks inadequately capture the total energy cost of the search process.  EA-HAS-Bench aims to remedy this by providing a large joint search space encompassing both architectural and hyperparameter configurations, along with associated energy consumption and performance metrics. A novel B\u00e9zier curve-based surrogate model (BCS) is proposed to efficiently predict performance across this vast search space.\\n\\n**Strengths:**\\n\\n* **Addresses a crucial problem:** The focus on energy consumption in AutoML is timely and important.  The authors correctly highlight the significant environmental impact of computationally intensive AutoML methods.\\n* **Large-scale and comprehensive benchmark:**  The sheer size of the EA-HAS-Bench search space (10 billion configurations) is a significant contribution. The inclusion of both architectural and hyperparameter variations is crucial for realistic evaluation of energy-aware methods.\\n* **Novel surrogate model:** The BCS model offers a potentially effective approach to handle the large search space and variable-length learning curves. The use of B\u00e9zier curves is innovative and addresses a limitation of existing surrogate models.\\n* **Thorough empirical evaluation:**  The paper includes extensive experiments comparing the proposed BCS model to existing methods and demonstrating the effectiveness of energy-aware AutoML algorithms on the benchmark.\\n\\n\\n**Weaknesses:**\\n\\n* **Theoretical justification of BCS:** While the BCS model is novel, the paper lacks a rigorous theoretical analysis of its properties.  The authors should provide a more formal justification for the choice of B\u00e9zier curves and discuss potential limitations or biases of this approach.  What are the guarantees on the accuracy and generalization ability of the BCS model?  How does the choice of degree affect the model\\'s performance and susceptibility to overfitting?  A more detailed analysis of the model\\'s capacity and expressiveness would strengthen the paper significantly.\\n* **Generalization ability of the benchmark:**  The paper focuses on CIFAR-10 and TinyImageNet.  How well do the findings generalize to other datasets and tasks?  The benchmark\\'s value would be significantly enhanced by demonstrating its applicability beyond these two datasets.  A discussion of potential limitations and biases due to dataset selection is needed.\\n* **Energy modeling assumptions:** The paper mentions that training time is not a perfect proxy for energy consumption. While this is true, the authors need to detail their methodology for accurately measuring and modeling energy consumption.  What hardware was used?  What factors were considered (e.g., idle time, GPU utilization)?  Transparency in this aspect is critical for reproducibility.\\n* **Energy-aware AutoML methods:** The proposed modifications to existing AutoML algorithms are relatively simple.  A more in-depth investigation of novel energy-aware optimization strategies would significantly improve the paper\\'s contribution.\\n\\n\\n**Clarity and Presentation:**\\n\\nThe paper is generally well-written and organized. However, some sections could benefit from improved clarity.  The explanation of the BCS model, in particular, could be more concise and intuitive.  More visual aids (e.g., diagrams) would be helpful in explaining the complex interactions between architectural parameters, hyperparameters, and energy consumption.\\n\\n**Overall Assessment:**\\n\\nThe paper presents a valuable contribution by addressing the critical issue of energy consumption in AutoML. The creation of EA-HAS-Bench is a significant step towards more reproducible and responsible AutoML research. However, the weaknesses related to the theoretical justification of the BCS model and the limited generalization analysis prevent the paper from reaching its full potential. Addressing these weaknesses would significantly strengthen the paper.\\n\\n\\n**Recommendation:**\\n\\nWhile the paper\\'s overall contribution is significant, the lack of rigorous theoretical justification for the key methodological components (BCS model) and the limited scope of the empirical evaluation currently prevent acceptance in its current form.  I recommend a **Reject**, but encourage the authors to address the weaknesses outlined above, particularly the theoretical analysis of the BCS model and generalization experiments on other datasets, before resubmission to a suitable venue.\\n\\n\\nFINAL DECISION: Reject\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.761378494999999}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here's a summary of the reviews:\n\n*   **Reviewer 1:** Recommends **Accept (with major revisions)**. Highlights the timely problem, large-scale benchmark, and novel surrogate model as strengths. Weaknesses include limited novelty in energy-aware modifications and lack of comparison with other energy-aware methods.\n*   **Reviewer 2:** Recommends **Accept (Conditional)**. Strengths include addressing an important problem, large-scale benchmark, and novel surrogate model. Weaknesses include reproducibility concerns and lack of baseline comparison for energy-aware methods.\n*   **Reviewer 3:** Recommends **Reject**. Strengths include addressing a crucial problem, a large-scale benchmark, and a novel surrogate model. Weaknesses include the theoretical justification of BCS and the generalization ability of the benchmark.\n\nBased on the reviews, the paper presents a valuable contribution to an important and timely problem. The creation of the EA-HAS-Bench is a significant achievement. However, there are concerns about reproducibility, the theoretical justification of the BCS model, and the limited novelty of the energy-aware modifications.\n\nGiven that two reviewers recommend acceptance with revisions and one recommends rejection, I believe the paper has merit but requires significant improvements before being considered for publication.\n\n**FINAL DECISION: Accept (Conditional)**\n"}], "ground_truth": "Accept: notable-top-25%", "response_time": 260.4924910068512}
