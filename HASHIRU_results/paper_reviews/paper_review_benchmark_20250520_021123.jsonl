{"paper_id": "P8DHF1Y_dph", "prompt_snippet": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed", "agent_review": [{"role": "user", "content": "Create THREE agents with relevant personalities, expertise, and review styles. Each agent should provide a review of the paper, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\n\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS: "}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": ""}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "I will create three agents to review the paper, following your instructions. I will then synthesize their reviews and provide a final decision.\n\n**Step 1: Get a list of available agents.**\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetAgents'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agents list retrieved successfully\",\n  \"agents\": {\n    \"ReviewerICLR1\": {\n      \"description\": \"Expert in robotics and generative models, provides detailed and constructive reviews.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"ReviewerICLR2\": {\n      \"description\": \"Experienced machine learning researcher, focuses on the clarity and novelty of the paper.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"ReviewerICLR3\": {\n      \"description\": \"Specializes in evaluating the experimental setup and results of machine learning papers.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275,\n      \"base_model\": \"gemini-1.5-pro\"\n    },\n    \"ReviewerFlash\": {\n      \"description\": \"A reviewer that uses the Gemini Flash model to provide quick reviews.\",\n      \"create_resource_cost\": 0,\n      \"invoke_resource_cost\": 0,\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017,\n      \"base_model\": \"gemini-1.5-flash\"\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetAgents` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetAgents', response={'result': {'status': 'success', 'message': 'Agents list retrieved successfully', 'agents': {'ReviewerICLR1': {'description': 'Expert in robotics and generative models, provides detailed and constructive reviews.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'ReviewerICLR2': {'description': 'Experienced machine learning researcher, focuses on the clarity and novelty of the paper.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'ReviewerICLR3': {'description': 'Specializes in evaluating the experimental setup and results of machine learning papers.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275, 'base_model': 'gemini-1.5-pro'}, 'ReviewerFlash': {'description': 'A reviewer that uses the Gemini Flash model to provide quick reviews.', 'create_resource_cost': 0, 'invoke_resource_cost': 0, 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017, 'base_model': 'gemini-1.5-flash'}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It appears that there are already three agents available: `ReviewerICLR1`, `ReviewerICLR2`, and `ReviewerICLR3`. These agents seem well-suited for the task.\n\n**Step 2: Check the budget and costs associated with invoking the agents.**\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"llama3.2\": {\n      \"description\": \"3 Billion parameter model\",\n      \"create_resource_cost\": 50,\n      \"invoke_resource_cost\": 30\n    },\n    \"mistral\": {\n      \"description\": \"7 Billion parameter model\",\n      \"create_resource_cost\": 75,\n      \"invoke_resource_cost\": 40\n    },\n    \"deepseek-r1\": {\n      \"description\": \"7 Billion reasoning model\",\n      \"create_resource_cost\": 28,\n      \"invoke_resource_cost\": 35\n    },\n    \"gemini-2.5-flash-preview-04-17\": {\n      \"description\": \"Adaptive thinking, cost efficiency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.5-pro-preview-03-25\": {\n      \"description\": \"Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash\": {\n      \"description\": \"Next generation features, speed, thinking, realtime streaming, and multimodal generation\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-2.0-flash-lite\": {\n      \"description\": \"Cost efficiency and low latency\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash\": {\n      \"description\": \"Fast and versatile performance across a diverse variety of tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-flash-8b\": {\n      \"description\": \"High volume and lower intelligence tasks\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.00017\n    },\n    \"gemini-1.5-pro\": {\n      \"description\": \"Complex reasoning tasks requiring more intelligence\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.0001275\n    },\n    \"gemini-2.0-flash-live-001\": {\n      \"description\": \"Low-latency bidirectional voice and video interactions\",\n      \"create_expense_cost\": 0.005,\n      \"invoke_expense_cost\": 0.000635\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'llama3.2': {'description': '3 Billion parameter model', 'create_resource_cost': 50, 'invoke_resource_cost': 30}, 'mistral': {'description': '7 Billion parameter model', 'create_resource_cost': 75, 'invoke_resource_cost': 40}, 'deepseek-r1': {'description': '7 Billion reasoning model', 'create_resource_cost': 28, 'invoke_resource_cost': 35}, 'gemini-2.5-flash-preview-04-17': {'description': 'Adaptive thinking, cost efficiency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.5-pro-preview-03-25': {'description': 'Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash': {'description': 'Next generation features, speed, thinking, realtime streaming, and multimodal generation', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-2.0-flash-lite': {'description': 'Cost efficiency and low latency', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash': {'description': 'Fast and versatile performance across a diverse variety of tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-flash-8b': {'description': 'High volume and lower intelligence tasks', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.00017}, 'gemini-1.5-pro': {'description': 'Complex reasoning tasks requiring more intelligence', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.0001275}, 'gemini-2.0-flash-live-001': {'description': 'Low-latency bidirectional voice and video interactions', 'create_expense_cost': 0.005, 'invoke_expense_cost': 0.000635}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The `invoke_expense_cost` for `ReviewerICLR1`, `ReviewerICLR2`, and `ReviewerICLR3` is $0.0001275 each. This seems affordable.\n\n**Step 3: Invoke the agents to review the paper.**\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\\n\\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:', 'agent_name': 'ReviewerICLR1'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Tool `AskAgent` failed to run.\",\n  \"output\": \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}\"\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\n\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:\",\n  \"agent_name\": \"ReviewerICLR1\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'error', 'message': 'Tool `AskAgent` failed to run.', 'output': \"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}\"}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "It appears I have exceeded my quota for the `gemini-1.5-pro` model. I will try using `ReviewerFlash` which uses the `gemini-1.5-flash` model instead.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerFlash', 'prompt': 'Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\\n\\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Learning to Generate All Feasible Actions\n\nThis paper introduces a novel method for training a generative neural network to produce all feasible actions for a given subtask within an interactive environment.  The core idea is to frame the problem as minimizing an f-divergence between the generated action distribution and the true distribution of feasible actions, estimated via interaction with the environment (or a learned critic).  The authors propose a novel gradient estimator using Kernel Density Estimation (KDE), resampling, and importance sampling, addressing challenges related to estimating densities and ensuring convergence. The method is evaluated on a simplified 2D robotic grasping task, comparing different f-divergences (FKL, RKL, JS) against Maximum Entropy RL and Conditional GANs.\n\n**Strengths:**\n\n* **Novel Approach:** The core idea of learning *all* feasible actions, rather than just an optimal one, is novel and addresses a significant limitation of existing skill learning methods. This approach could be particularly valuable in situations requiring robustness and adaptability to unforeseen downstream tasks.\n* **Comprehensive Methodology:** The paper provides a detailed description of the proposed method, including the derivation of the gradient estimator, the use of KDE and importance sampling, and the strategies for handling imbalanced data and maximizing information gain during training (balanced replay memory and maximum uncertainty collection).\n* **Rigorous Evaluation:** The authors conduct a thorough experimental evaluation, comparing their method against relevant baselines on a well-defined, albeit simplified, robotic grasping task. The use of multiple f-divergences and the inclusion of action optimization allow for a comprehensive analysis of the approach's performance.  The visualizations effectively communicate the multimodal nature of the problem and the performance of different methods.\n* **Addressing Convergence Issues:** The authors explicitly acknowledge and address the convergence issues encountered during development, providing valuable insights into the challenges of training generative models in this context.  The resampling strategy is a key contribution to the method\u2019s success.\n\n**Weaknesses:**\n\n* **Simplified Task:** The 2D robotic grasping task, while useful for demonstrating the core concept, is significantly simplified compared to real-world robotic manipulation.  The generalizability of the approach to higher-dimensional, more complex tasks remains unclear.  The authors acknowledge this limitation but a more substantial discussion of scalability is needed.\n* **Computational Cost:**  The method requires significant computational resources, as noted by the authors (48 hours on a single NVIDIA A100 GPU for 1 million interaction steps).  This raises concerns about applicability to more complex tasks and larger datasets.  A detailed analysis of computational complexity would strengthen the paper.\n* **Hyperparameter Sensitivity:** The paper mentions hyperparameters (bandwidths of KDEs, etc.) but doesn't delve into their sensitivity or provide guidance on tuning them.  A more comprehensive sensitivity analysis is needed to assess the robustness of the method.\n* **KDE Limitations:** The reliance on KDE for density estimation is a potential limitation, particularly in higher-dimensional spaces.  The paper briefly mentions this limitation, but a more detailed discussion of potential alternatives or strategies for mitigating the curse of dimensionality would be beneficial.\n\n\n**Clarity and Presentation:**\n\nThe paper is generally well-written and clearly structured.  The mathematical derivations are presented systematically, and the figures effectively illustrate the method and results.  However, some sections could benefit from more concise explanations.\n\n\n**Originality and Significance:**\n\nThe core idea of learning all feasible actions is novel and potentially impactful. The proposed method and gradient estimator contribute to the field of generative models for interactive learning. However, the simplified experimental setting limits the immediate impact.\n\n\n**Recommendation:**\n\nThe paper presents a promising novel approach to skill learning, with a strong theoretical foundation and a reasonably thorough experimental evaluation.  However, the limitations related to scalability and computational cost need to be addressed more comprehensively.  The paper would benefit from a more detailed discussion of these limitations and potential mitigation strategies, as well as a more in-depth analysis of hyperparameter sensitivity.\n\nFINAL DECISION: Accept (with major revisions)\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.8192112875\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ReviewerFlash\",\n  \"prompt\": \"Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\n\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': \"## ICLR 2023 Review: Learning to Generate All Feasible Actions\\n\\nThis paper introduces a novel method for training a generative neural network to produce all feasible actions for a given subtask within an interactive environment.  The core idea is to frame the problem as minimizing an f-divergence between the generated action distribution and the true distribution of feasible actions, estimated via interaction with the environment (or a learned critic).  The authors propose a novel gradient estimator using Kernel Density Estimation (KDE), resampling, and importance sampling, addressing challenges related to estimating densities and ensuring convergence. The method is evaluated on a simplified 2D robotic grasping task, comparing different f-divergences (FKL, RKL, JS) against Maximum Entropy RL and Conditional GANs.\\n\\n**Strengths:**\\n\\n* **Novel Approach:** The core idea of learning *all* feasible actions, rather than just an optimal one, is novel and addresses a significant limitation of existing skill learning methods. This approach could be particularly valuable in situations requiring robustness and adaptability to unforeseen downstream tasks.\\n* **Comprehensive Methodology:** The paper provides a detailed description of the proposed method, including the derivation of the gradient estimator, the use of KDE and importance sampling, and the strategies for handling imbalanced data and maximizing information gain during training (balanced replay memory and maximum uncertainty collection).\\n* **Rigorous Evaluation:** The authors conduct a thorough experimental evaluation, comparing their method against relevant baselines on a well-defined, albeit simplified, robotic grasping task. The use of multiple f-divergences and the inclusion of action optimization allow for a comprehensive analysis of the approach's performance.  The visualizations effectively communicate the multimodal nature of the problem and the performance of different methods.\\n* **Addressing Convergence Issues:** The authors explicitly acknowledge and address the convergence issues encountered during development, providing valuable insights into the challenges of training generative models in this context.  The resampling strategy is a key contribution to the method\u2019s success.\\n\\n**Weaknesses:**\\n\\n* **Simplified Task:** The 2D robotic grasping task, while useful for demonstrating the core concept, is significantly simplified compared to real-world robotic manipulation.  The generalizability of the approach to higher-dimensional, more complex tasks remains unclear.  The authors acknowledge this limitation but a more substantial discussion of scalability is needed.\\n* **Computational Cost:**  The method requires significant computational resources, as noted by the authors (48 hours on a single NVIDIA A100 GPU for 1 million interaction steps).  This raises concerns about applicability to more complex tasks and larger datasets.  A detailed analysis of computational complexity would strengthen the paper.\\n* **Hyperparameter Sensitivity:** The paper mentions hyperparameters (bandwidths of KDEs, etc.) but doesn't delve into their sensitivity or provide guidance on tuning them.  A more comprehensive sensitivity analysis is needed to assess the robustness of the method.\\n* **KDE Limitations:** The reliance on KDE for density estimation is a potential limitation, particularly in higher-dimensional spaces.  The paper briefly mentions this limitation, but a more detailed discussion of potential alternatives or strategies for mitigating the curse of dimensionality would be beneficial.\\n\\n\\n**Clarity and Presentation:**\\n\\nThe paper is generally well-written and clearly structured.  The mathematical derivations are presented systematically, and the figures effectively illustrate the method and results.  However, some sections could benefit from more concise explanations.\\n\\n\\n**Originality and Significance:**\\n\\nThe core idea of learning all feasible actions is novel and potentially impactful. The proposed method and gradient estimator contribute to the field of generative models for interactive learning. However, the simplified experimental setting limits the immediate impact.\\n\\n\\n**Recommendation:**\\n\\nThe paper presents a promising novel approach to skill learning, with a strong theoretical foundation and a reasonably thorough experimental evaluation.  However, the limitations related to scalability and computational cost need to be addressed more comprehensively.  The paper would benefit from a more detailed discussion of these limitations and potential mitigation strategies, as well as a more in-depth analysis of hyperparameter sensitivity.\\n\\nFINAL DECISION: Accept (with major revisions)\\n\", 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.8192112875}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerFlash', 'prompt': 'Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\\n\\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Learning to Generate All Feasible Actions\n\nThis paper presents a novel method for training a generative neural network to produce all feasible actions for a given subtask within an interactive environment.  The approach cleverly frames the problem as minimizing an f-divergence between the generated action distribution and the true distribution of feasible actions, estimated using a critic network.  The authors propose a novel gradient estimator utilizing Kernel Density Estimation (KDE), resampling, and importance sampling, addressing challenges associated with estimating densities and ensuring convergence. The method is evaluated on a simplified 2D robotic grasping task, comparing various f-divergences (FKL, RKL, JS) against maximum entropy RL and conditional GANs.\n\n**Strengths:**\n\n* **Novel Approach:** The core idea of learning all feasible actions rather than a single optimal action is unique and addresses a significant limitation of existing skill learning methods.  This opens up possibilities for more robust and adaptable skills in complex scenarios.\n* **Methodological Rigor:** The paper provides a thorough mathematical derivation of the proposed gradient estimator, clearly explaining the challenges and the rationale behind the design choices (e.g., resampling, importance sampling).  The authors acknowledge and address the convergence issues encountered during development.\n* **Comprehensive Evaluation:** The experimental setup, while simplified, allows for a controlled comparison of different methods and divergence measures.  The authors present both qualitative (heatmaps) and quantitative (accuracy, mode coverage) results, providing a solid evaluation of the proposed approach.  The inclusion of Maximum Entropy RL and Conditional GANs as baselines is appropriate.\n* **Clear Writing:** The paper is generally well-written and easy to follow. The authors effectively explain the motivation, methodology, and results.\n\n\n**Weaknesses:**\n\n* **Simplified Task:** The 2D robotic grasping task, while useful for demonstrating the core idea, significantly limits the generalizability of the findings.  Scaling this approach to higher-dimensional, more complex tasks is a crucial next step. The authors acknowledge this limitation, but a more detailed discussion of the challenges involved in scaling up would strengthen the paper.\n* **Computational Cost:** The method relies on computationally expensive KDE and resampling steps, potentially hindering its applicability to real-world, high-dimensional problems.  A discussion of the computational complexity and potential strategies for optimization (e.g., approximation methods for KDE) would be valuable.\n* **Convergence Issues:**  While the authors address convergence issues by introducing resampling and a balanced replay memory, a more in-depth analysis of the convergence properties (theoretical guarantees or empirical analysis under different conditions) would strengthen the claims.  The authors mention potential reasons for the initial convergence problems, but a more detailed investigation and analysis would be beneficial.\n* **Hyperparameter Sensitivity:** The paper does not provide a thorough analysis of the sensitivity of the method to hyperparameter choices (e.g., bandwidths of KDEs, resampling rate).  This would provide insights into the robustness and practical applicability of the proposed approach.\n\n\n**Overall Assessment:**\n\nThe paper presents a valuable contribution to the field of skill learning by introducing a novel approach for generating all feasible actions. The methodological rigor and comprehensive evaluation are commendable. However, the simplified experimental setup and lack of deeper analysis of convergence and scalability raise concerns about the generalizability of the findings.  Addressing the weaknesses outlined above would significantly strengthen the paper.\n\n\n**Recommendation:**\n\nWhile the core idea is promising and the paper presents a solid contribution, the limitations regarding scalability and the need for a more thorough analysis of convergence prevent it from being ready for acceptance in its current form.  I recommend a **Reject** for ICLR 2023, but encourage the authors to address the weaknesses mentioned above and resubmit to a future conference or journal.  Addressing the scalability concerns, providing a more detailed analysis of the convergence issues and hyperparameter sensitivity, and possibly extending the experiments to more complex tasks will be crucial for improving the impact and significance of this work.\n\n\n**FINAL DECISION: Reject**\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.8183974975\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ReviewerFlash\",\n  \"prompt\": \"Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\n\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## ICLR 2023 Review: Learning to Generate All Feasible Actions\\n\\nThis paper presents a novel method for training a generative neural network to produce all feasible actions for a given subtask within an interactive environment.  The approach cleverly frames the problem as minimizing an f-divergence between the generated action distribution and the true distribution of feasible actions, estimated using a critic network.  The authors propose a novel gradient estimator utilizing Kernel Density Estimation (KDE), resampling, and importance sampling, addressing challenges associated with estimating densities and ensuring convergence. The method is evaluated on a simplified 2D robotic grasping task, comparing various f-divergences (FKL, RKL, JS) against maximum entropy RL and conditional GANs.\\n\\n**Strengths:**\\n\\n* **Novel Approach:** The core idea of learning all feasible actions rather than a single optimal action is unique and addresses a significant limitation of existing skill learning methods.  This opens up possibilities for more robust and adaptable skills in complex scenarios.\\n* **Methodological Rigor:** The paper provides a thorough mathematical derivation of the proposed gradient estimator, clearly explaining the challenges and the rationale behind the design choices (e.g., resampling, importance sampling).  The authors acknowledge and address the convergence issues encountered during development.\\n* **Comprehensive Evaluation:** The experimental setup, while simplified, allows for a controlled comparison of different methods and divergence measures.  The authors present both qualitative (heatmaps) and quantitative (accuracy, mode coverage) results, providing a solid evaluation of the proposed approach.  The inclusion of Maximum Entropy RL and Conditional GANs as baselines is appropriate.\\n* **Clear Writing:** The paper is generally well-written and easy to follow. The authors effectively explain the motivation, methodology, and results.\\n\\n\\n**Weaknesses:**\\n\\n* **Simplified Task:** The 2D robotic grasping task, while useful for demonstrating the core idea, significantly limits the generalizability of the findings.  Scaling this approach to higher-dimensional, more complex tasks is a crucial next step. The authors acknowledge this limitation, but a more detailed discussion of the challenges involved in scaling up would strengthen the paper.\\n* **Computational Cost:** The method relies on computationally expensive KDE and resampling steps, potentially hindering its applicability to real-world, high-dimensional problems.  A discussion of the computational complexity and potential strategies for optimization (e.g., approximation methods for KDE) would be valuable.\\n* **Convergence Issues:**  While the authors address convergence issues by introducing resampling and a balanced replay memory, a more in-depth analysis of the convergence properties (theoretical guarantees or empirical analysis under different conditions) would strengthen the claims.  The authors mention potential reasons for the initial convergence problems, but a more detailed investigation and analysis would be beneficial.\\n* **Hyperparameter Sensitivity:** The paper does not provide a thorough analysis of the sensitivity of the method to hyperparameter choices (e.g., bandwidths of KDEs, resampling rate).  This would provide insights into the robustness and practical applicability of the proposed approach.\\n\\n\\n**Overall Assessment:**\\n\\nThe paper presents a valuable contribution to the field of skill learning by introducing a novel approach for generating all feasible actions. The methodological rigor and comprehensive evaluation are commendable. However, the simplified experimental setup and lack of deeper analysis of convergence and scalability raise concerns about the generalizability of the findings.  Addressing the weaknesses outlined above would significantly strengthen the paper.\\n\\n\\n**Recommendation:**\\n\\nWhile the core idea is promising and the paper presents a solid contribution, the limitations regarding scalability and the need for a more thorough analysis of convergence prevent it from being ready for acceptance in its current form.  I recommend a **Reject** for ICLR 2023, but encourage the authors to address the weaknesses mentioned above and resubmit to a future conference or journal.  Addressing the scalability concerns, providing a more detailed analysis of the convergence issues and hyperparameter sensitivity, and possibly extending the experiments to more complex tasks will be crucial for improving the impact and significance of this work.\\n\\n\\n**FINAL DECISION: Reject**\\n', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.8183974975}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ReviewerFlash', 'prompt': 'Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\\n\\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## ICLR 2023 Review: Learning to Generate All Feasible Actions\n\nThis paper introduces a novel method for training a generative neural network to produce all feasible actions for a given subtask within an interactive environment.  The approach leverages ideas from contextual bandits and reinforcement learning but departs from the typical objective of finding a single optimal action. Instead, it aims to learn the entire set of feasible actions, represented as a probability distribution. This is achieved by minimizing an f-divergence between the generated distribution and the true distribution of feasible actions.  The authors propose a novel gradient estimator based on kernel density estimation (KDE), resampling, and importance sampling, addressing challenges related to the estimation of probability densities and ensuring convergence.  The method is evaluated on a simplified 2D robotic grasping task, comparing different f-divergences (FKL, RKL, JS) against maximum entropy RL and conditional GANs.\n\n**Strengths:**\n\n* **Novelty:** The core idea of learning all feasible actions instead of just the optimal one is novel and addresses a significant limitation of existing RL and CB methods in situations with multiple valid solutions. This has important implications for skill learning and transfer.\n* **Technical Soundness:** The mathematical formulation of the problem and the derivation of the gradient estimator are generally well-presented and rigorous. The authors acknowledge and address the convergence issues encountered, providing a plausible explanation and a solution involving resampling.  The use of KDE for density estimation is a reasonable choice given the problem setting.\n* **Comprehensive Evaluation:** The paper includes a thorough comparison of different f-divergences and related methods on a well-defined, albeit simplified, robotic grasping task. The qualitative and quantitative results provide strong evidence supporting the claims. The inclusion of a control experiment (maximum entropy RL with KDE) strengthens the analysis.\n* **Clear Writing:** The paper is generally well-written and easy to follow. The authors effectively explain the motivation, methodology, and results.  The supplementary material provides further details on the experimental setup and network architectures.\n\n**Weaknesses:**\n\n* **Simplified Task:** The 2D robotic grasping task, while useful for demonstrating the core idea, is a significant simplification of real-world scenarios. The scalability to higher-dimensional action spaces and more complex tasks remains unproven. The authors acknowledge this limitation, but a more substantial discussion on how the method might scale is needed.\n* **Computational Cost:** The reliance on KDE and resampling introduces significant computational overhead.  The authors mention that training takes 48 hours on an A100 GPU, which highlights a potential bottleneck for larger-scale applications.  Further discussion on computational complexity and potential optimizations would be beneficial.\n* **Hyperparameter Sensitivity:** The performance of the method likely depends heavily on the choice of hyperparameters (bandwidths of the KDEs, resampling rate).  A more thorough sensitivity analysis would strengthen the claims.\n* **Limited Ablation Study:** While the authors compare different f-divergences, a more comprehensive ablation study exploring the impact of different components of the proposed method (e.g., balanced replay memory, maximum uncertainty collection, action optimization) is warranted.\n\n\n**Overall Assessment:**\n\nThis paper presents a valuable contribution to the field of skill learning and generative models in interactive environments. The core idea is novel and well-motivated, and the proposed method shows promising results on a simplified task.  While the limitations related to scalability and computational cost are acknowledged, addressing them more thoroughly would significantly improve the paper's impact.  The weaknesses listed above, particularly the lack of extensive ablation studies and sensitivity analysis, need to be addressed before acceptance.  However, the overall quality and originality of the work are sufficient to warrant a strong consideration for publication.\n\n\n**FINAL DECISION: Accept (with minor revisions)**\n\nThe authors should address the weaknesses mentioned above in a revised version.  Specifically, they should include:\n\n* A more detailed discussion on the scalability of the proposed method to higher-dimensional problems.\n* A more thorough analysis of the computational complexity and potential optimizations.\n* A sensitivity analysis of the hyperparameters.\n* A more comprehensive ablation study investigating the contributions of the different components of the proposed method.\n\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9.817583707499999\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ReviewerFlash\",\n  \"prompt\": \"Review the following paper and provide a detailed review including strengths and weaknesses, and recommend Accept/Reject for ICLR 2023. The review should be detailed and include strengths and weaknesses. Finish the entire review and DO NOT STOP in the middle. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning to Generate All Feasible Actions\n\n1 INTRODUCTION: Complex tasks can often be decomposed into multiple subtasks, with corresponding skills that solve these subtasks. Learning reusable and transferable skills is an active area of research (Kalashnikov et al. (2021); Chebotar et al. (2021)). However, given a subtask, learning or even defining the corresponding skill is not straightforward. Consider a robotic scenario where a robot is tasked to grasp an object and handle it in different follow-up tasks. Different follow-up tasks can have different optimal grasps if the object has multiple feasible grasping poses. Therefore, a grasping skill cannot be trained based on optimality definitions of individual follow-up tasks. However, a grasping algorithm that learned all feasible grasps could support all possible follow-up tasks even without explicit knowledge thereof during training. The follow-up tasks can then select their respective optimal grasp among the proposed feasible options. Therefore, we consider a skill to be defined by the set of all feasible actions of a subtask. We propose a novel method to train a generative neural network to generate all feasible actions of a subtask by interacting with an environment. The interaction loop is adopted from Contextual Bandit (CB) (Langford et al. (2008)) and Reinforcement Learning (RL) (Sutton & Barto (2018)): the environment presents a state for which the actor selects an action, which is tested in the environment, yielding either a success or failure outcome. As in CB, we limit ourselves to one-step interactions as opposed to sequential multi-step interactions common in RL. However, we do not minimize regret, typically done in CB. Instead, we optimize the final policy as in RL. Unlike CB and RL, the approach does not aim to find one optimal solution for a given problem but aims to learn all feasible ones. By interpreting the feasibility of each action given a state as a posterior probability distribution over the actions, a target probability density function (pdf) is defined. The actor is trained to minimize a divergence of its output distribution to this target pdf. The training algorithm in the method proposed can be used with any given f-divergence, including Reverse Kullback-Leibler (RKL), Forward Kullback-Leibler (FKL), and Jensen-Shannon (JS). The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode (Jerfel et al. (2021)). The training algorithm presented in this paper uses Kernel Density Estimation (KDE) to estimate the pdf of the actor and Monte Carlo integration with importance sampling to estimate the normalization of the target. The divergences are estimated using samples from a proposal distribution which is a separate KDE based on the original samples of the actor. This resampling step is necessary for convergence, which is discussed in Section 3.3. As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously. The critic\u2019s feasibility estimate of an action is then used to form the target distribution. The learning algorithm has been tested on robotic grasping problem with reduced dimensionality. The problem assumes that the object to be grasped is an extrusion of a 2D shape and that the robot is actuating a parallel gripper constrained to be vertical. We test FKL, RKL, and JS divergences and compare them to implementations of maximum entropy (ME) RL and Generative Adversarial Networks (GANs). The contributions of this paper are the following: \u2022 Design of a new learning method for generative neural network models to explicitly learn to generate all feasible actions. \u2022 Introduction of a novel gradient estimator for f-divergences that takes advantage of KDEs, resampling, and importance sampling. \u2022 Application of the proposed learning algorithm to a 2D robotic grasping problem, comparing the proposed gradient estimators for f-divergences with related methods. The rest of this work is structured as follows. Section 2 discusses the related work. Section 3 describes the optimization problem followed by the methodology in Section 4. The evaluation setup is described in Section 5 and the results are presented in Section 6. Section 7 concludes and gives an outlook on future work. 2 RELATED WORK: Learning a generic skill is commonly applied in many fields, such as in natural language processing (NLP) and computer vision (CV). Large language models are trained generically in NLP and applied to new tasks with zero or few-shot learning (Brown et al. (2020)). Similarly, in CV, feature extraction networks can be used as part of specialized applications (Shin et al. (2016)). Most current approaches to learning reusable skills within an interactive environment focus on implicit learning of the generic subtask, typically through multi-task learning (Deisenroth et al. (2014); Kalashnikov et al. (2021)). CBs have been successfully applied to several interactive learning problems with discrete action spaces (Langford & Zhang (2007); Agarwal et al. (2014); Foster & Rakhlin (2020); Simchi-Levi & Xu (2021)). In several cases, the context and action spaces have been embedded in a linear multidimensional action space. The embedding keeps the interaction linear while the action and context embeddings can be non-linear (Chernozhukov et al. (2019); Foster et al. (2020); Zhu et al. (2022)). Recently, there has been an increased interest in extending the approach to continuous action spaces. However, most works are limited to 1D actions (Chernozhukov et al. (2019); Majzoubi et al. (2020); Zhu & Mineiro (2022)). Learning from an interactive environment is also the focus of RL (Sutton & Barto (2018)). Many RL approaches use Actor-Critic (AC) architectures, among which the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. (2018)) is most related to our work. In SAC, the state-action value function of the critic is transformed into an energy-based distribution (Haarnoja et al. (2017)), yielding the target of the actor\u2019s distribution. SAC uses RKL as the loss function for the actor, which yields maximum entropy RL. Through a reparameterization trick, which usually uses the family of Gaussians, the RKL is minimized through a direct gradient from the critic. GANs propose a similar architecture to AC, training a generator and discriminator adversarially. This adversarial training is equivalent to minimizing the JS divergence (Goodfellow et al. (2014)) and has been extended to arbitrary f-divergences (Nowozin et al. (2016)). Conditional GANs (Mirza & Osindero (2014)) offer an alternative solution to the posterior sampling problem, as a generator conditioned on a given state can be trained to provide successful actions adversarially. However, the problem analyzed in our paper is not naturally adversarial, as actions that have not yet been tested in the interactive environment should not be implicitly rejected. The discriminator learns to discriminate between tested successful actions from untested ones, providing the generator with inconsistent gradients. Expert knowledge is used in Imitation Learning (IL) to derive a policy from demonstration data. The policy may be learned in a supervised manner in behavior cloning (Pomerleau (1988)) or as a combination of Inverse Reinforcement Learning (IRL) (Ng et al. (2000)) to learn a reward function and a subsequent RL procedure. Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), mitigating the intermediate IRL step by using a generative adversary. As discussed in Li et al. (2017), the policy learned by GAIL tends to interpolate between modes leading to erroneous behavior in multimodal settings. Using f-divergence minimization, the authors in Ke et al. (2020); Ghasemipour et al. (2020) intentionally collapse modes to avoid interpolation. IL and adversarial approaches require large amounts of expert data. However, expert data is limited in an interactive environment. Additionally, given that we aim to find all possible actions, even more expert data representative of all possible actions would be required. Posterior sampling has been a long-standing problem in statistics. State-of-the-art methods in Bayesian statistics rely on Markov Chain Monte Carlo (MCMC) algorithms (Hastings (1970); Gelfand & Smith (1990)), eliminating the need to normalize the distribution which is often an intractable problem (Kruschke (2015)). Variational Inference (VI) relies instead on fitting the posterior with a family of parametric probability distributions that can then be sampled from (Jordan et al. (1999); Wainwright & Jordan (2008)). Neural samplers offer another alternative by approximating the posterior with a generative neural network (Nowozin et al. (2016); Hu et al. (2018)). Normalizing flows also infer the pdf for each sample using invertible mappings (Rezende & Mohamed (2015); Tabak & Turner (2013); Tabak & Vanden-Eijnden (2010)). While this approach does not require density estimates, it limits its applicability to specific neural network designs. 3 OPTIMIZATION PROBLEM:  3.1 PROBLEM FORMULATION: An interactive environment, simulated or physical, is defined as a function g : S \u00d7 A 7\u2192 {0, 1}, where S is the state space of the problem, and A is the corresponding action space. For all s \u2208 S we associate a feasible action space A+s such that g(s, a) = 1,\u2200a \u2208 A+s and an infeasible action space A\u2212s such that g(s, a) = 0,\u2200a \u2208 A\u2212s , with A+s \u222a A\u2212s = A. The goal of this work is to find a state-dependent surjective map \u03c0s : Z \u2192 A+s , referred to as policy, where Z is a latent space of appropriate dimensionality. For a given state and uniform sampling from the latent space Z , the pdf of \u03c0s is a function qs : A 7\u2192 R, which is the posterior distribution qs(a) = q(a|s). For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s, a)\u222b A g(s, a) da , (1) which is the true posterior ps(a) = p(a|s). The optimal solution satisfies Df (ps || qs) = 0, where Df is an f-divergence, for example from Table 1. This implies ps = qs, therefore the support of qs is equal to the support of ps, which is A+s by definition. Thus, the optimal policy is the solution to the optimization problem: \u03c0\u0303s = argmin\u03c0s\u223c\u03a0 Df (ps || qs) , (2) with \u03a0 being an arbitrary family of distributions. To generalize over all states s \u2208 S , the policy can be modeled as a neural sampler \u03c0\u03b8 : S \u00d7 Z 7\u2192 A, with a corresponding pdf q\u03b8(a|s), where \u03b8 indicates the parameters of the neural network. Assuming that the environment can be used efficiently for repeated controlled experiments, i.e., testing several actions for the same state, the above optimization problem can be solved directly on the environment. If this is not possible, a critic network can be used to imitate the environment, which is further discussed in Section 4. Note that the system state is often only partially observable, and the action must be inferred from an observation. For simplicity of notation in the following derivation of the gradients, we assume that the state is directly observable, and we omit the state and action dependence of q and p. 3.2 F-DIVERGENCES: The f-divergence between two pdfs p and q is a generalization of the Kullback-Leibler (KL) divergence and has the form (Liese & Vajda (2006)) Df (p || q\u03b8) = \u222b A p f ( q\u03b8 p ) da, (3) where f : (0,\u221e) \u2192 R is a convex function. Different choices of f lead to well known divergences as summarized in Table 1. The gradients of the f-divergence w.r.t. \u03b8 can be estimated commuting the derivative with the integral (L\u2019Ecuyer (1995)) and using the score function gradient estimator (Kleijnen & Rubinstein (1996)) as \u2202 \u2202\u03b8 Df = \u2202 \u2202\u03b8 \u222b A p f ( q\u03b8 p ) da = \u222b A p f \u2032 ( q\u03b8 p ) 1 p \u2202 \u2202\u03b8 q\u03b8 da = \u222b A q\u03b8 f \u2032 ( q\u03b8 p ) \u2202 \u2202\u03b8 log q\u03b8 da, (4) using the fact that p does not depend on \u03b8. Since q\u03b8 is normalized to 1 and thus \u2202\u03b8 \u222b A q da =\u222b A q \u2202\u03b8 log q da = 0, a Lagrangian term \u03bb can be added to the gradient estimator: \u2202 \u2202\u03b8 Df = \u222b A q\u03b8 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 da. (5) If the support of q\u03b8 includes all of A the above formula can be rewritten as the expectation on q\u03b8 as \u2202 \u2202\u03b8 Df = Eq\u03b8 [( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (6) Sampling from a proposal distribution q\u2032, the expectation can be computed with importance sampling (Robert & Casella (2004); Liu (2001)) as \u2202 \u2202\u03b8 Df \u2248 Eq\u2032 [ q\u03b8 q\u2032 ( f \u2032 ( q\u03b8 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u03b8 ] . (7) 3.3 GRADIENT ESTIMATION: Given a sample a \u223c q\u03b8, it is not possible to directly evaluate q\u03b8(a) as it is not available in closed form. Therefore, q\u03b8 needs to be estimated to compute the gradients of the f-divergence. Given N sampled actions ai \u223c q\u03b8, q\u03b8 can be approximated with a KDE by q\u03b8(a) \u2248 q\u0302\u03b8,\u03c3(a) = 1 N \u2211 ai\u223cq\u03b8 k\u03c3(a\u2212 ai), (8) where k\u03c3 is a Gaussian kernel with a diagonal bandwidth matrix \u03c3. The KDE makes the estimate of the expectation possible. Using equation 6, computing the expectation value as the average over the samples yields \u2202 \u2202\u03b8 Df \u2248 1 N \u2211 ai\u223cq\u03b8 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3. (9) This gradient estimator turned out not to converge in our experiments. While a systematic investigation of the convergence issue was not completed, we suspect two primary reasons for this. First, the support q\u03b8 usually does not cover the whole action space A, which is necessary for the expectation formulation in equation 6. Second, evaluating q\u03b8(ai) based on a KDE, which uses aj as supports, has a bias for j = i. Adding Gaussian noise to the samples gives full support in A and reduces the bias at the support points of the KDE, which lead to convergence in the experiments. The new smoothed samples are given by a\u2217j = ai + \u03f5 for mi \u2264 j < m(i+ 1) and \u03f5 \u223c N (0, \u03c3\u2032), where m indicates the number of smoothed samples drawn for each original sample. This is equivalent to sampling from a KDE with ai as supports and \u03c3\u2032 as bandwidth. The gradient, using importance sampling in equation 7, can be rewritten after resampling as follows \u2202 \u2202\u03b8 Df \u2248 1 M \u2211 a\u2217j\u223cq\u0302\u03b8,\u03c3\u2032 q\u0302\u03b8,\u03c3 q\u0302\u03b8,\u03c3\u2032 ( f \u2032 ( q\u0302\u03b8,\u03c3 p ) + \u03bb ) \u2202 \u2202\u03b8 log q\u0302\u03b8,\u03c3, (10) with M = mN . Additionally, equation 10 requires an estimate of p, which in turn requires an estimate of the volume in equation 1\u222b A g(a) da \u2248 1 M \u2211 a\u2217j g(a\u2217j ) q\u0302\u03b8,\u03c3\u2032(a\u2217j ) . (11) This estimation is similar to self-normalized importance sampling (Murphy (2012)) but uses the proposal distribution. The bandwidth \u03c3\u2032 of the proposal distribution is a hyper-parameter. Setting \u03c3\u2032 = c \u03c3, experiments show that c > 1 helps convergence. Intuitively, a larger bandwidth enables the exploration of nearby modes in the action space. Specific estimators for the different f-divergences can be obtained substituting f \u2032 from Table 1 into equation 10. A summary of the gradient estimators used in this work is given in Table 2. 4 METHODOLOGY: The derivation in Section 3.3 assumes that the training could be performed directly on the interactive environment. Typically, this is not possible, either because of reproducibility in real experiments or computational cost for simulations. An auxiliary neural network \u03be\u03d5 : S \u00d7A \u2192 R with parameters \u03d5, can be trained to imitate g of the environment. The policy can then be trained to match the distribution of the feasible actions according to this auxiliary neural network. We refer to \u03c0\u03b8 and \u03be\u03d5 as generative actor and critic, respectively. The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome. The actor trains on a batch of states from the memory. For each state, multiple actions are sampled from the actor, used as support for q\u0302\u03b8,\u03c3 and q\u0302\u03b8,\u03c3\u2032 . New samples are drawn from the proposal distribution q\u0302\u03b8,\u03c3\u2032 . These samples are evaluated by the critic \u03be\u03d5, and the gradients are computed according to equation 10. While the general interaction loop is standard in RL, two changes have proven beneficial to the convergence: balanced replay memory and maximum uncertainty collection. Additionally, an action optimization can take advantage of the density estimate to improve performance after training. 4.1 BALANCED REPLAY MEMORY: Since the environment yields either a success or a failure, the critic is a binary classifier that suffers from unbalanced data when being trained. Its memory dataset continuously grows through the interactions between the actor and the environment. In the beginning, the actor performs poorly, yielding primarily experiences with failure labels, with the opposite at the end of the training. This labeling bias prevented the critic from distinguishing between success and failure outcomes, making convergence impossible. To avoid the critic from biasing towards failure or success labels, we use two replay memories, one for failures and one for successes. When training the critic, half of the experiences are sampled from the positive replay memory and the other half from the negative replay memory. With this strategy, the labeling bias can be mitigated. The potentially amplified classification bias (e.g., complicated shapes have more failure labels) did not appear to hinder convergence. This memory can be prefilled with imitation examples to bootstrap the critic learning. While it is possible to minimize the use of expert knowledge, this paper focuses on the main learning method, while the impact of imitation learning will be analyzed in future work. 4.2 MAXIMUM-UNCERTAINTY COLLECTION: Given one state of the environment, the actor can generate several candidate actions. Depending on the training stage and the state, these proposed actions can have a different degree of information for the critic. Selecting actions for which the critic predicts \u03be \u2248 0.5, i.e., it cannot decide between success and failure, can provide higher information content. This strategy has proven to improve the convergence in our tests. 4.3 ACTION OPTIMIZATION: Optimal performance in an environment with multiple disconnected sets of feasible actions would require a multimodal action distribution with a probability density of zero between the modes. Since the actor is a continuous neural network and the latent space is continuous, the actor cannot generate only positive actions. However, the actor aims to minimize the probability density for actions in the gaps between the modes. The probability density at each action can be estimated using the KDE, and the actions with the lowest corresponding density can be rejected. The accuracy of actors with strong multimodal performance like FKL is significantly increased from action optimization as shown in Section 6. 5 EXPERIMENTAL SETUP:  5.1 ROBOTIC GRASPING SIMULATION: The proposed architecture was tested in a simplified robotic grasping simulation. We assume a vertical configuration of a parallel gripper with three degrees of freedom x, y, and \u03b1 and an object that is an extrusion of a 2D shape. The success of a grasp is only determined by the relative position and alignment of the gripper to the outline of the object as seen from a camera positioned above the experiment. We developed an algorithm that, given the alignment of the gripper, i.e., x, y, and \u03b1 and a simulated picture of the object from a fixed camera, provides a success/failure outcome in a deterministic and reproducible manner. Given the maximum aperture of the parallel gripper l and the width of the gripper claws w, the simulation analyzes the cropped image content of dimensions l\u00d7w between the gripper claws before the claws close on the object. The simulation checks if the object is sufficiently present and if it is equidistant from the claws and aligned within parameterized margins. Figure 4 in the appendix shows successful grasping poses and the respective gripper content for some objects. The choice of this simulation, as opposed to existing robotic simulations, was motivated by efficiency while iterating through several combinations of models and parameters. The target distribution fidelity is not of primary concern in this work. The focus is instead on the capability of the proposed method to learn all feasible actions. 5.2 COMPARISON: In the evaluation, we are comparing different f-divergences with each other and with two other approaches. The analyzed f-divergences are the FKL, RKL, and JS divergences. The two other approaches are an ME RL algorithm similar to SAC in Haarnoja et al. (2018), which trains the actor to minimize min \u03b8 Es\u223cM,z\u223cZ [log q\u03b8(\u03c0\u03b8(s, z)|s)\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z))] , (12) with M being the replay memory. The critic is trained as described in Section 4. Instead of using the reparameterization trick with a known distribution to estimate the entropy, we use the KDE. The other approach is an implementation of a conditional GAN (Mirza & Osindero (2014)) with a growing dataset. The min-max optimization problem is given through min \u03b8 max \u03d5 Es,a\u223cMp,z\u223cZ [log(\u03be\u03d5(s, a))\u2212 log(1\u2212 \u03be\u03d5(s, \u03c0\u03b8(s, z)))] , (13) with the positive replay memory Mp, which grows through successful interactions with the environment. An asterisk is added (e.g., JS*) when using action optimization, rejecting 10% of the proposed actions with the lowest probability density. The actor gradient estimators for all approaches are listed in Table 2. 6 RESULTS: For each configuration, 3 agents were trained for 1 million interaction steps with the environment, taking approximately 48 hours on a single NVIDIA A100 GPU. At the start of the training, 80k examples, including positives and negatives, for randomly generated shapes were added to the balanced replay memory to bootstrap the critic and discriminator learning. Figure 1 shows the problem, the ground truth feasible picking positions, the critic estimate, and a heat-map of the actor\u2019s proposed actions. All figures are projections taking the maximum over the dimension that is not shown. In the problem visualization in Figure 1a, five feasible picks are shown in different colors, which correspond to the markers in Figure 1b. These markers highlight the complex multimodality of the problem. While it appears that, e.g., red and purple are in the same mode in the x-y projection, it is visible in the x-\u03b1 projection that they are not directly connected. Figure 1c shows that the critic has an approximate understanding of the feasible regions of the action space, showing five modes clearly in the x-y projection. The actor distribution in Figure 1d also shows all five modes, while the output is significantly sharper in the centers of the modes. This is due to the use of the KDEs and the choice of bandwidth \u03c3. In the qualitative comparison in Figure 2, the actor distributions of the different algorithms are shown for three different shapes. While the H and 8 shapes were trained on, the Box shape has not been seen during training. The different subfigures show the action heat maps of all implemented algorithms, showing only the x-y projections. The H-row shows that JS and FKL learned all five modes, with JS having the fewest samples in the connecting area. RKL and the GAN show two modes. The ME implementation collapses in a single mode. The 8-row and the Box-row show a similar pattern with the most pronounced spread of the action distributions in JS and FKL and mostly collapsed action regions in the other approaches. To quantify the multimodal capabilities, each algorithm\u2019s accuracy and shares of modes on all shapes were evaluated. For each shape, 1024 random states were generated that differ in pose, color, and geometry (example variations can be seen in the Appendix A). For each state, 1024 actions were sampled from the different actors. The actions were then evaluated, and the mode of each action was recorded. The modes were then ranked and averaged over all the states of that shape by frequency. By averaging the ranks instead of the modes, the last rank shows the average ratio of the least frequent mode for each state. Figure 3 shows the shares of each rank for the H and Box shapes for all the algorithms, with the asterisk indicating that action optimization was applied. This figure presents the multimodal capabilities of the JS and FKL algorithms, which are the only ones with the last ranked mode present for the H and with significantly more pronounced modes than the others for the Box. The generalization capability of the GAN implementation is significantly lower than the others, as seen on the Box shape. To quantify the performance, Table 3 shows the accuracy (feasible actions generated over total actions generated) for each shape and the last ranked mode for the H, T, and Box shapes. The table shows that ME has solid performance on all shapes but fails to find the different modes. The GAN algorithm performs well with some modes present, but overall it is weaker than the others. RKL, has high scores but mostly fails at finding all the modes. FKL shows good performance in mode finding, with an overall accuracy similar to RKL. JS is on par with the ME accuracy with the addition that it repeatably finds all the modes. Generally, action optimization improves accuracy but does not help mode finding, slightly decreasing the least ranked mode for most approaches. The maximum deviations in the superscript show that all approaches learn reliably with the GAN having the highest performance deviations among runs. 7 CONCLUSION AND FUTURE WORK: This work proposes to learn a skill by training a generator to generate all feasible actions for the respective subtask. To this end, the output distribution of the generator is learned to match a uniform distribution over the feasible action space. While learning within a 2D grasping simulation, the method shows stable convergence for FKL, RKL, and JS. As expected, FKL is more efficient in visiting all the modes. JS has the highest accuracy while reliably visiting all the modes. The proposed learning strategy expands the current state-of-the-art training within multimodal interactive environments by showing competitive accuracy while visiting all the modes. Some limitations have emerged during experimentation. Currently, many imitation examples are required to bootstrap the critic\u2019s learning. A possibility to mitigate this could be the progressive tuning of the KDEs or learning their parameters during training. This approach could favor exploration initially and divergence estimation later in training. A complementary strategy could be using curriculum learning techniques that start with simple problems where solutions are less sparse in the action space. Furthermore, the proposed approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially. The limit on the degrees of freedom will be explored in future work. A mitigation of this issue can come from the rich literature on non-parametric density estimation in higher-dimensional problems and its applicability to the proposed method. Another approach could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories. 8 REPRODUCIBILITY STATEMENT: The final version will include a GitHub link to the code for learning and evaluation. A GRASPING SIMULATION EXAMPLES: B NEURAL NETWORK ARCHITECTURES The neural network design was guided by simplicity and inspired by GANs. Features that rely on domain-specific knowledge are avoided to better evaluate the learning method presented in the paper. The structure of the actor and critic neural networks are illustrated in Figure 5. The residual feature extraction network He et al. (2016) is shared between actor and critic. As a peculiarity of the network and of the loss, the actor\u2019s inferred action has four components, [x, y, r sin\u03b1, r cos\u03b1], with r \u2208 [0, \u221a 2]. The angle can be extracted trivially with the arctan of the ratio of the third and fourth action components. As the scale factor r does not change the angle, the critic receives the normalized action [x, y, sin\u03b1, cos\u03b1] as input. To avoid the actor from reaching the singularity at r = 0 and the distribution q being spread along the radius, g(s, a) and \u03be(s, a) are scaled with an unnormalized Gaussian on the radius, centered at 0.5 with the standard deviation of 0.4. C HYPERPARAMETERS:\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': \"## ICLR 2023 Review: Learning to Generate All Feasible Actions\\n\\nThis paper introduces a novel method for training a generative neural network to produce all feasible actions for a given subtask within an interactive environment.  The approach leverages ideas from contextual bandits and reinforcement learning but departs from the typical objective of finding a single optimal action. Instead, it aims to learn the entire set of feasible actions, represented as a probability distribution. This is achieved by minimizing an f-divergence between the generated distribution and the true distribution of feasible actions.  The authors propose a novel gradient estimator based on kernel density estimation (KDE), resampling, and importance sampling, addressing challenges related to the estimation of probability densities and ensuring convergence.  The method is evaluated on a simplified 2D robotic grasping task, comparing different f-divergences (FKL, RKL, JS) against maximum entropy RL and conditional GANs.\\n\\n**Strengths:**\\n\\n* **Novelty:** The core idea of learning all feasible actions instead of just the optimal one is novel and addresses a significant limitation of existing RL and CB methods in situations with multiple valid solutions. This has important implications for skill learning and transfer.\\n* **Technical Soundness:** The mathematical formulation of the problem and the derivation of the gradient estimator are generally well-presented and rigorous. The authors acknowledge and address the convergence issues encountered, providing a plausible explanation and a solution involving resampling.  The use of KDE for density estimation is a reasonable choice given the problem setting.\\n* **Comprehensive Evaluation:** The paper includes a thorough comparison of different f-divergences and related methods on a well-defined, albeit simplified, robotic grasping task. The qualitative and quantitative results provide strong evidence supporting the claims. The inclusion of a control experiment (maximum entropy RL with KDE) strengthens the analysis.\\n* **Clear Writing:** The paper is generally well-written and easy to follow. The authors effectively explain the motivation, methodology, and results.  The supplementary material provides further details on the experimental setup and network architectures.\\n\\n**Weaknesses:**\\n\\n* **Simplified Task:** The 2D robotic grasping task, while useful for demonstrating the core idea, is a significant simplification of real-world scenarios. The scalability to higher-dimensional action spaces and more complex tasks remains unproven. The authors acknowledge this limitation, but a more substantial discussion on how the method might scale is needed.\\n* **Computational Cost:** The reliance on KDE and resampling introduces significant computational overhead.  The authors mention that training takes 48 hours on an A100 GPU, which highlights a potential bottleneck for larger-scale applications.  Further discussion on computational complexity and potential optimizations would be beneficial.\\n* **Hyperparameter Sensitivity:** The performance of the method likely depends heavily on the choice of hyperparameters (bandwidths of the KDEs, resampling rate).  A more thorough sensitivity analysis would strengthen the claims.\\n* **Limited Ablation Study:** While the authors compare different f-divergences, a more comprehensive ablation study exploring the impact of different components of the proposed method (e.g., balanced replay memory, maximum uncertainty collection, action optimization) is warranted.\\n\\n\\n**Overall Assessment:**\\n\\nThis paper presents a valuable contribution to the field of skill learning and generative models in interactive environments. The core idea is novel and well-motivated, and the proposed method shows promising results on a simplified task.  While the limitations related to scalability and computational cost are acknowledged, addressing them more thoroughly would significantly improve the paper's impact.  The weaknesses listed above, particularly the lack of extensive ablation studies and sensitivity analysis, need to be addressed before acceptance.  However, the overall quality and originality of the work are sufficient to warrant a strong consideration for publication.\\n\\n\\n**FINAL DECISION: Accept (with minor revisions)**\\n\\nThe authors should address the weaknesses mentioned above in a revised version.  Specifically, they should include:\\n\\n* A more detailed discussion on the scalability of the proposed method to higher-dimensional problems.\\n* A more thorough analysis of the computational complexity and potential optimizations.\\n* A sensitivity analysis of the hyperparameters.\\n* A more comprehensive ablation study investigating the contributions of the different components of the proposed method.\\n\", 'remaining_resource_budget': 100, 'remaining_expense_budget': 9.817583707499999}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "**Step 4: Synthesize the reviews and provide a final decision.**\n\nHere's a summary of the reviews:\n\n*   **Reviewer 1:** Accept (with major revisions) - Highlights novelty, comprehensive methodology, rigorous evaluation, and addressing convergence issues. Concerns about the simplified task, computational cost, hyperparameter sensitivity, and KDE limitations.\n*   **Reviewer 2:** Reject - Acknowledges the core idea and solid contribution but raises concerns about scalability and convergence. Recommends addressing weaknesses and resubmitting.\n*   **Reviewer 3:** Accept (with minor revisions) - Emphasizes novelty, technical soundness, comprehensive evaluation, and clear writing. Points out limitations in scalability, computational cost, hyperparameter sensitivity, and limited ablation study.\n\nOverall, the reviewers acknowledge the novelty and potential of the approach. However, they also raise concerns about the limitations of the experimental setup, scalability to higher-dimensional problems, computational cost, and hyperparameter sensitivity. Two reviewers recommend acceptance with revisions, while one recommends rejection.\n\nConsidering the positive feedback on the novelty and potential impact of the work, and the fact that two out of three reviewers recommend acceptance with revisions, I lean towards accepting the paper with the expectation that the authors will address the identified weaknesses in a revised version.\n\n**FINAL DECISION: Accept (with major revisions)**\n"}], "ground_truth": "Reject", "response_time": 435.12551712989807}
